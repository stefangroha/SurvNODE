{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "random_seed = 137\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping class from https://github.com/Bjarten/early-stopping-pytorch\n",
    "from SurvNODE.EarlyStopping import EarlyStopping\n",
    "from SurvNODE.SurvNODE_x_ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measures(odesurv,initial,x,Tstart,Tstop,From,To,trans,status, multiplier=1.,points=500):\n",
    "    with torch.no_grad():\n",
    "        time_grid = np.linspace(0, multiplier, points)\n",
    "        pvec = torch.zeros((points,x.shape[0]))\n",
    "        surv_ode = odesurv.predict(x,torch.from_numpy(np.linspace(0,multiplier,points)).float().to(x.device))\n",
    "        pvec = torch.einsum(\"ilkj,k->ilj\",(surv_ode[:,:,:,:],initial))[:,:,0].cpu()\n",
    "        pvec = np.array(pvec.cpu().detach())\n",
    "        surv_ode_df = pd.DataFrame(pvec)\n",
    "        surv_ode_df.loc[:,\"time\"] = np.linspace(0,multiplier,points)\n",
    "        surv_ode_df = surv_ode_df.set_index([\"time\"])\n",
    "        ev_ode = EvalSurv(surv_ode_df, np.array(Tstop.cpu()), np.array(status.cpu()), censor_surv='km')\n",
    "        conc = ev_ode.concordance_td('antolini')\n",
    "        ibs = ev_ode.integrated_brier_score(time_grid)\n",
    "        inbll = ev_ode.integrated_nbll(time_grid)\n",
    "    return conc,ibs,inbll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataloader(df,Tmax,batchsize):\n",
    "#     cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "#     cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "\n",
    "#     standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "#     leave = [(col, None) for col in cols_leave]\n",
    "#     x_mapper = DataFrameMapper(standardize + leave)\n",
    "#     X = x_mapper.fit_transform(df).astype('float32')\n",
    "\n",
    "    X = df[['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']].values\n",
    "    \n",
    "    X = torch.from_numpy(X).float().to(device)\n",
    "    T = torch.from_numpy(df[[\"duration\"]].values).float().flatten().to(device)\n",
    "    T = T/Tmax\n",
    "    T[T==0] = 1e-8\n",
    "    E = torch.from_numpy(df[[\"event\"]].values).float().flatten().to(device)\n",
    "\n",
    "    Tstart = torch.from_numpy(np.array([0 for i in range(T.shape[0])])).float().to(device)\n",
    "    From = torch.tensor([1],device=device).repeat((T.shape))\n",
    "    To = torch.tensor([2],device=device).repeat((T.shape))\n",
    "    trans = torch.tensor([1],device=device).repeat((T.shape))\n",
    "\n",
    "    dataset = TensorDataset(X,Tstart,T,From,To,trans,E)\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def odesurv_manual_benchmark(df_train, df_test,config,name):\n",
    "    torch.cuda.empty_cache()\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.loc[:,\"event\"])\n",
    "    \n",
    "    Tmax = df_train[\"duration\"].max()\n",
    "    \n",
    "    train_loader = make_dataloader(df_train,Tmax/config[\"multiplier\"],int(len(df_train)*config[\"batch_size\"]))\n",
    "    val_loader = make_dataloader(df_val,Tmax/config[\"multiplier\"],len(df_val))\n",
    "    test_loader = make_dataloader(df_test,Tmax/config[\"multiplier\"],len(df_test))\n",
    "    \n",
    "    num_in = 9\n",
    "    num_latent = config[\"num_latent\"]\n",
    "    layers_encoder =  [config[\"encoder_neurons\"]]*config[\"num_encoder_layers\"]\n",
    "    dropout_encoder = [config[\"encoder_dropout\"]]*config[\"num_encoder_layers\"]\n",
    "    layers_odefunc1 =  [config[\"odefunc_neurons1\"]]*config[\"num_odefunc_layers1\"]\n",
    "#     layers_odefunc2 =  [config[\"odefunc_neurons2\"]]*config[\"num_odefunc_layers2\"]\n",
    "\n",
    "    trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "    encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "    odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc1,config[\"softplus_beta\"]).to(device)\n",
    "    block = ODEBlock(odefunc).to(device)\n",
    "    odesurv = SurvNODE(block,encoder).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(odesurv.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config[\"scheduler_gamma\"], patience=config[\"scheduler_epoch\"])\n",
    "\n",
    "    early_stopping = EarlyStopping(name=name,patience=config[\"patience\"], verbose=True)\n",
    "    t = tqdm(range(200))\n",
    "    \n",
    "        \n",
    "    for i in t:\n",
    "        odesurv.train()\n",
    "        \n",
    "        loss_all = 0\n",
    "        for mini,ds in enumerate(train_loader):\n",
    "            myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "            optimizer.zero_grad()\n",
    "            myloss.backward()    \n",
    "            optimizer.step()\n",
    "\n",
    "            loss_all += myloss\n",
    "        odesurv.eval()\n",
    "        with torch.no_grad():\n",
    "            lossval,conc,ibs,ibnll = 0., 0., 0., 0.\n",
    "            for _,ds in enumerate(val_loader):\n",
    "                t1,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                lossval += t1.item()\n",
    "                t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds,multiplier=config[\"multiplier\"])\n",
    "                conc += t1\n",
    "                ibs += t2\n",
    "                ibnll += t3\n",
    "            early_stopping(lossval/len(val_loader), odesurv)\n",
    "            scheduler.step(lossval/len(val_loader))\n",
    "#             tune.report(score=lossval/len(val_loader), iterations=i)\n",
    "            \n",
    "            conc_test,ibs_test,ibnll_test = 0., 0., 0.\n",
    "            print(\"it: \"+str(i)+\", train loss=\"+str(loss_all.item())+\", validation loss=\"+str(lossval/len(val_loader))+\", c=\"+str(conc/len(val_loader))+\", ibs=\"+str(ibs/len(val_loader))+\", ibnll=\"+str(ibnll/len(val_loader)))\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            t.refresh()\n",
    "            t.set_postfix({\"loss.:\": myloss/len(train_loader)})\n",
    "\n",
    "#     odesurv.load_state_dict(torch.load(name+'_checkpoint.pt'))\n",
    "\n",
    "    odesurv.eval()\n",
    "    with torch.no_grad():\n",
    "        conc,ibs,ibnll = 0., 0., 0.\n",
    "        for _,ds in enumerate(test_loader):\n",
    "            t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds,multiplier=config[\"multiplier\"])\n",
    "            conc += t1\n",
    "            ibs += t2\n",
    "            ibnll += t3\n",
    "    return lossval/len(val_loader), conc/len(test_loader), ibs/len(test_loader), ibnll/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pycox import datasets\n",
    "\n",
    "kfold = StratifiedKFold(5, shuffle=True)\n",
    "df_all = datasets.metabric.read_df()\n",
    "gen = kfold.split(df_all.iloc[:,df_all.columns.values!=\"event\"],df_all.loc[:,\"event\"])\n",
    "\n",
    "config = {'batch_size': 0.1, \n",
    "          'encoder_dropout': 0.0, \n",
    "          'encoder_neurons': 223, \n",
    "          'lr': 0.00005, \n",
    "          'mu': 0.0001, \n",
    "          'multiplier': 1.0, \n",
    "          'num_encoder_layers': 3, \n",
    "          'num_latent': 145, \n",
    "          'num_odefunc_layers1': 4, \n",
    "          'num_odefunc_layers2': 3, \n",
    "          'odefunc_neurons1': 1421, \n",
    "          'odefunc_neurons2': 1231, \n",
    "          'patience': 20, \n",
    "          'scheduler_epoch': 5, \n",
    "          'scheduler_gamma': 0.1, \n",
    "          'softplus_beta': 1.0, \n",
    "          'weight_decay': 0.0001}\n",
    "\n",
    "odesurv_bench_vals = []\n",
    "# for g in gen:\n",
    "#     df_train = df_all.iloc[g[0]]\n",
    "#     df_test =  df_all.iloc[g[1]]\n",
    "#     _, conc, ibs, ibnll = odesurv_manual_benchmark(df_train,df_test,config,\"metabric_test\")\n",
    "#     odesurv_bench_vals.append([conc,ibs,ibnll])\n",
    "#     scores = torch.tensor(odesurv_bench_vals)\n",
    "#     print(scores)\n",
    "#     print(torch.mean(scores, dim=0))\n",
    "#     print(torch.std(scores, dim=0))\n",
    "    \n",
    "# print(scores)\n",
    "# print(torch.mean(scores, dim=0))\n",
    "# print(torch.std(scores, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "# print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "# print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox import datasets\n",
    "df = datasets.metabric.read_df()\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df.loc[:,\"event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from hyperopt import hp\n",
    "args = {\n",
    "    \"lr\": hp.choice(\"lr\", [1e-4, 5e-4]),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-3, 1e-4]),\n",
    "    \"num_latent\": hp.randint('num_latent', 20, 200),\n",
    "    \"encoder_neurons\": hp.randint('encoder_neurons', 100, 800),\n",
    "    \"num_encoder_layers\": hp.randint(\"num_encoder_layers\", 2, 5),\n",
    "    \"encoder_dropout\": 0.,\n",
    "    \"odefunc_neurons1\": hp.randint('odefunc_neurons1', 100, 1500),\n",
    "    \"num_odefunc_layers1\": hp.randint(\"num_odefunc_layers1\", 2, 5),\n",
    "#     \"odefunc_neurons2\": hp.randint('odefunc_neurons2', 100, 1500),\n",
    "#     \"num_odefunc_layers2\": 3,\n",
    "    \"batch_size\": 128,\n",
    "    \"multiplier\": 1.,\n",
    "    \"mu\": 1e-4,\n",
    "    \"softplus_beta\": hp.choice(\"softplus_beta\", [1., 0.1]),\n",
    "    \"scheduler_epoch\": 5,\n",
    "    \"scheduler_gamma\": 0.1,\n",
    "    \"patience\": 20\n",
    "}\n",
    "args = {'batch_size': hp.uniform('batch_size', 0.01, 0.5), \n",
    "          'encoder_dropout': hp.choice('encoder_dropout', [0.0,0.1,0.2]), \n",
    "          'encoder_neurons': hp.quniform('encoder_neurons', 20, 300), \n",
    "          'lr': 0.00005, #hp.choice('lr', [0.00005, 0.0005, 0.0001]), \n",
    "          'mu': 0.0001, \n",
    "          'multiplier': hp.choice('multiplier', [1.0, 2.0]), \n",
    "          'num_encoder_layers': 3, \n",
    "          'num_latent': hp.quniform('num_latent', 20, 200), \n",
    "          'num_odefunc_layers1': hp.randint('num_odefunc_layers1', 2, 5), \n",
    "#           'num_odefunc_layers2': 3, \n",
    "          'odefunc_neurons1': hp.quniform('odefunc_neurons1', 1000, 2000), \n",
    "#           'odefunc_neurons2': 1231, \n",
    "          'patience': 20, \n",
    "          'scheduler_epoch': 5, \n",
    "          'scheduler_gamma': 0.1, \n",
    "          'softplus_beta': 1.0, \n",
    "          'weight_decay': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an objective function\n",
    "def objective(args):\n",
    "    print(args)\n",
    "    score, conc, ibs, ibnll = odesurv_manual_benchmark(df_train,df_test,args,\"metabrick_test\")\n",
    "    print(\"\\nResults: {}, {}, {}\".format(conc, ibs, ibnll))\n",
    "    with open(\"hp_sep_log.txt\", \"a\") as f:\n",
    "        f.write(str(args))\n",
    "        f.write(\"\\nResults: {}, {}, {}\".format(conc, ibs, ibnll))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 730, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 3, 'num_latent': 129, 'num_odefunc_layers1': 2, 'odefunc_neurons1': 556, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.0001}\n",
      "  0%|                                                                          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfc7cc1cefc4b4fa7ba451f7733bea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.408804).  Saving model ...                                                        \n",
      "it: 0, train loss=0.43715882301330566, validation loss=0.4088039994239807, c=0.5318760811448341, ibs=0.28881434103571474, ibnll=1.0297291634181962\n",
      "Validation loss decreased (0.408804 --> 0.401936).  Saving model ...                                                   \n",
      "it: 1, train loss=0.4290658235549927, validation loss=0.40193551778793335, c=0.5025947476018242, ibs=0.2871722229746581, ibnll=1.0169307097269589\n",
      "Validation loss decreased (0.401936 --> 0.393154).  Saving model ...                                                   \n",
      "it: 2, train loss=0.421892374753952, validation loss=0.39315417408943176, c=0.5269067463437648, ibs=0.2850657185358277, ibnll=1.000712252099843\n",
      "Validation loss decreased (0.393154 --> 0.384757).  Saving model ...                                                   \n",
      "it: 3, train loss=0.41279610991477966, validation loss=0.3847571909427643, c=0.5802170152539707, ibs=0.282988812544743, ibnll=0.9850816182068313\n",
      "Validation loss decreased (0.384757 --> 0.374771).  Saving model ...                                                   \n",
      "it: 4, train loss=0.40398189425468445, validation loss=0.37477076053619385, c=0.5893693977040415, ibs=0.2804347843206378, ibnll=0.9665105347673099\n",
      "Validation loss decreased (0.374771 --> 0.365017).  Saving model ...                                                   \n",
      "it: 5, train loss=0.39349281787872314, validation loss=0.3650173842906952, c=0.5545211511243906, ibs=0.2778182180821122, ibnll=0.9481315336466071\n",
      "Validation loss decreased (0.365017 --> 0.356092).  Saving model ...                                                   \n",
      "it: 6, train loss=0.38332024216651917, validation loss=0.356092244386673, c=0.48878754521151124, ibs=0.27535515670169136, ibnll=0.931276775761099\n",
      "Validation loss decreased (0.356092 --> 0.346638).  Saving model ...                                                   \n",
      "it: 7, train loss=0.37397587299346924, validation loss=0.3466377854347229, c=0.5809089479477906, ibs=0.2726630273783867, ibnll=0.9132676918143817\n",
      "Validation loss decreased (0.346638 --> 0.338049).  Saving model ...                                                   \n",
      "it: 8, train loss=0.3641107678413391, validation loss=0.3380486071109772, c=0.6281176285579494, ibs=0.2701269239886526, ibnll=0.8968123806567067\n",
      "Validation loss decreased (0.338049 --> 0.329476).  Saving model ...                                                   \n",
      "it: 9, train loss=0.35514670610427856, validation loss=0.3294755518436432, c=0.5534203491114955, ibs=0.26749160361502056, ibnll=0.880326101180388\n",
      "Validation loss decreased (0.329476 --> 0.321210).  Saving model ...                                                   \n",
      "it: 10, train loss=0.3461788594722748, validation loss=0.3212098479270935, c=0.502626199087907, ibs=0.2648559027345287, ibnll=0.8642678984580338\n",
      "Validation loss decreased (0.321210 --> 0.313237).  Saving model ...                                                   \n",
      "it: 11, train loss=0.337502658367157, validation loss=0.3132370114326477, c=0.47579808145934893, ibs=0.2622220226685243, ibnll=0.848705149872671\n",
      "Validation loss decreased (0.313237 --> 0.305521).  Saving model ...                                                   \n",
      "it: 12, train loss=0.32914912700653076, validation loss=0.3055209517478943, c=0.515867274728731, ibs=0.259587207292669, ibnll=0.833591135262488\n",
      "Validation loss decreased (0.305521 --> 0.298076).  Saving model ...                                                   \n",
      "it: 13, train loss=0.32107454538345337, validation loss=0.29807570576667786, c=0.5350526812391886, ibs=0.2569569427323508, ibnll=0.8189244464098758\n",
      "Validation loss decreased (0.298076 --> 0.290933).  Saving model ...                                                   \n",
      "it: 14, train loss=0.3132804334163666, validation loss=0.2909330129623413, c=0.48976254128007546, ibs=0.2543334914727786, ibnll=0.804725940105612\n",
      "Validation loss decreased (0.290933 --> 0.284051).  Saving model ...                                                   \n",
      "it: 15, train loss=0.30578336119651794, validation loss=0.28405138850212097, c=0.5334486554489699, ibs=0.25171807379794003, ibnll=0.7909751647092056\n",
      "Validation loss decreased (0.284051 --> 0.277434).  Saving model ...                                                   \n",
      "it: 16, train loss=0.29855167865753174, validation loss=0.27743425965309143, c=0.5866331184148451, ibs=0.24911283224993686, ibnll=0.7776605195348038\n",
      "Validation loss decreased (0.277434 --> 0.271077).  Saving model ...                                                   \n",
      "it: 17, train loss=0.2915932834148407, validation loss=0.27107664942741394, c=0.4531530114797924, ibs=0.2465209552399625, ibnll=0.764776699588128\n",
      "Validation loss decreased (0.271077 --> 0.264976).  Saving model ...                                                   \n",
      "it: 18, train loss=0.2849023640155792, validation loss=0.2649763226509094, c=0.32835351470356977, ibs=0.2439471964725432, ibnll=0.7523312207128777\n",
      "Validation loss decreased (0.264976 --> 0.259132).  Saving model ...                                                   \n",
      "it: 19, train loss=0.2784762680530548, validation loss=0.2591315805912018, c=0.3873565025947476, ibs=0.24139475009083272, ibnll=0.7403193758456948\n",
      "Validation loss decreased (0.259132 --> 0.253538).  Saving model ...                                                   \n",
      "it: 20, train loss=0.2723134458065033, validation loss=0.2535383403301239, c=0.33577606541909105, ibs=0.23886757772478798, ibnll=0.7287393709334986\n",
      "Validation loss decreased (0.253538 --> 0.248192).  Saving model ...                                                   \n",
      "it: 21, train loss=0.2664102017879486, validation loss=0.24819239974021912, c=0.49765686428683753, ibs=0.2363695620673419, ibnll=0.7175891668817631\n",
      "Validation loss decreased (0.248192 --> 0.243094).  Saving model ...                                                   \n",
      "it: 22, train loss=0.26076245307922363, validation loss=0.2430935800075531, c=0.4480578707343922, ibs=0.23390421165709485, ibnll=0.7068652297551524\n",
      "Validation loss decreased (0.243094 --> 0.238230).  Saving model ...                                                   \n",
      "it: 23, train loss=0.25536954402923584, validation loss=0.2382301539182663, c=0.48548513917282593, ibs=0.23147557482954245, ibnll=0.6965650180556235\n",
      "Validation loss decreased (0.238230 --> 0.233607).  Saving model ...                                                   \n",
      "it: 24, train loss=0.25022029876708984, validation loss=0.23360730707645416, c=0.5063374744456676, ibs=0.22908715953345526, ibnll=0.6866840018374211\n",
      "Validation loss decreased (0.233607 --> 0.229210).  Saving model ...                                                   \n",
      "it: 25, train loss=0.24531957507133484, validation loss=0.22920966148376465, c=0.5164963044503853, ibs=0.22674255455333836, ibnll=0.6772174826413764\n",
      "Validation loss decreased (0.229210 --> 0.225041).  Saving model ...                                                   \n",
      "it: 26, train loss=0.2406521439552307, validation loss=0.2250412553548813, c=0.3799339518792263, ibs=0.22444444546520909, ibnll=0.6681582261093315\n",
      "Validation loss decreased (0.225041 --> 0.221090).  Saving model ...                                                   \n",
      "it: 27, train loss=0.23622190952301025, validation loss=0.22108986973762512, c=0.3738323635791791, ibs=0.22219621369724205, ibnll=0.6594998959782982\n",
      "Validation loss decreased (0.221090 --> 0.217350).  Saving model ...                                                   \n",
      "it: 28, train loss=0.23201698064804077, validation loss=0.21735048294067383, c=0.46120459191696805, ibs=0.22000056018416195, ibnll=0.65123465694093\n",
      "Validation loss decreased (0.217350 --> 0.213820).  Saving model ...                                                   \n",
      "it: 29, train loss=0.22803199291229248, validation loss=0.213819682598114, c=0.43972322692247207, ibs=0.21785954273386499, ibnll=0.6433536169330832\n",
      "Validation loss decreased (0.213820 --> 0.210487).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.224263533949852, validation loss=0.21048656105995178, c=0.40622739424437804, ibs=0.21577546153722205, ibnll=0.6358479137522306\n",
      "Validation loss decreased (0.210487 --> 0.207347).  Saving model ...                                                   \n",
      "it: 31, train loss=0.22070057690143585, validation loss=0.20734748244285583, c=0.4651360276773078, ibs=0.2137503811140742, ibnll=0.6287078515854274\n",
      "Validation loss decreased (0.207347 --> 0.204393).  Saving model ...                                                   \n",
      "it: 32, train loss=0.2173394113779068, validation loss=0.2043931931257248, c=0.5709073753734863, ibs=0.21178570043044265, ibnll=0.621923310872244\n",
      "Validation loss decreased (0.204393 --> 0.201618).  Saving model ...                                                   \n",
      "it: 33, train loss=0.21417075395584106, validation loss=0.20161780714988708, c=0.4938826859569115, ibs=0.2098826812735606, ibnll=0.6154839329448738\n",
      "Validation loss decreased (0.201618 --> 0.199014).  Saving model ...                                                   \n",
      "it: 34, train loss=0.21118837594985962, validation loss=0.19901414215564728, c=0.5355244535304293, ibs=0.2080427435760487, ibnll=0.6093793549879839\n",
      "Validation loss decreased (0.199014 --> 0.196576).  Saving model ...                                                   \n",
      "it: 35, train loss=0.208385169506073, validation loss=0.1965755820274353, c=0.5570687214970907, ibs=0.206266179888031, ibnll=0.6035970668215116\n",
      "Validation loss decreased (0.196576 --> 0.194291).  Saving model ...                                                   \n",
      "it: 36, train loss=0.2057541459798813, validation loss=0.19429057836532593, c=0.5796508885044819, ibs=0.20455366574660724, ibnll=0.5981266883103661\n",
      "Validation loss decreased (0.194291 --> 0.192163).  Saving model ...                                                   \n",
      "it: 37, train loss=0.2032833993434906, validation loss=0.1921631544828415, c=0.5832992608900771, ibs=0.2029062751987407, ibnll=0.5929588199863013\n",
      "Validation loss decreased (0.192163 --> 0.190168).  Saving model ...                                                   \n",
      "it: 38, train loss=0.20097793638706207, validation loss=0.19016848504543304, c=0.5609687057713477, ibs=0.20132204876899873, ibnll=0.5880790874512521\n",
      "Validation loss decreased (0.190168 --> 0.188325).  Saving model ...                                                   \n",
      "it: 39, train loss=0.1988103836774826, validation loss=0.18832457065582275, c=0.5559679194841957, ibs=0.19980237229447043, ibnll=0.583477761009276\n",
      "Validation loss decreased (0.188325 --> 0.186593).  Saving model ...                                                   \n",
      "it: 40, train loss=0.19680112600326538, validation loss=0.1865931749343872, c=0.5732347853436075, ibs=0.19834626987898346, ibnll=0.5791415258720857\n",
      "Validation loss decreased (0.186593 --> 0.185000).  Saving model ...                                                   \n",
      "it: 41, train loss=0.19491031765937805, validation loss=0.18499968945980072, c=0.6011322534989778, ibs=0.19695328480236865, ibnll=0.5750584071678924\n",
      "Validation loss decreased (0.185000 --> 0.183509).  Saving model ...                                                   \n",
      "it: 42, train loss=0.19316516816616058, validation loss=0.18350926041603088, c=0.6011951564711433, ibs=0.19562181544279744, ibnll=0.5712188625802723\n",
      "Validation loss decreased (0.183509 --> 0.182136).  Saving model ...                                                   \n",
      "it: 43, train loss=0.19152800738811493, validation loss=0.1821357160806656, c=0.6108507626985376, ibs=0.19435123257219045, ibnll=0.5676120642829185\n",
      "Validation loss decreased (0.182136 --> 0.180859).  Saving model ...                                                   \n",
      "it: 44, train loss=0.19001249969005585, validation loss=0.18085913360118866, c=0.6256329611574147, ibs=0.19314053281162893, ibnll=0.5642242376100218\n",
      "Validation loss decreased (0.180859 --> 0.179683).  Saving model ...                                                   \n",
      "it: 45, train loss=0.18860162794589996, validation loss=0.17968258261680603, c=0.6147192954867118, ibs=0.19198754926706746, ibnll=0.5610449613097558\n",
      "Validation loss decreased (0.179683 --> 0.178594).  Saving model ...                                                   \n",
      "it: 46, train loss=0.187295600771904, validation loss=0.1785936951637268, c=0.6225192640352256, ibs=0.19089384151244684, ibnll=0.5580678288061361\n",
      "Validation loss decreased (0.178594 --> 0.177601).  Saving model ...                                                   \n",
      "it: 47, train loss=0.1860853135585785, validation loss=0.17760100960731506, c=0.5798395974209781, ibs=0.18985352582032136, ibnll=0.5552829512828787\n",
      "Validation loss decreased (0.177601 --> 0.176679).  Saving model ...                                                   \n",
      "it: 48, train loss=0.18496963381767273, validation loss=0.1766790896654129, c=0.5847460292498821, ibs=0.1888652661018417, ibnll=0.552670972416606\n",
      "Validation loss decreased (0.176679 --> 0.175837).  Saving model ...                                                   \n",
      "it: 49, train loss=0.18393337726593018, validation loss=0.17583678662776947, c=0.6169838024846674, ibs=0.18793539244244473, ibnll=0.5502404835651382\n",
      "Validation loss decreased (0.175837 --> 0.175063).  Saving model ...                                                   \n",
      "it: 50, train loss=0.18298493325710297, validation loss=0.1750626564025879, c=0.6244063532001887, ibs=0.18705153041906633, ibnll=0.5479712105573351\n",
      "Validation loss decreased (0.175063 --> 0.174358).  Saving model ...                                                   \n",
      "it: 51, train loss=0.18210354447364807, validation loss=0.17435842752456665, c=0.5803113697122189, ibs=0.18621631317528645, ibnll=0.5458502229022999\n",
      "Validation loss decreased (0.174358 --> 0.173710).  Saving model ...                                                   \n",
      "it: 52, train loss=0.18129780888557434, validation loss=0.1737104058265686, c=0.5553388897625413, ibs=0.18542623491184723, ibnll=0.5438621590646605\n",
      "Validation loss decreased (0.173710 --> 0.173118).  Saving model ...                                                   \n",
      "it: 53, train loss=0.1805533468723297, validation loss=0.1731177717447281, c=0.5953451800597578, ibs=0.1846799126222707, ibnll=0.5420016232278204\n",
      "Validation loss decreased (0.173118 --> 0.172580).  Saving model ...                                                   \n",
      "it: 54, train loss=0.17987121641635895, validation loss=0.17257995903491974, c=0.5896839125648687, ibs=0.18398074202162448, ibnll=0.540280006065523\n",
      "Validation loss decreased (0.172580 --> 0.172094).  Saving model ...                                                   \n",
      "it: 55, train loss=0.17924782633781433, validation loss=0.17209354043006897, c=0.5569743670388426, ibs=0.18331648628628874, ibnll=0.5386705896775101\n",
      "Validation loss decreased (0.172094 --> 0.171649).  Saving model ...                                                   \n",
      "it: 56, train loss=0.17867577075958252, validation loss=0.1716492474079132, c=0.4556691303664098, ibs=0.18269524620045213, ibnll=0.5371787310927824\n",
      "Validation loss decreased (0.171649 --> 0.171246).  Saving model ...                                                   \n",
      "it: 57, train loss=0.1781516820192337, validation loss=0.17124633491039276, c=0.5786129894637522, ibs=0.18210921112382666, ibnll=0.535783999378092\n",
      "Validation loss decreased (0.171246 --> 0.170883).  Saving model ...                                                   \n",
      "it: 58, train loss=0.17767365276813507, validation loss=0.1708826720714569, c=0.5969806573360591, ibs=0.181566888182506, ibnll=0.5345085192626465\n",
      "Validation loss decreased (0.170883 --> 0.170554).  Saving model ...                                                   \n",
      "it: 59, train loss=0.1772376447916031, validation loss=0.17055408656597137, c=0.615128164805787, ibs=0.18104961686035062, ibnll=0.533307636732032\n",
      "Validation loss decreased (0.170554 --> 0.170263).  Saving model ...                                                   \n",
      "it: 60, train loss=0.17683812975883484, validation loss=0.17026261985301971, c=0.5423808774964617, ibs=0.18056681978363387, ibnll=0.5321999239423417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.170263 --> 0.170001).  Saving model ...                                                   \n",
      "it: 61, train loss=0.1764790266752243, validation loss=0.1700010597705841, c=0.515301147979242, ibs=0.18011765520098783, ibnll=0.5311793426095048\n",
      "Validation loss decreased (0.170001 --> 0.169756).  Saving model ...                                                   \n",
      "it: 62, train loss=0.17615236341953278, validation loss=0.1697555035352707, c=0.6110709231011165, ibs=0.1796914734476861, ibnll=0.5302094810620012\n",
      "Validation loss decreased (0.169756 --> 0.169544).  Saving model ...                                                   \n",
      "it: 63, train loss=0.17584918439388275, validation loss=0.1695440709590912, c=0.6051580437175657, ibs=0.17929978944836866, ibnll=0.5293374160920684\n",
      "Validation loss decreased (0.169544 --> 0.169359).  Saving model ...                                                   \n",
      "it: 64, train loss=0.17558042705059052, validation loss=0.16935916244983673, c=0.5703097971379147, ibs=0.1789330097459185, ibnll=0.5285385591284673\n",
      "Validation loss decreased (0.169359 --> 0.169191).  Saving model ...                                                   \n",
      "it: 65, train loss=0.17533691227436066, validation loss=0.16919146478176117, c=0.5769775121874509, ibs=0.1785887051290811, ibnll=0.5277848105384745\n",
      "Validation loss decreased (0.169191 --> 0.169029).  Saving model ...                                                   \n",
      "it: 66, train loss=0.17511726915836334, validation loss=0.16902874410152435, c=0.6395030665198931, ibs=0.17826130687737063, ibnll=0.5270607992313101\n",
      "Validation loss decreased (0.169029 --> 0.168907).  Saving model ...                                                   \n",
      "it: 67, train loss=0.17490774393081665, validation loss=0.16890724003314972, c=0.6135241390155685, ibs=0.17797384182147968, ibnll=0.5264405346762624\n",
      "Validation loss decreased (0.168907 --> 0.168792).  Saving model ...                                                   \n",
      "it: 68, train loss=0.1747378706932068, validation loss=0.16879215836524963, c=0.6100330240603868, ibs=0.1777051112422997, ibnll=0.5258640866071693\n",
      "Validation loss decreased (0.168792 --> 0.168696).  Saving model ...                                                   \n",
      "it: 69, train loss=0.1745797097682953, validation loss=0.16869601607322693, c=0.5703412486239975, ibs=0.1774379466578397, ibnll=0.5253118739139183\n",
      "Validation loss decreased (0.168696 --> 0.168594).  Saving model ...                                                   \n",
      "it: 70, train loss=0.17443732917308807, validation loss=0.1685943603515625, c=0.6094983487969806, ibs=0.17719593269566475, ibnll=0.5248040056860925\n",
      "Validation loss decreased (0.168594 --> 0.168508).  Saving model ...                                                   \n",
      "it: 71, train loss=0.1742953658103943, validation loss=0.1685076653957367, c=0.5810033024060387, ibs=0.17698181075695138, ibnll=0.524325306996758\n",
      "Validation loss decreased (0.168508 --> 0.168446).  Saving model ...                                                   \n",
      "it: 72, train loss=0.17417693138122559, validation loss=0.16844649612903595, c=0.6040257902185878, ibs=0.17679204156750727, ibnll=0.5239229992967934\n",
      "Validation loss decreased (0.168446 --> 0.168421).  Saving model ...                                                   \n",
      "it: 73, train loss=0.17409420013427734, validation loss=0.16842146217823029, c=0.35549614719295486, ibs=0.17662786720256907, ibnll=0.5236295629333597\n",
      "Validation loss decreased (0.168421 --> 0.168377).  Saving model ...                                                   \n",
      "it: 74, train loss=0.1740315854549408, validation loss=0.1683773547410965, c=0.4365466268281176, ibs=0.17645631256948757, ibnll=0.5232993774375191\n",
      "Validation loss decreased (0.168377 --> 0.168327).  Saving model ...                                                   \n",
      "it: 75, train loss=0.17394903302192688, validation loss=0.16832664608955383, c=0.5372542852649788, ibs=0.1762794530250982, ibnll=0.5229316213242565\n",
      "Validation loss decreased (0.168327 --> 0.168294).  Saving model ...                                                   \n",
      "it: 76, train loss=0.17386429011821747, validation loss=0.16829410195350647, c=0.591099229438591, ibs=0.17616013850625628, ibnll=0.5227146685473802\n",
      "Validation loss decreased (0.168294 --> 0.168257).  Saving model ...                                                   \n",
      "it: 77, train loss=0.17380236089229584, validation loss=0.16825710237026215, c=0.328322063217487, ibs=0.17597802648294528, ibnll=0.522342932213803\n",
      "Validation loss decreased (0.168257 --> 0.168235).  Saving model ...                                                   \n",
      "it: 78, train loss=0.173736110329628, validation loss=0.1682347059249878, c=0.5084761754992924, ibs=0.17584117358067744, ibnll=0.52207990796333\n",
      "Validation loss decreased (0.168235 --> 0.168217).  Saving model ...                                                   \n",
      "it: 79, train loss=0.17368599772453308, validation loss=0.1682174950838089, c=0.5455260261047334, ibs=0.1757662612766332, ibnll=0.5219563406513887\n",
      "Validation loss decreased (0.168217 --> 0.168205).  Saving model ...                                                   \n",
      "it: 80, train loss=0.17364320158958435, validation loss=0.16820485889911652, c=0.4740367982387168, ibs=0.1756453485017447, ibnll=0.521772655235052\n",
      "Validation loss decreased (0.168205 --> 0.168178).  Saving model ...                                                   \n",
      "it: 81, train loss=0.1735924929380417, validation loss=0.16817806661128998, c=0.6109136656707029, ibs=0.17555626126157986, ibnll=0.521599943636721\n",
      "Validation loss decreased (0.168178 --> 0.168167).  Saving model ...                                                   \n",
      "it: 82, train loss=0.17354443669319153, validation loss=0.16816680133342743, c=0.571284793206479, ibs=0.1754177123735132, ibnll=0.5212894627776182\n",
      "Validation loss decreased (0.168167 --> 0.168160).  Saving model ...                                                   \n",
      "it: 83, train loss=0.1735135167837143, validation loss=0.16815972328186035, c=0.4128636578078314, ibs=0.17536351758822583, ibnll=0.521203840981071\n",
      "Validation loss decreased (0.168160 --> 0.168151).  Saving model ...                                                   \n",
      "it: 84, train loss=0.17349675297737122, validation loss=0.16815146803855896, c=0.4077685170624312, ibs=0.1753168353102099, ibnll=0.5211669768497166\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 85, train loss=0.1734648495912552, validation loss=0.1681712120771408, c=0.45554332442207895, ibs=0.17525673325687405, ibnll=0.5210456451863429\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 86, train loss=0.17345881462097168, validation loss=0.16818644106388092, c=0.4329611574146878, ibs=0.17525248171370741, ibnll=0.5210898502888899\n",
      "Validation loss decreased (0.168151 --> 0.168147).  Saving model ...                                                   \n",
      "it: 87, train loss=0.17346391081809998, validation loss=0.16814686357975006, c=0.5740525239817581, ibs=0.1751280689701392, ibnll=0.5208174516753301\n",
      "Validation loss decreased (0.168147 --> 0.168125).  Saving model ...                                                   \n",
      "it: 88, train loss=0.1734095811843872, validation loss=0.16812530159950256, c=0.593206479006133, ibs=0.17506169029823754, ibnll=0.520702363649459\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 89, train loss=0.17336171865463257, validation loss=0.16815467178821564, c=0.3963516276144048, ibs=0.17501667635046203, ibnll=0.5206676456073934\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 90, train loss=0.17336925864219666, validation loss=0.16815413534641266, c=0.31586727472873094, ibs=0.17497309713952613, ibnll=0.5205894183323923\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.17335879802703857, validation loss=0.16813994944095612, c=0.4327724484981915, ibs=0.17489889557566587, ibnll=0.5204152300447697\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 92, train loss=0.17334237694740295, validation loss=0.16814233362674713, c=0.38521780154112284, ibs=0.1748820115691329, ibnll=0.5204245243222355\n",
      "EarlyStopping counter: 5 out of 20                                                                                     \n",
      "it: 93, train loss=0.17332902550697327, validation loss=0.16815394163131714, c=0.44022645069979555, ibs=0.17490259907465572, ibnll=0.5205435782445875\n",
      "EarlyStopping counter: 6 out of 20                                                                                     \n",
      "it: 94, train loss=0.173318549990654, validation loss=0.1681586503982544, c=0.4350684069822299, ibs=0.1748474434216072, ibnll=0.5203771766878585\n",
      "EarlyStopping counter: 7 out of 20                                                                                     \n",
      "it: 95, train loss=0.17332544922828674, validation loss=0.16814693808555603, c=0.4634690989149237, ibs=0.1748363048666435, ibnll=0.5203546629601804\n",
      "EarlyStopping counter: 8 out of 20                                                                                     \n",
      "it: 96, train loss=0.17330767214298248, validation loss=0.168134868144989, c=0.5083818210410442, ibs=0.1748280737021085, ibnll=0.5203425707877223\n",
      "Validation loss decreased (0.168125 --> 0.168121).  Saving model ...                                                   \n",
      "it: 97, train loss=0.1732919067144394, validation loss=0.16812126338481903, c=0.563264664255386, ibs=0.17482469718812146, ibnll=0.5203411526086562\n",
      "Validation loss decreased (0.168121 --> 0.168118).  Saving model ...                                                   \n",
      "it: 98, train loss=0.17327187955379486, validation loss=0.1681184470653534, c=0.5870419877339205, ibs=0.17485520487627312, ibnll=0.5204157538962131\n",
      "Validation loss decreased (0.168118 --> 0.168118).  Saving model ...                                                   \n",
      "it: 99, train loss=0.17326916754245758, validation loss=0.16811800003051758, c=0.5817895895581067, ibs=0.1748802131350473, ibnll=0.5204940880667575\n",
      "Validation loss decreased (0.168118 --> 0.168106).  Saving model ...                                                   \n",
      "it: 100, train loss=0.17326004803180695, validation loss=0.16810569167137146, c=0.5792105676993238, ibs=0.17490420302314785, ibnll=0.5205638479725786\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 101, train loss=0.17324301600456238, validation loss=0.16812758147716522, c=0.5687686743198617, ibs=0.17492027351363237, ibnll=0.5206299950628346\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 102, train loss=0.17324750125408173, validation loss=0.1681249439716339, c=0.5685799654033653, ibs=0.174920101325126, ibnll=0.5206384577820605\n",
      "Validation loss decreased (0.168106 --> 0.168091).  Saving model ...                                                   \n",
      "it: 103, train loss=0.17324309051036835, validation loss=0.16809141635894775, c=0.5741154269539236, ibs=0.17489209307537756, ibnll=0.5205648706824396\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 104, train loss=0.17321494221687317, validation loss=0.16809600591659546, c=0.5838339361534832, ibs=0.17489297548338636, ibnll=0.5205601581660775\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 105, train loss=0.17322224378585815, validation loss=0.1680970937013626, c=0.5827016826545054, ibs=0.17489146182750237, ibnll=0.5205520275768928\n",
      "Validation loss decreased (0.168091 --> 0.168077).  Saving model ...                                                   \n",
      "it: 106, train loss=0.17322224378585815, validation loss=0.1680774986743927, c=0.5843371599308067, ibs=0.17487737904437434, ibnll=0.5205128977466142\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 107, train loss=0.17320133745670319, validation loss=0.16808100044727325, c=0.5754363893693977, ibs=0.17488217310394363, ibnll=0.5205294695825965\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 108, train loss=0.17319506406784058, validation loss=0.16808940470218658, c=0.5683912564868691, ibs=0.17490014655395267, ibnll=0.52057740179065\n",
      "Validation loss decreased (0.168077 --> 0.168074).  Saving model ...                                                   \n",
      "it: 109, train loss=0.1731969118118286, validation loss=0.16807445883750916, c=0.5781097656864287, ibs=0.17490846640801264, ibnll=0.5206001399207283\n",
      "Validation loss decreased (0.168074 --> 0.168068).  Saving model ...                                                   \n",
      "it: 110, train loss=0.17318251729011536, validation loss=0.16806794703006744, c=0.5877653719138229, ibs=0.17492063550904668, ibnll=0.5206413078824806\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 111, train loss=0.17317821085453033, validation loss=0.16806979477405548, c=0.5930177700896367, ibs=0.1749292133319574, ibnll=0.5206773597156102\n",
      "Validation loss decreased (0.168068 --> 0.168064).  Saving model ...                                                   \n",
      "it: 112, train loss=0.17317649722099304, validation loss=0.16806361079216003, c=0.5957225978927504, ibs=0.17492418542423865, ibnll=0.5206705302147151\n",
      "Validation loss decreased (0.168064 --> 0.168062).  Saving model ...                                                   \n",
      "it: 113, train loss=0.17316727340221405, validation loss=0.1680619865655899, c=0.5954709860040887, ibs=0.1749227599661037, ibnll=0.5206666889742527\n",
      "Validation loss decreased (0.168062 --> 0.168059).  Saving model ...                                                   \n",
      "it: 114, train loss=0.17316538095474243, validation loss=0.1680593639612198, c=0.5947161503381034, ibs=0.17492073022279583, ibnll=0.5206606030739105\n",
      "Validation loss decreased (0.168059 --> 0.168056).  Saving model ...                                                   \n",
      "it: 115, train loss=0.17316249012947083, validation loss=0.16805621981620789, c=0.5944645384494417, ibs=0.17491859435307225, ibnll=0.5206535594842744\n",
      "Validation loss decreased (0.168056 --> 0.168053).  Saving model ...                                                   \n",
      "it: 116, train loss=0.17315909266471863, validation loss=0.1680528223514557, c=0.5945588929076899, ibs=0.17491686694280043, ibnll=0.5206468454987015\n",
      "Validation loss decreased (0.168053 --> 0.168050).  Saving model ...                                                   \n",
      "it: 117, train loss=0.17315542697906494, validation loss=0.16804952919483185, c=0.5956282434345023, ibs=0.17491588938916638, ibnll=0.5206412429094831\n",
      "Validation loss decreased (0.168050 --> 0.168047).  Saving model ...                                                   \n",
      "it: 118, train loss=0.17315180599689484, validation loss=0.16804657876491547, c=0.5942443780468627, ibs=0.17491614941828665, ibnll=0.520637917682689\n",
      "Validation loss decreased (0.168047 --> 0.168044).  Saving model ...                                                   \n",
      "it: 119, train loss=0.17314831912517548, validation loss=0.16804419457912445, c=0.5927347067148923, ibs=0.17491806105111746, ibnll=0.5206381717999385\n",
      "Validation loss decreased (0.168044 --> 0.168043).  Saving model ...                                                   \n",
      "it: 120, train loss=0.17314551770687103, validation loss=0.16804319620132446, c=0.5931750275200504, ibs=0.17492153061607718, ibnll=0.5206425116115933\n",
      "Validation loss decreased (0.168043 --> 0.168043).  Saving model ...                                                   \n",
      "it: 121, train loss=0.17314448952674866, validation loss=0.16804315149784088, c=0.592986318603554, ibs=0.1749218732725282, ibnll=0.5206430172106403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.168043 --> 0.168043).  Saving model ...                                                   \n",
      "it: 122, train loss=0.17314445972442627, validation loss=0.16804301738739014, c=0.5927976096870577, ibs=0.17492212591670378, ibnll=0.5206433320206882\n",
      "Validation loss decreased (0.168043 --> 0.168043).  Saving model ...                                                   \n",
      "it: 123, train loss=0.1731443554162979, validation loss=0.1680428683757782, c=0.5929234156313886, ibs=0.17492229564483133, ibnll=0.5206435032733865\n",
      "Validation loss decreased (0.168043 --> 0.168043).  Saving model ...                                                   \n",
      "it: 124, train loss=0.17314423620700836, validation loss=0.16804258525371552, c=0.5929548671174713, ibs=0.17492239413207797, ibnll=0.5206435245512827\n",
      "Validation loss decreased (0.168043 --> 0.168042).  Saving model ...                                                   \n",
      "it: 125, train loss=0.17314404249191284, validation loss=0.16804228723049164, c=0.5928605126592231, ibs=0.17492239624698502, ibnll=0.5206433653655802\n",
      "Validation loss decreased (0.168042 --> 0.168042).  Saving model ...                                                   \n",
      "it: 126, train loss=0.17314381897449493, validation loss=0.16804192960262299, c=0.5930492215757195, ibs=0.17492233454762443, ibnll=0.5206430718885258\n",
      "Validation loss decreased (0.168042 --> 0.168042).  Saving model ...                                                   \n",
      "it: 127, train loss=0.17314353585243225, validation loss=0.16804154217243195, c=0.5931435760339676, ibs=0.17492221245669512, ibnll=0.520642668011399\n",
      "Validation loss decreased (0.168042 --> 0.168041).  Saving model ...                                                   \n",
      "it: 128, train loss=0.17314322292804718, validation loss=0.1680411547422409, c=0.5934580908947947, ibs=0.1749220364655938, ibnll=0.5206421719073322\n",
      "Validation loss decreased (0.168041 --> 0.168041).  Saving model ...                                                   \n",
      "it: 129, train loss=0.17314289510250092, validation loss=0.16804076731204987, c=0.5936467998112911, ibs=0.17492182195338318, ibnll=0.5206416120265352\n",
      "Validation loss decreased (0.168041 --> 0.168040).  Saving model ...                                                   \n",
      "it: 130, train loss=0.17314255237579346, validation loss=0.16804032027721405, c=0.5939613146721182, ibs=0.17492156841434073, ibnll=0.5206409730156841\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 131, train loss=0.1731422245502472, validation loss=0.168039932847023, c=0.5941814750746973, ibs=0.17492128689569225, ibnll=0.5206403028477286\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 132, train loss=0.17314186692237854, validation loss=0.16803990304470062, c=0.5940242176442837, ibs=0.17492125116718987, ibnll=0.5206402163347039\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 133, train loss=0.17314183712005615, validation loss=0.16803990304470062, c=0.5940242176442837, ibs=0.1749212270614114, ibnll=0.5206401699219873\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 134, train loss=0.17314179241657257, validation loss=0.16803982853889465, c=0.5940556691303664, ibs=0.1749211857185195, ibnll=0.5206400678285753\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 135, train loss=0.17314176261425018, validation loss=0.16803976893424988, c=0.593992766158201, ibs=0.1749211414884154, ibnll=0.5206399793492551\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 136, train loss=0.1731417179107666, validation loss=0.16803975403308868, c=0.593992766158201, ibs=0.1749211059848496, ibnll=0.520639907683653\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 137, train loss=0.1731416881084442, validation loss=0.1680396944284439, c=0.5938669602138701, ibs=0.17492107338855653, ibnll=0.5206398369626069\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 138, train loss=0.17314167320728302, validation loss=0.16803966462612152, c=0.5940242176442837, ibs=0.17492106627199713, ibnll=0.5206398186449234\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 139, train loss=0.17314164340496063, validation loss=0.16803966462612152, c=0.5940242176442837, ibs=0.17492105581900602, ibnll=0.5206397962235831\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 140, train loss=0.17314162850379944, validation loss=0.16803967952728271, c=0.5939613146721182, ibs=0.17492106890932438, ibnll=0.5206398205260542\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 141, train loss=0.17314162850379944, validation loss=0.16803964972496033, c=0.5939298631860356, ibs=0.17492105987971843, ibnll=0.5206397995202908\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 142, train loss=0.17314161360263824, validation loss=0.16803964972496033, c=0.5940242176442837, ibs=0.17492105367044639, ibnll=0.5206397854917104\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 143, train loss=0.17314162850379944, validation loss=0.16803964972496033, c=0.5939613146721182, ibs=0.17492105172686542, ibnll=0.5206397851598452\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 144, train loss=0.17314162850379944, validation loss=0.16803964972496033, c=0.5938984116999528, ibs=0.1749210495278645, ibnll=0.5206397822246674\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 145, train loss=0.17314159870147705, validation loss=0.16803964972496033, c=0.5940556691303664, ibs=0.1749210440714508, ibnll=0.5206397662196167\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 146, train loss=0.17314162850379944, validation loss=0.16803964972496033, c=0.5941185721025318, ibs=0.17492103693262193, ibnll=0.5206397523849431\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 147, train loss=0.17314161360263824, validation loss=0.16803961992263794, c=0.5940871206164491, ibs=0.17492102864550454, ibnll=0.5206397338906965\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 148, train loss=0.17314159870147705, validation loss=0.16803963482379913, c=0.5939613146721182, ibs=0.1749210281363797, ibnll=0.5206397386379255\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 149, train loss=0.17314159870147705, validation loss=0.16803961992263794, c=0.5940242176442837, ibs=0.174921029246209, ibnll=0.5206397359875722\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 150, train loss=0.17314158380031586, validation loss=0.16803961992263794, c=0.5940556691303664, ibs=0.17492101522601433, ibnll=0.5206397054817988\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 151, train loss=0.17314158380031586, validation loss=0.16803961992263794, c=0.5940242176442837, ibs=0.1749210204948695, ibnll=0.5206397245360791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 152, train loss=0.17314159870147705, validation loss=0.16803964972496033, c=0.5939613146721182, ibs=0.1749210189360962, ibnll=0.5206397185614865\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 153, train loss=0.17314156889915466, validation loss=0.16803963482379913, c=0.5937411542695392, ibs=0.17492101523477727, ibnll=0.5206397129810643\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 154, train loss=0.17314158380031586, validation loss=0.16803961992263794, c=0.593992766158201, ibs=0.1749210087165389, ibnll=0.5206396976058102\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 155, train loss=0.17314155399799347, validation loss=0.16803961992263794, c=0.5940242176442837, ibs=0.17492100983682016, ibnll=0.520639698559621\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 156, train loss=0.17314156889915466, validation loss=0.16803960502147675, c=0.5939613146721182, ibs=0.1749210080716559, ibnll=0.5206396961730633\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 157, train loss=0.17314156889915466, validation loss=0.16803960502147675, c=0.5940242176442837, ibs=0.1749210123050575, ibnll=0.5206397019910465\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 158, train loss=0.17314156889915466, validation loss=0.16803957521915436, c=0.5941185721025318, ibs=0.17492099568643654, ibnll=0.5206396621412734\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 159, train loss=0.17314153909683228, validation loss=0.16803960502147675, c=0.5941500235886146, ibs=0.1749209926801858, ibnll=0.5206396531946857\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 160, train loss=0.17314156889915466, validation loss=0.16803960502147675, c=0.5940871206164491, ibs=0.1749209836395779, ibnll=0.5206396299115306\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 161, train loss=0.17314155399799347, validation loss=0.16803957521915436, c=0.59421292656078, ibs=0.17492098783199272, ibnll=0.5206396476616153\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 162, train loss=0.17314153909683228, validation loss=0.16803957521915436, c=0.5940242176442837, ibs=0.17492098788725186, ibnll=0.5206396449972708\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 163, train loss=0.17314153909683228, validation loss=0.16803957521915436, c=0.5941500235886146, ibs=0.174920982314298, ibnll=0.5206396354134365\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 164, train loss=0.17314153909683228, validation loss=0.16803960502147675, c=0.5941814750746973, ibs=0.17492098032535064, ibnll=0.5206396281490733\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 165, train loss=0.17314155399799347, validation loss=0.16803954541683197, c=0.5940556691303664, ibs=0.1749209629005745, ibnll=0.5206395963961938\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 166, train loss=0.17314153909683228, validation loss=0.16803954541683197, c=0.5941814750746973, ibs=0.17492096153564818, ibnll=0.520639595168863\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 167, train loss=0.17314152419567108, validation loss=0.16803954541683197, c=0.5941185721025318, ibs=0.17492096799806364, ibnll=0.5206396081873222\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 168, train loss=0.17314152419567108, validation loss=0.16803954541683197, c=0.5939298631860356, ibs=0.1749209548893812, ibnll=0.5206395819419551\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 169, train loss=0.1731415092945099, validation loss=0.16803954541683197, c=0.5940556691303664, ibs=0.17492095589006126, ibnll=0.5206395825884171\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 170, train loss=0.17314152419567108, validation loss=0.16803954541683197, c=0.5940871206164491, ibs=0.1749209444438012, ibnll=0.5206395598938519\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 171, train loss=0.1731414943933487, validation loss=0.16803953051567078, c=0.5939298631860356, ibs=0.17492094271121505, ibnll=0.5206395488479723\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 172, train loss=0.1731415092945099, validation loss=0.16803954541683197, c=0.5941185721025318, ibs=0.17492093812850137, ibnll=0.520639544025914\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 173, train loss=0.1731415092945099, validation loss=0.16803953051567078, c=0.593992766158201, ibs=0.17492093128845443, ibnll=0.5206395263129665\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 174, train loss=0.1731414943933487, validation loss=0.16803951561450958, c=0.5938984116999528, ibs=0.17492093327285974, ibnll=0.5206395372530104\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 175, train loss=0.1731414943933487, validation loss=0.16803951561450958, c=0.5941814750746973, ibs=0.17492092434526693, ibnll=0.5206395150744985\n",
      "Validation loss decreased (0.168040 --> 0.168040).  Saving model ...                                                   \n",
      "it: 176, train loss=0.1731414943933487, validation loss=0.1680395007133484, c=0.5941185721025318, ibs=0.17492091736025267, ibnll=0.5206395017011245\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 177, train loss=0.1731415092945099, validation loss=0.16803953051567078, c=0.5940556691303664, ibs=0.17492091961170333, ibnll=0.5206395026829892\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 178, train loss=0.1731414943933487, validation loss=0.16803953051567078, c=0.593992766158201, ibs=0.17492091556575023, ibnll=0.5206394985037868\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 179, train loss=0.1731414943933487, validation loss=0.16803951561450958, c=0.5941185721025318, ibs=0.1749209114801682, ibnll=0.5206394907049069\n",
      "Validation loss decreased (0.168040 --> 0.168039).  Saving model ...                                                   \n",
      "it: 180, train loss=0.1731414943933487, validation loss=0.168039470911026, c=0.593992766158201, ibs=0.17492090371399174, ibnll=0.5206394797787193\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 181, train loss=0.1731414794921875, validation loss=0.1680395007133484, c=0.5939298631860356, ibs=0.1749209029699176, ibnll=0.5206394785248437\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 182, train loss=0.1731414943933487, validation loss=0.1680395007133484, c=0.5939613146721182, ibs=0.17492090018836803, ibnll=0.520639472187227\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 183, train loss=0.1731414943933487, validation loss=0.16803953051567078, c=0.593992766158201, ibs=0.1749208931891914, ibnll=0.5206394525980829\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 184, train loss=0.1731414496898651, validation loss=0.168039470911026, c=0.5939613146721182, ibs=0.17492089303653066, ibnll=0.5206394625943891\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 185, train loss=0.1731414496898651, validation loss=0.168039470911026, c=0.5940242176442837, ibs=0.17492088279594056, ibnll=0.5206394387938489\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 186, train loss=0.1731414496898651, validation loss=0.1680395007133484, c=0.593992766158201, ibs=0.1749208867867594, ibnll=0.5206394444004467\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 187, train loss=0.1731414496898651, validation loss=0.1680395007133484, c=0.5940242176442837, ibs=0.17492088177499643, ibnll=0.5206394360647626\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 188, train loss=0.1731414496898651, validation loss=0.1680394560098648, c=0.593992766158201, ibs=0.1749208661168871, ibnll=0.5206393989958452\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 189, train loss=0.1731414496898651, validation loss=0.1680394858121872, c=0.5939298631860356, ibs=0.17492087008501267, ibnll=0.5206394080783657\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 190, train loss=0.1731414645910263, validation loss=0.168039470911026, c=0.5939613146721182, ibs=0.17492085986910813, ibnll=0.5206393819609901\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 191, train loss=0.17314141988754272, validation loss=0.1680394560098648, c=0.593772605755622, ibs=0.17492086745414814, ibnll=0.5206394064730904\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 192, train loss=0.17314141988754272, validation loss=0.168039470911026, c=0.5940871206164491, ibs=0.17492086500322, ibnll=0.5206394079665616\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 193, train loss=0.17314141988754272, validation loss=0.1680394560098648, c=0.5940242176442837, ibs=0.1749208539047132, ibnll=0.5206393765383674\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 194, train loss=0.17314141988754272, validation loss=0.1680394560098648, c=0.5940242176442837, ibs=0.17492085120164105, ibnll=0.5206393715517124\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 195, train loss=0.1731414496898651, validation loss=0.16803942620754242, c=0.5939613146721182, ibs=0.17492084404341338, ibnll=0.5206393538943542\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 196, train loss=0.17314141988754272, validation loss=0.16803942620754242, c=0.5938355087277873, ibs=0.17492083594186061, ibnll=0.520639340815781\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 197, train loss=0.17314141988754272, validation loss=0.16803942620754242, c=0.5939298631860356, ibs=0.1749208345462197, ibnll=0.5206393312944627\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 198, train loss=0.17314141988754272, validation loss=0.16803942620754242, c=0.5940871206164491, ibs=0.17492082906682707, ibnll=0.5206393205695818\n",
      "Validation loss decreased (0.168039 --> 0.168039).  Saving model ...                                                   \n",
      "it: 199, train loss=0.17314141988754272, validation loss=0.16803939640522003, c=0.5938669602138701, ibs=0.17492082916074414, ibnll=0.520639323831529\n",
      "                                                                                                                       \n",
      "Results: 0.5676693977632916, 0.18425607473092043, 0.5438057255940589\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 697, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 2, 'num_latent': 147, 'num_odefunc_layers1': 3, 'odefunc_neurons1': 286, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 1.0, 'weight_decay': 0.0001}\n",
      "  1%|                                      | 1/100 [1:58:36<195:42:59, 7116.96s/trial, best loss: 0.16803939640522003]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a122ec54551e4105a28a3c10561bfb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.268016).  Saving model ...                                                        \n",
      "it: 0, train loss=0.28365153074264526, validation loss=0.26801562309265137, c=0.4778272998364703, ibs=0.2433038298878813, ibnll=0.6741336855645694\n",
      "Validation loss decreased (0.268016 --> 0.258524).  Saving model ...                                                   \n",
      "it: 1, train loss=0.27343636751174927, validation loss=0.25852420926094055, c=0.4649693782665854, ibs=0.2342575429789681, ibnll=0.6543262784818654\n",
      "Validation loss decreased (0.258524 --> 0.245797).  Saving model ...                                                   \n",
      "it: 2, train loss=0.26387307047843933, validation loss=0.24579688906669617, c=0.41735338442299674, ibs=0.22220770136867987, ibnll=0.6283975421581239\n",
      "Validation loss decreased (0.245797 --> 0.233190).  Saving model ...                                                   \n",
      "it: 3, train loss=0.2510251998901367, validation loss=0.2331899255514145, c=0.4497707378074198, ibs=0.210347792568991, ibnll=0.6031656637231435\n",
      "Validation loss decreased (0.233190 --> 0.222130).  Saving model ...                                                   \n",
      "it: 4, train loss=0.2382628321647644, validation loss=0.22213006019592285, c=0.4794946612370539, ibs=0.2001213813559169, ibnll=0.5815310115589979\n",
      "Validation loss decreased (0.222130 --> 0.211673).  Saving model ...                                                   \n",
      "it: 5, train loss=0.22699174284934998, validation loss=0.21167337894439697, c=0.48921024785968514, ibs=0.19074884042925783, ibnll=0.5616471667188011\n",
      "Validation loss decreased (0.211673 --> 0.202218).  Saving model ...                                                   \n",
      "it: 6, train loss=0.2164166122674942, validation loss=0.2022184431552887, c=0.4100105813319652, ibs=0.18259422130647207, ibnll=0.5441436493584887\n",
      "Validation loss decreased (0.202218 --> 0.193930).  Saving model ...                                                   \n",
      "it: 7, train loss=0.20682841539382935, validation loss=0.19393034279346466, c=0.4813864751338699, ibs=0.17580748380887726, ibnll=0.529245485236604\n",
      "Validation loss decreased (0.193930 --> 0.187126).  Saving model ...                                                   \n",
      "it: 8, train loss=0.1983436495065689, validation loss=0.187126025557518, c=0.5418603905473435, ibs=0.17064264536727558, ibnll=0.5174936963673167\n",
      "Validation loss decreased (0.187126 --> 0.181659).  Saving model ...                                                   \n",
      "it: 9, train loss=0.19139453768730164, validation loss=0.18165946006774902, c=0.4264276781992497, ibs=0.1669006646944313, ibnll=0.5085060547852446\n",
      "Validation loss decreased (0.181659 --> 0.177315).  Saving model ...                                                   \n",
      "it: 10, train loss=0.18578560650348663, validation loss=0.17731508612632751, c=0.5382691506076249, ibs=0.16433330406169677, ibnll=0.5018119851659323\n",
      "Validation loss decreased (0.177315 --> 0.174131).  Saving model ...                                                   \n",
      "it: 11, train loss=0.18130548298358917, validation loss=0.17413055896759033, c=0.5678968801103024, ibs=0.16285705628148275, ibnll=0.4973585119149861\n",
      "Validation loss decreased (0.174131 --> 0.171910).  Saving model ...                                                   \n",
      "it: 12, train loss=0.17799167335033417, validation loss=0.1719096601009369, c=0.48295764260749674, ibs=0.1622461900367265, ibnll=0.494739817335691\n",
      "Validation loss decreased (0.171910 --> 0.170498).  Saving model ...                                                   \n",
      "it: 13, train loss=0.17565098404884338, validation loss=0.17049843072891235, c=0.46028794048802385, ibs=0.16229880093928117, ibnll=0.4936140246383217\n",
      "Validation loss decreased (0.170498 --> 0.169750).  Saving model ...                                                   \n",
      "it: 14, train loss=0.17413341999053955, validation loss=0.16975031793117523, c=0.4704844967454388, ibs=0.16283827854673888, ibnll=0.4936580780574376\n",
      "Validation loss decreased (0.169750 --> 0.169524).  Saving model ...                                                   \n",
      "it: 15, train loss=0.1732906550168991, validation loss=0.16952365636825562, c=0.5805303491839549, ibs=0.16370901620392575, ibnll=0.4945759321484869\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 16, train loss=0.17297960817813873, validation loss=0.16967526078224182, c=0.5879052169173052, ibs=0.16476188740068995, ibnll=0.4960649071655631\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 17, train loss=0.17305970191955566, validation loss=0.1700897216796875, c=0.5933562061115208, ibs=0.16591030011903266, ibnll=0.49792886068962927\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 18, train loss=0.1734023541212082, validation loss=0.17065924406051636, c=0.5916247154262995, ibs=0.16704528590937767, ibnll=0.499921321351196\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 19, train loss=0.1739233136177063, validation loss=0.17130312323570251, c=0.5713277968384263, ibs=0.16811618231244624, ibnll=0.5019124366798747\n",
      "EarlyStopping counter: 5 out of 20                                                                                     \n",
      "it: 20, train loss=0.17451906204223633, validation loss=0.1719421148300171, c=0.594767050373553, ibs=0.16908581657382638, ibnll=0.5037733113681325\n",
      "EarlyStopping counter: 6 out of 20                                                                                     \n",
      "it: 21, train loss=0.17511950433254242, validation loss=0.1725195348262787, c=0.5852759162471542, ibs=0.1698988284528232, ibnll=0.5053794752502607\n",
      "EarlyStopping counter: 7 out of 20                                                                                     \n",
      "it: 22, train loss=0.1756649613380432, validation loss=0.17256391048431396, c=0.586141661589765, ibs=0.16996148257334942, ibnll=0.5055045616280446\n",
      "EarlyStopping counter: 8 out of 20                                                                                     \n",
      "it: 23, train loss=0.17570650577545166, validation loss=0.172592893242836, c=0.5883220572674511, ibs=0.17000549474904367, ibnll=0.5055924718965936\n",
      "EarlyStopping counter: 9 out of 20                                                                                     \n",
      "it: 24, train loss=0.1757330745458603, validation loss=0.17260795831680298, c=0.5903421297335428, ibs=0.170032177159954, ibnll=0.5056458423039819\n",
      "EarlyStopping counter: 10 out of 20                                                                                    \n",
      "it: 25, train loss=0.17574599385261536, validation loss=0.17261067032814026, c=0.5859492737358515, ibs=0.170042567748705, ibnll=0.5056664415990769\n",
      "EarlyStopping counter: 11 out of 20                                                                                    \n",
      "it: 26, train loss=0.17574770748615265, validation loss=0.17260564863681793, c=0.5870394715746946, ibs=0.17003828793899792, ibnll=0.5056573627979537\n",
      "EarlyStopping counter: 12 out of 20                                                                                    \n",
      "it: 27, train loss=0.1757422834634781, validation loss=0.17259320616722107, c=0.5801455734761278, ibs=0.17002141596940323, ibnll=0.5056231448443345\n",
      "EarlyStopping counter: 13 out of 20                                                                                    \n",
      "it: 28, train loss=0.17572927474975586, validation loss=0.17259100079536438, c=0.578959181710328, ibs=0.17001855538616426, ibnll=0.5056173730351504\n",
      "EarlyStopping counter: 14 out of 20                                                                                    \n",
      "it: 29, train loss=0.1757269948720932, validation loss=0.1725877821445465, c=0.5797607977683009, ibs=0.17001459261929264, ibnll=0.5056093771811271\n",
      "EarlyStopping counter: 15 out of 20                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.17572376132011414, validation loss=0.1725836992263794, c=0.5799531856222144, ibs=0.17000963837349176, ibnll=0.5055993596490447\n",
      "EarlyStopping counter: 16 out of 20                                                                                    \n",
      "it: 31, train loss=0.17571961879730225, validation loss=0.17257873713970184, c=0.5791195049219225, ibs=0.17000372578339226, ibnll=0.5055874094939448\n",
      "EarlyStopping counter: 17 out of 20                                                                                    \n",
      "it: 32, train loss=0.17571468651294708, validation loss=0.1725730448961258, c=0.5788309231410523, ibs=0.16999695635970657, ibnll=0.5055737147982483\n",
      "EarlyStopping counter: 18 out of 20                                                                                    \n",
      "it: 33, train loss=0.17570903897285461, validation loss=0.17256669700145721, c=0.5780293070830795, ibs=0.16998937816603527, ibnll=0.5055583443338194\n",
      "EarlyStopping counter: 19 out of 20                                                                                    \n",
      "it: 34, train loss=0.1757027804851532, validation loss=0.1725659817457199, c=0.5760412992593068, ibs=0.1699885476029182, ibnll=0.5055566619761414\n",
      "EarlyStopping counter: 20 out of 20                                                                                    \n",
      "it: 35, train loss=0.1757020354270935, validation loss=0.17256523668766022, c=0.5764902042517716, ibs=0.16998765065222135, ibnll=0.5055548494926003\n",
      "Early stopping                                                                                                         \n",
      "                                                                                                                       \n",
      "Results: 0.5733110061200486, 0.1917256047747439, 0.5620991959580386\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 368, 'lr': 0.0001, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 3, 'num_latent': 108, 'num_odefunc_layers1': 2, 'odefunc_neurons1': 741, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.0001}\n",
      "  2%|                                      | 2/100 [2:22:03<102:18:11, 3758.08s/trial, best loss: 0.16803939640522003]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9714ddb293604ef6acc519634bd07fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.456188).  Saving model ...                                                        \n",
      "it: 0, train loss=0.42666083574295044, validation loss=0.4561879336833954, c=0.48797483966452887, ibs=0.3133387881183362, ibnll=1.1618860207373418\n",
      "Validation loss decreased (0.456188 --> 0.453868).  Saving model ...                                                   \n",
      "it: 1, train loss=0.42451268434524536, validation loss=0.45386800169944763, c=0.5230019733596448, ibs=0.3128141345765937, ibnll=1.157485980767316\n",
      "Validation loss decreased (0.453868 --> 0.451506).  Saving model ...                                                   \n",
      "it: 2, train loss=0.42230093479156494, validation loss=0.45150643587112427, c=0.5094351258016774, ibs=0.31227372612519544, ibnll=1.1529746262468736\n",
      "Validation loss decreased (0.451506 --> 0.449064).  Saving model ...                                                   \n",
      "it: 3, train loss=0.42006081342697144, validation loss=0.44906413555145264, c=0.5142760236803158, ibs=0.3117180651657439, ibnll=1.1483477769726569\n",
      "Validation loss decreased (0.449064 --> 0.446562).  Saving model ...                                                   \n",
      "it: 4, train loss=0.4177406132221222, validation loss=0.4465622901916504, c=0.5389121854958067, ibs=0.31114948582656693, ibnll=1.1436309708741077\n",
      "Validation loss decreased (0.446562 --> 0.444047).  Saving model ...                                                   \n",
      "it: 5, train loss=0.41536566615104675, validation loss=0.44404736161231995, c=0.5439689195855945, ibs=0.3105682546799184, ibnll=1.138818686637889\n",
      "Validation loss decreased (0.444047 --> 0.441479).  Saving model ...                                                   \n",
      "it: 6, train loss=0.41297754645347595, validation loss=0.4414789080619812, c=0.5317895905278737, ibs=0.3099728135104315, ibnll=1.13389908180552\n",
      "Validation loss decreased (0.441479 --> 0.438851).  Saving model ...                                                   \n",
      "it: 7, train loss=0.4105437695980072, validation loss=0.438850998878479, c=0.50832511100148, ibs=0.30936325154172456, ibnll=1.128880657872994\n",
      "Validation loss decreased (0.438851 --> 0.436165).  Saving model ...                                                   \n",
      "it: 8, train loss=0.4080527424812317, validation loss=0.43616506457328796, c=0.5210286137148495, ibs=0.3087378611116218, ibnll=1.1237518582931183\n",
      "Validation loss decreased (0.436165 --> 0.433441).  Saving model ...                                                   \n",
      "it: 9, train loss=0.40550994873046875, validation loss=0.4334411919116974, c=0.5372163295510607, ibs=0.3080973982968061, ibnll=1.1185303749007631\n",
      "Validation loss decreased (0.433441 --> 0.430683).  Saving model ...                                                   \n",
      "it: 10, train loss=0.4029379189014435, validation loss=0.4306833744049072, c=0.5382338431179082, ibs=0.3074432760041479, ibnll=1.113208728136705\n",
      "Validation loss decreased (0.430683 --> 0.427895).  Saving model ...                                                   \n",
      "it: 11, train loss=0.4003349542617798, validation loss=0.4278949499130249, c=0.5345646275283671, ibs=0.30677878090188704, ibnll=1.107827706968294\n",
      "Validation loss decreased (0.427895 --> 0.425061).  Saving model ...                                                   \n",
      "it: 12, train loss=0.3977031111717224, validation loss=0.4250612258911133, c=0.5220461272816971, ibs=0.3060984565895245, ibnll=1.1023525865106052\n",
      "Validation loss decreased (0.425061 --> 0.422208).  Saving model ...                                                   \n",
      "it: 13, train loss=0.3950291574001312, validation loss=0.422208309173584, c=0.5272878638381845, ibs=0.3054076047448725, ibnll=1.0968187275549572\n",
      "Validation loss decreased (0.422208 --> 0.419341).  Saving model ...                                                   \n",
      "it: 14, train loss=0.39233919978141785, validation loss=0.41934120655059814, c=0.5347496299950666, ibs=0.3047082571380871, ibnll=1.0912404471941195\n",
      "Validation loss decreased (0.419341 --> 0.416451).  Saving model ...                                                   \n",
      "it: 15, train loss=0.38964298367500305, validation loss=0.41645100712776184, c=0.5354896398618648, ibs=0.3039975105320784, ibnll=1.0856110347454975\n",
      "Validation loss decreased (0.416451 --> 0.413552).  Saving model ...                                                   \n",
      "it: 16, train loss=0.38692909479141235, validation loss=0.4135517179965973, c=0.533732116428219, ibs=0.3032788774592987, ibnll=1.079949626392063\n",
      "Validation loss decreased (0.413552 --> 0.410648).  Saving model ...                                                   \n",
      "it: 17, train loss=0.38421016931533813, validation loss=0.4106479287147522, c=0.5408855451406018, ibs=0.30255289296251325, ibnll=1.0742711108984075\n",
      "Validation loss decreased (0.410648 --> 0.407746).  Saving model ...                                                   \n",
      "it: 18, train loss=0.38148999214172363, validation loss=0.4077457785606384, c=0.5657375431672422, ibs=0.3018205622311973, ibnll=1.0685830770297189\n",
      "Validation loss decreased (0.407746 --> 0.404857).  Saving model ...                                                   \n",
      "it: 19, train loss=0.37877339124679565, validation loss=0.40485697984695435, c=0.5764676862358165, ibs=0.3010844835894147, ibnll=1.0629052600469084\n",
      "Validation loss decreased (0.404857 --> 0.401980).  Saving model ...                                                   \n",
      "it: 20, train loss=0.3760683536529541, validation loss=0.4019801914691925, c=0.5401763690182536, ibs=0.30034400481622514, ibnll=1.057241127508338\n",
      "Validation loss decreased (0.401980 --> 0.399126).  Saving model ...                                                   \n",
      "it: 21, train loss=0.3733765184879303, validation loss=0.399125874042511, c=0.5230328071040947, ibs=0.29960333329351546, ibnll=1.0516117299737786\n",
      "Validation loss decreased (0.399126 --> 0.396289).  Saving model ...                                                   \n",
      "it: 22, train loss=0.37070953845977783, validation loss=0.3962887227535248, c=0.5085101134681795, ibs=0.2988611234666506, ibnll=1.0460058825281944\n",
      "Validation loss decreased (0.396289 --> 0.393472).  Saving model ...                                                   \n",
      "it: 23, train loss=0.36806100606918335, validation loss=0.39347174763679504, c=0.5072767636901825, ibs=0.2981165269264473, ibnll=1.0404240045427624\n",
      "Validation loss decreased (0.393472 --> 0.390677).  Saving model ...                                                   \n",
      "it: 24, train loss=0.36543217301368713, validation loss=0.39067748188972473, c=0.5235569807597434, ibs=0.2973705677904766, ibnll=1.0348773735540426\n",
      "Validation loss decreased (0.390677 --> 0.387911).  Saving model ...                                                   \n",
      "it: 25, train loss=0.36282598972320557, validation loss=0.3879108130931854, c=0.5240194869264924, ibs=0.2966245224809635, ibnll=1.0293740849528459\n",
      "Validation loss decreased (0.387911 --> 0.385172).  Saving model ...                                                   \n",
      "it: 26, train loss=0.3602446913719177, validation loss=0.385171502828598, c=0.5250370004933399, ibs=0.2958789997266886, ibnll=1.0239179318008889\n",
      "Validation loss decreased (0.385172 --> 0.382461).  Saving model ...                                                   \n",
      "it: 27, train loss=0.35769128799438477, validation loss=0.382461279630661, c=0.5407005426739023, ibs=0.29513402655435134, ibnll=1.0185113211352546\n",
      "Validation loss decreased (0.382461 --> 0.379781).  Saving model ...                                                   \n",
      "it: 28, train loss=0.35516655445098877, validation loss=0.37978124618530273, c=0.5488406512086828, ibs=0.2943895530338035, ibnll=1.0131561279995414\n",
      "Validation loss decreased (0.379781 --> 0.377132).  Saving model ...                                                   \n",
      "it: 29, train loss=0.3526690602302551, validation loss=0.3771323263645172, c=0.5522631968426246, ibs=0.29364580898614695, ibnll=1.007855041664761\n",
      "Validation loss decreased (0.377132 --> 0.374516).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.3502001464366913, validation loss=0.37451595067977905, c=0.5561174148988653, ibs=0.2929032457222501, ibnll=1.0026092615474407\n",
      "Validation loss decreased (0.374516 --> 0.371927).  Saving model ...                                                   \n",
      "it: 31, train loss=0.34775999188423157, validation loss=0.3719266653060913, c=0.5444314257523434, ibs=0.29216166951175476, ibnll=0.9974120085728424\n",
      "Validation loss decreased (0.371927 --> 0.369368).  Saving model ...                                                   \n",
      "it: 32, train loss=0.345347136259079, validation loss=0.3693682551383972, c=0.5321287617168229, ibs=0.29142129917706266, ibnll=0.9922638720246855\n",
      "Validation loss decreased (0.369368 --> 0.366844).  Saving model ...                                                   \n",
      "it: 33, train loss=0.34296169877052307, validation loss=0.3668440580368042, c=0.5321287617168229, ibs=0.2906823416244322, ibnll=0.9871670379238431\n",
      "Validation loss decreased (0.366844 --> 0.364354).  Saving model ...                                                   \n",
      "it: 34, train loss=0.3406050503253937, validation loss=0.3643539845943451, c=0.5382030093734583, ibs=0.28994462662697423, ibnll=0.982124519726655\n",
      "Validation loss decreased (0.364354 --> 0.361893).  Saving model ...                                                   \n",
      "it: 35, train loss=0.3382761776447296, validation loss=0.3618927001953125, c=0.5352429699062654, ibs=0.289208415772004, ibnll=0.9771347373204023\n",
      "Validation loss decreased (0.361893 --> 0.359459).  Saving model ...                                                   \n",
      "it: 36, train loss=0.33597657084465027, validation loss=0.3594592809677124, c=0.52916872224963, ibs=0.28847393889267053, ibnll=0.9721984364470999\n",
      "Validation loss decreased (0.359459 --> 0.357057).  Saving model ...                                                   \n",
      "it: 37, train loss=0.33370447158813477, validation loss=0.3570568263530731, c=0.5205969412925505, ibs=0.2877411940726884, ibnll=0.9673159100345697\n",
      "Validation loss decreased (0.357057 --> 0.354686).  Saving model ...                                                   \n",
      "it: 38, train loss=0.3314588665962219, validation loss=0.3546861708164215, c=0.5247286630488407, ibs=0.28701010732354615, ibnll=0.9624868758740885\n",
      "Validation loss decreased (0.354686 --> 0.352345).  Saving model ...                                                   \n",
      "it: 39, train loss=0.3292403519153595, validation loss=0.35234534740448, c=0.5368463246176616, ibs=0.2862807083006785, ibnll=0.9577107129087221\n",
      "Validation loss decreased (0.352345 --> 0.350033).  Saving model ...                                                   \n",
      "it: 40, train loss=0.3270490765571594, validation loss=0.35003334283828735, c=0.5476381351751357, ibs=0.2855531159354941, ibnll=0.9529873406703739\n",
      "Validation loss decreased (0.350033 --> 0.347751).  Saving model ...                                                   \n",
      "it: 41, train loss=0.32488441467285156, validation loss=0.3477511703968048, c=0.5503515046867291, ibs=0.28482728682952524, ibnll=0.9483150925680985\n",
      "Validation loss decreased (0.347751 --> 0.345498).  Saving model ...                                                   \n",
      "it: 42, train loss=0.3227464556694031, validation loss=0.34549832344055176, c=0.5386655155402073, ibs=0.28410329721390143, ibnll=0.9436931259912448\n",
      "Validation loss decreased (0.345498 --> 0.343273).  Saving model ...                                                   \n",
      "it: 43, train loss=0.32063478231430054, validation loss=0.3432726263999939, c=0.5256536753823384, ibs=0.2833813815434999, ibnll=0.9391221337388859\n",
      "Validation loss decreased (0.343273 --> 0.341072).  Saving model ...                                                   \n",
      "it: 44, train loss=0.3185490667819977, validation loss=0.341072142124176, c=0.517482733103108, ibs=0.2826615061616861, ibnll=0.9346004823428596\n",
      "Validation loss decreased (0.341072 --> 0.338897).  Saving model ...                                                   \n",
      "it: 45, train loss=0.3164893388748169, validation loss=0.338896781206131, c=0.5192094227923039, ibs=0.28194355467854937, ibnll=0.9301267260739142\n",
      "Validation loss decreased (0.338897 --> 0.336749).  Saving model ...                                                   \n",
      "it: 46, train loss=0.3144550323486328, validation loss=0.3367489278316498, c=0.5270103601381352, ibs=0.28122770518316526, ibnll=0.925701705982018\n",
      "Validation loss decreased (0.336749 --> 0.334629).  Saving model ...                                                   \n",
      "it: 47, train loss=0.3124465048313141, validation loss=0.3346293270587921, c=0.535828811050814, ibs=0.28051406131523166, ibnll=0.9213256702984788\n",
      "Validation loss decreased (0.334629 --> 0.332537).  Saving model ...                                                   \n",
      "it: 48, train loss=0.3104632496833801, validation loss=0.33253663778305054, c=0.5367846571287617, ibs=0.2798025795216823, ibnll=0.9169975048564978\n",
      "Validation loss decreased (0.332537 --> 0.330468).  Saving model ...                                                   \n",
      "it: 49, train loss=0.30850476026535034, validation loss=0.3304682970046997, c=0.5272878638381845, ibs=0.27909334497128063, ibnll=0.9127163922595782\n",
      "Validation loss decreased (0.330468 --> 0.328423).  Saving model ...                                                   \n",
      "it: 50, train loss=0.3065703511238098, validation loss=0.3284232020378113, c=0.5257461766156882, ibs=0.2783864065589027, ibnll=0.9084815732712717\n",
      "Validation loss decreased (0.328423 --> 0.326402).  Saving model ...                                                   \n",
      "it: 51, train loss=0.30466029047966003, validation loss=0.3264021575450897, c=0.5275962012826838, ibs=0.27768176241775844, ibnll=0.90429298671321\n",
      "Validation loss decreased (0.326402 --> 0.324406).  Saving model ...                                                   \n",
      "it: 52, train loss=0.3027740716934204, validation loss=0.3244059383869171, c=0.5308954119388258, ibs=0.27697935398311385, ibnll=0.9001479088089365\n",
      "Validation loss decreased (0.324406 --> 0.322435).  Saving model ...                                                   \n",
      "it: 53, train loss=0.30091115832328796, validation loss=0.32243478298187256, c=0.5335471139615194, ibs=0.27627941503878456, ibnll=0.896050281186229\n",
      "Validation loss decreased (0.322435 --> 0.320488).  Saving model ...                                                   \n",
      "it: 54, train loss=0.29907143115997314, validation loss=0.3204878866672516, c=0.5363221509620129, ibs=0.275581888450343, ibnll=0.8919970226549752\n",
      "Validation loss decreased (0.320488 --> 0.318565).  Saving model ...                                                   \n",
      "it: 55, train loss=0.29725468158721924, validation loss=0.31856468319892883, c=0.5317279230389739, ibs=0.2748867567740117, ibnll=0.8879872768839971\n",
      "Validation loss decreased (0.318565 --> 0.316665).  Saving model ...                                                   \n",
      "it: 56, train loss=0.2954607903957367, validation loss=0.31666532158851624, c=0.5270103601381352, ibs=0.2741940911111498, ibnll=0.8840210553303097\n",
      "Validation loss decreased (0.316665 --> 0.314790).  Saving model ...                                                   \n",
      "it: 57, train loss=0.2936895489692688, validation loss=0.31479009985923767, c=0.5295695609274791, ibs=0.27350392679523955, ibnll=0.88009805238401\n",
      "Validation loss decreased (0.314790 --> 0.312939).  Saving model ...                                                   \n",
      "it: 58, train loss=0.29194092750549316, validation loss=0.3129386007785797, c=0.5289837197829305, ibs=0.27281630614384245, ibnll=0.8762175935871908\n",
      "Validation loss decreased (0.312939 --> 0.311110).  Saving model ...                                                   \n",
      "it: 59, train loss=0.2902143895626068, validation loss=0.3111099898815155, c=0.5357363098174642, ibs=0.2721312936756361, ibnll=0.8723793614736902\n",
      "Validation loss decreased (0.311110 --> 0.309303).  Saving model ...                                                   \n",
      "it: 60, train loss=0.28850969672203064, validation loss=0.30930331349372864, c=0.54581894425259, ibs=0.27144887797775547, ibnll=0.8685827209770336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.309303 --> 0.307517).  Saving model ...                                                   \n",
      "it: 61, train loss=0.28682637214660645, validation loss=0.30751746892929077, c=0.5476689689195856, ibs=0.27076904894061116, ibnll=0.8648271271522524\n",
      "Validation loss decreased (0.307517 --> 0.305753).  Saving model ...                                                   \n",
      "it: 62, train loss=0.2851641774177551, validation loss=0.30575305223464966, c=0.5503206709422792, ibs=0.270091883159939, ibnll=0.8611123882480596\n",
      "Validation loss decreased (0.305753 --> 0.304011).  Saving model ...                                                   \n",
      "it: 63, train loss=0.2835230827331543, validation loss=0.30401092767715454, c=0.5526640355204736, ibs=0.26941733783666993, ibnll=0.8574376593773838\n",
      "Validation loss decreased (0.304011 --> 0.302292).  Saving model ...                                                   \n",
      "it: 64, train loss=0.2819029688835144, validation loss=0.30229151248931885, c=0.5561482486433152, ibs=0.2687454427357251, ibnll=0.853802490979059\n",
      "Validation loss decreased (0.302292 --> 0.300594).  Saving model ...                                                   \n",
      "it: 65, train loss=0.28030315041542053, validation loss=0.30059438943862915, c=0.5598174642328564, ibs=0.2680762245112285, ibnll=0.8502064290815916\n",
      "Validation loss decreased (0.300594 --> 0.298919).  Saving model ...                                                   \n",
      "it: 66, train loss=0.27872350811958313, validation loss=0.29891929030418396, c=0.5606191415885545, ibs=0.26740972648970035, ibnll=0.8466492780860064\n",
      "Validation loss decreased (0.298919 --> 0.297265).  Saving model ...                                                   \n",
      "it: 67, train loss=0.277164101600647, validation loss=0.2972654402256012, c=0.5570115934879132, ibs=0.26674596115058297, ibnll=0.8431304180270066\n",
      "Validation loss decreased (0.297265 --> 0.295632).  Saving model ...                                                   \n",
      "it: 68, train loss=0.2756249010562897, validation loss=0.2956322431564331, c=0.549179822397632, ibs=0.2660850146867448, ibnll=0.8396498746229315\n",
      "Validation loss decreased (0.295632 --> 0.294018).  Saving model ...                                                   \n",
      "it: 69, train loss=0.2741056978702545, validation loss=0.29401838779449463, c=0.5549457326097681, ibs=0.26542687116450214, ibnll=0.8362070831993909\n",
      "Validation loss decreased (0.294018 --> 0.292424).  Saving model ...                                                   \n",
      "it: 70, train loss=0.2726062536239624, validation loss=0.2924237847328186, c=0.5552849037987173, ibs=0.26477156874353136, ibnll=0.8328017021900154\n",
      "Validation loss decreased (0.292424 --> 0.290850).  Saving model ...                                                   \n",
      "it: 71, train loss=0.27112528681755066, validation loss=0.2908500134944916, c=0.558738283177109, ibs=0.26411911538275284, ibnll=0.8294331872398103\n",
      "Validation loss decreased (0.290850 --> 0.289296).  Saving model ...                                                   \n",
      "it: 72, train loss=0.26966381072998047, validation loss=0.28929582238197327, c=0.5592932905772077, ibs=0.2634695651100847, ibnll=0.8261013473204688\n",
      "Validation loss decreased (0.289296 --> 0.287760).  Saving model ...                                                   \n",
      "it: 73, train loss=0.2682214081287384, validation loss=0.2877599596977234, c=0.5571965959546127, ibs=0.26282294667104256, ibnll=0.8228058998399065\n",
      "Validation loss decreased (0.287760 --> 0.286244).  Saving model ...                                                   \n",
      "it: 74, train loss=0.26679736375808716, validation loss=0.28624358773231506, c=0.5591082881105082, ibs=0.2621792419794337, ibnll=0.8195460164688311\n",
      "Validation loss decreased (0.286244 --> 0.284747).  Saving model ...                                                   \n",
      "it: 75, train loss=0.26539137959480286, validation loss=0.28474700450897217, c=0.5637025160335472, ibs=0.26153845270906223, ibnll=0.816321442680442\n",
      "Validation loss decreased (0.284747 --> 0.283269).  Saving model ...                                                   \n",
      "it: 76, train loss=0.2640034258365631, validation loss=0.28326940536499023, c=0.56666255550074, ibs=0.260900648949247, ibnll=0.8131321522451502\n",
      "Validation loss decreased (0.283269 --> 0.281809).  Saving model ...                                                   \n",
      "it: 77, train loss=0.2626340985298157, validation loss=0.28180935978889465, c=0.57082511100148, ibs=0.260265839821568, ibnll=0.8099777851079504\n",
      "Validation loss decreased (0.281809 --> 0.280369).  Saving model ...                                                   \n",
      "it: 78, train loss=0.26128143072128296, validation loss=0.2803686559200287, c=0.5721509620128269, ibs=0.25963408421101647, ibnll=0.8068580463109758\n",
      "Validation loss decreased (0.280369 --> 0.278946).  Saving model ...                                                   \n",
      "it: 79, train loss=0.25994741916656494, validation loss=0.27894556522369385, c=0.5769301924025654, ibs=0.25900541654814047, ibnll=0.8037727121068092\n",
      "Validation loss decreased (0.278946 --> 0.277541).  Saving model ...                                                   \n",
      "it: 80, train loss=0.2586302161216736, validation loss=0.27754080295562744, c=0.5778552047360631, ibs=0.25837981377198926, ibnll=0.8007212719482086\n",
      "Validation loss decreased (0.277541 --> 0.276155).  Saving model ...                                                   \n",
      "it: 81, train loss=0.25733044743537903, validation loss=0.27615490555763245, c=0.5807535767143562, ibs=0.25775722222107283, ibnll=0.7977030795781345\n",
      "Validation loss decreased (0.276155 --> 0.274785).  Saving model ...                                                   \n",
      "it: 82, train loss=0.25604867935180664, validation loss=0.27478528022766113, c=0.5763443512580168, ibs=0.25713765590553006, ibnll=0.7947178742853321\n",
      "Validation loss decreased (0.274785 --> 0.273434).  Saving model ...                                                   \n",
      "it: 83, train loss=0.25478243827819824, validation loss=0.27343350648880005, c=0.5784410458806117, ibs=0.25652118505599947, ibnll=0.7917653669882266\n",
      "Validation loss decreased (0.273434 --> 0.272100).  Saving model ...                                                   \n",
      "it: 84, train loss=0.25353333353996277, validation loss=0.27209994196891785, c=0.5796435619141589, ibs=0.2559078912911242, ibnll=0.7888455697026929\n",
      "Validation loss decreased (0.272100 --> 0.270783).  Saving model ...                                                   \n",
      "it: 85, train loss=0.25230082869529724, validation loss=0.2707834839820862, c=0.5747718302910706, ibs=0.25529776908921337, ibnll=0.7859581369292846\n",
      "Validation loss decreased (0.270783 --> 0.269483).  Saving model ...                                                   \n",
      "it: 86, train loss=0.25108465552330017, validation loss=0.26948297023773193, c=0.5764676862358165, ibs=0.2546908853916829, ibnll=0.7831029383391537\n",
      "Validation loss decreased (0.269483 --> 0.268201).  Saving model ...                                                   \n",
      "it: 87, train loss=0.2498839795589447, validation loss=0.2682011127471924, c=0.5735076467686235, ibs=0.2540872398623378, ibnll=0.780279450918404\n",
      "Validation loss decreased (0.268201 --> 0.266934).  Saving model ...                                                   \n",
      "it: 88, train loss=0.24870051443576813, validation loss=0.2669336199760437, c=0.5784718796250616, ibs=0.2534869014416391, ibnll=0.7774877086942017\n",
      "Validation loss decreased (0.266934 --> 0.265686).  Saving model ...                                                   \n",
      "it: 89, train loss=0.2475307285785675, validation loss=0.26568603515625, c=0.5801985693142575, ibs=0.2528898437611987, ibnll=0.7747272360852484\n",
      "Validation loss decreased (0.265686 --> 0.264449).  Saving model ...                                                   \n",
      "it: 90, train loss=0.24637995660305023, validation loss=0.26444944739341736, c=0.5829427725703009, ibs=0.2522959845183053, ibnll=0.7719972963161101\n",
      "Validation loss decreased (0.264449 --> 0.263236).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.24523931741714478, validation loss=0.2632361650466919, c=0.5790885545140602, ibs=0.25170528314464596, ibnll=0.7692974628899933\n",
      "Validation loss decreased (0.263236 --> 0.262030).  Saving model ...                                                   \n",
      "it: 92, train loss=0.24412114918231964, validation loss=0.2620304524898529, c=0.5796743956586088, ibs=0.25111782985919673, ibnll=0.7666277933556799\n",
      "Validation loss decreased (0.262030 --> 0.260851).  Saving model ...                                                   \n",
      "it: 93, train loss=0.24301038682460785, validation loss=0.26085054874420166, c=0.5792118894918599, ibs=0.25053377193928494, ibnll=0.7639885156857601\n",
      "Validation loss decreased (0.260851 --> 0.259680).  Saving model ...                                                   \n",
      "it: 94, train loss=0.24192386865615845, validation loss=0.2596798539161682, c=0.581277750370005, ibs=0.24995312073465525, ibnll=0.7613791159115851\n",
      "Validation loss decreased (0.259680 --> 0.258520).  Saving model ...                                                   \n",
      "it: 95, train loss=0.24084606766700745, validation loss=0.25851988792419434, c=0.5826036013813517, ibs=0.24937589467886825, ibnll=0.758799403514834\n",
      "Validation loss decreased (0.258520 --> 0.257382).  Saving model ...                                                   \n",
      "it: 96, train loss=0.2397785484790802, validation loss=0.257382333278656, c=0.5853169708929452, ibs=0.24880208348262917, ibnll=0.7562490740390438\n",
      "Validation loss decreased (0.257382 --> 0.256257).  Saving model ...                                                   \n",
      "it: 97, train loss=0.23873229324817657, validation loss=0.2562572658061981, c=0.5857486433152442, ibs=0.24823162728215242, ibnll=0.7537275625044255\n",
      "Validation loss decreased (0.256257 --> 0.255144).  Saving model ...                                                   \n",
      "it: 98, train loss=0.23769766092300415, validation loss=0.25514426827430725, c=0.5877220029600395, ibs=0.2476644676927279, ibnll=0.751234277870637\n",
      "Validation loss decreased (0.255144 --> 0.254050).  Saving model ...                                                   \n",
      "it: 99, train loss=0.23667459189891815, validation loss=0.25405022501945496, c=0.5892636901825358, ibs=0.2471007300814928, ibnll=0.7487693396099555\n",
      "Validation loss decreased (0.254050 --> 0.252969).  Saving model ...                                                   \n",
      "it: 100, train loss=0.2356692999601364, validation loss=0.2529694139957428, c=0.5897570300937346, ibs=0.24654044930596783, ibnll=0.7463325120555124\n",
      "Validation loss decreased (0.252969 --> 0.251900).  Saving model ...                                                   \n",
      "it: 101, train loss=0.23467664420604706, validation loss=0.2519000470638275, c=0.5903120374938332, ibs=0.24598377007808167, ibnll=0.7439241073868841\n",
      "Validation loss decreased (0.251900 --> 0.250850).  Saving model ...                                                   \n",
      "it: 102, train loss=0.23369459807872772, validation loss=0.2508498430252075, c=0.589572027627035, ibs=0.24543054181675092, ibnll=0.7415432264040857\n",
      "Validation loss decreased (0.250850 --> 0.249811).  Saving model ...                                                   \n",
      "it: 103, train loss=0.23273086547851562, validation loss=0.24981118738651276, c=0.5943512580167736, ibs=0.24488061501300382, ibnll=0.7391889112092079\n",
      "Validation loss decreased (0.249811 --> 0.248785).  Saving model ...                                                   \n",
      "it: 104, train loss=0.23177781701087952, validation loss=0.24878491461277008, c=0.5923470646275284, ibs=0.24433389906496755, ibnll=0.7368604260889297\n",
      "Validation loss decreased (0.248785 --> 0.247774).  Saving model ...                                                   \n",
      "it: 105, train loss=0.23083671927452087, validation loss=0.24777370691299438, c=0.5982979773063641, ibs=0.24379055437582087, ibnll=0.7345582042625665\n",
      "Validation loss decreased (0.247774 --> 0.246777).  Saving model ...                                                   \n",
      "it: 106, train loss=0.22990994155406952, validation loss=0.24677656590938568, c=0.5970646275283671, ibs=0.24325081155518405, ibnll=0.7322830437155822\n",
      "Validation loss decreased (0.246777 --> 0.245791).  Saving model ...                                                   \n",
      "it: 107, train loss=0.22899633646011353, validation loss=0.2457909733057022, c=0.6000863344844598, ibs=0.24271476678419904, ibnll=0.7300350523870414\n",
      "Validation loss decreased (0.245791 --> 0.244821).  Saving model ...                                                   \n",
      "it: 108, train loss=0.2280934602022171, validation loss=0.24482139945030212, c=0.6023680315737543, ibs=0.2421824277262859, ibnll=0.7278139349782051\n",
      "Validation loss decreased (0.244821 --> 0.243864).  Saving model ...                                                   \n",
      "it: 109, train loss=0.2272055298089981, validation loss=0.24386350810527802, c=0.6034780463739516, ibs=0.24165371022081927, ibnll=0.7256191137949582\n",
      "Validation loss decreased (0.243864 --> 0.242918).  Saving model ...                                                   \n",
      "it: 110, train loss=0.22632847726345062, validation loss=0.2429179847240448, c=0.6039097187962507, ibs=0.24112855498232777, ibnll=0.7234501005435263\n",
      "Validation loss decreased (0.242918 --> 0.241987).  Saving model ...                                                   \n",
      "it: 111, train loss=0.2254629135131836, validation loss=0.24198657274246216, c=0.6047422298963986, ibs=0.24060704376990197, ibnll=0.7213067984591564\n",
      "Validation loss decreased (0.241987 --> 0.241067).  Saving model ...                                                   \n",
      "it: 112, train loss=0.22461044788360596, validation loss=0.24106742441654205, c=0.6072397631968426, ibs=0.24008873493316352, ibnll=0.7191869768230702\n",
      "Validation loss decreased (0.241067 --> 0.240158).  Saving model ...                                                   \n",
      "it: 113, train loss=0.22377008199691772, validation loss=0.2401582896709442, c=0.6064380858411446, ibs=0.23957250584147805, ibnll=0.7170850129729149\n",
      "Validation loss decreased (0.240158 --> 0.239262).  Saving model ...                                                   \n",
      "it: 114, train loss=0.22294020652770996, validation loss=0.23926231265068054, c=0.6055439072520967, ibs=0.2390597816493281, ibnll=0.7150071325696913\n",
      "Validation loss decreased (0.239262 --> 0.238379).  Saving model ...                                                   \n",
      "it: 115, train loss=0.22212287783622742, validation loss=0.23837938904762268, c=0.6064997533300444, ibs=0.23855153697570688, ibnll=0.7129575955406444\n",
      "Validation loss decreased (0.238379 --> 0.237508).  Saving model ...                                                   \n",
      "it: 116, train loss=0.22131705284118652, validation loss=0.2375081479549408, c=0.6091514553527381, ibs=0.23804738726400546, ibnll=0.7109344906948176\n",
      "Validation loss decreased (0.237508 --> 0.236651).  Saving model ...                                                   \n",
      "it: 117, train loss=0.2205214649438858, validation loss=0.23665107786655426, c=0.6098606314750863, ibs=0.23754707789136315, ibnll=0.7089363271422372\n",
      "Validation loss decreased (0.236651 --> 0.235804).  Saving model ...                                                   \n",
      "it: 118, train loss=0.2197389006614685, validation loss=0.2358035296201706, c=0.6097372964972866, ibs=0.23705008330765714, ibnll=0.706960781320285\n",
      "Validation loss decreased (0.235804 --> 0.234967).  Saving model ...                                                   \n",
      "it: 119, train loss=0.21896551549434662, validation loss=0.234967440366745, c=0.6096447952639369, ibs=0.2365555776078254, ibnll=0.7050036664002588\n",
      "Validation loss decreased (0.234967 --> 0.234142).  Saving model ...                                                   \n",
      "it: 120, train loss=0.2182040810585022, validation loss=0.23414187133312225, c=0.6071472619634929, ibs=0.23606409434776884, ibnll=0.7030674210606496\n",
      "Validation loss decreased (0.234142 --> 0.233328).  Saving model ...                                                   \n",
      "it: 121, train loss=0.2174529880285263, validation loss=0.23332804441452026, c=0.6089664528860385, ibs=0.23557637483870625, ibnll=0.701155234811284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.233328 --> 0.232526).  Saving model ...                                                   \n",
      "it: 122, train loss=0.21671262383460999, validation loss=0.2325255125761032, c=0.6114331524420326, ibs=0.23509304827104266, ibnll=0.6992692318149881\n",
      "Validation loss decreased (0.232526 --> 0.231737).  Saving model ...                                                   \n",
      "it: 123, train loss=0.21598199009895325, validation loss=0.2317366749048233, c=0.612913172175629, ibs=0.23461472936108768, ibnll=0.6974109312752477\n",
      "Validation loss decreased (0.231737 --> 0.230957).  Saving model ...                                                   \n",
      "it: 124, train loss=0.21526268124580383, validation loss=0.23095713555812836, c=0.6135915145535273, ibs=0.2341409298699778, ibnll=0.6955775809477944\n",
      "Validation loss decreased (0.230957 --> 0.230188).  Saving model ...                                                   \n",
      "it: 125, train loss=0.2145516276359558, validation loss=0.2301877737045288, c=0.6159657128761716, ibs=0.23366833003536489, ibnll=0.6937575092618795\n",
      "Validation loss decreased (0.230188 --> 0.229427).  Saving model ...                                                   \n",
      "it: 126, train loss=0.21385209262371063, validation loss=0.22942684590816498, c=0.6182474099654662, ibs=0.23320092024555406, ibnll=0.6919657242729619\n",
      "Validation loss decreased (0.229427 --> 0.228686).  Saving model ...                                                   \n",
      "it: 127, train loss=0.21315796673297882, validation loss=0.22868631780147552, c=0.6175382338431179, ibs=0.23274537224913705, ibnll=0.6902244077531852\n",
      "Validation loss decreased (0.228686 --> 0.227946).  Saving model ...                                                   \n",
      "it: 128, train loss=0.2124784290790558, validation loss=0.22794580459594727, c=0.6146398618648249, ibs=0.2322812206247779, ibnll=0.6884586878597487\n",
      "Validation loss decreased (0.227946 --> 0.227214).  Saving model ...                                                   \n",
      "it: 129, train loss=0.21180571615695953, validation loss=0.2272135466337204, c=0.6153182042427232, ibs=0.2318165982973212, ibnll=0.6867011888877177\n",
      "Validation loss decreased (0.227214 --> 0.226496).  Saving model ...                                                   \n",
      "it: 130, train loss=0.2111435830593109, validation loss=0.2264963984489441, c=0.6167057227429699, ibs=0.23136614705081707, ibnll=0.6850020594958748\n",
      "Validation loss decreased (0.226496 --> 0.225798).  Saving model ...                                                   \n",
      "it: 131, train loss=0.21048623323440552, validation loss=0.22579841315746307, c=0.6119881598421312, ibs=0.23092906498783192, ibnll=0.6833606318768174\n",
      "Validation loss decreased (0.225798 --> 0.225101).  Saving model ...                                                   \n",
      "it: 132, train loss=0.2098427563905716, validation loss=0.2251005321741104, c=0.6003638381845091, ibs=0.23048014820674806, ibnll=0.6816886846650283\n",
      "Validation loss decreased (0.225101 --> 0.224405).  Saving model ...                                                   \n",
      "it: 133, train loss=0.20920822024345398, validation loss=0.22440460324287415, c=0.6031388751850024, ibs=0.2300282181238389, ibnll=0.6800084786081113\n",
      "Validation loss decreased (0.224405 --> 0.223730).  Saving model ...                                                   \n",
      "it: 134, train loss=0.20858138799667358, validation loss=0.22373025119304657, c=0.606222249629995, ibs=0.22959837620084533, ibnll=0.6784090635497934\n",
      "Validation loss decreased (0.223730 --> 0.223066).  Saving model ...                                                   \n",
      "it: 135, train loss=0.2079603672027588, validation loss=0.22306595742702484, c=0.6123889985199803, ibs=0.22916936375479655, ibnll=0.6768138585032865\n",
      "Validation loss decreased (0.223066 --> 0.222400).  Saving model ...                                                   \n",
      "it: 136, train loss=0.20734788477420807, validation loss=0.2223997861146927, c=0.615009866798224, ibs=0.22872977801026673, ibnll=0.6752006751732387\n",
      "Validation loss decreased (0.222400 --> 0.221754).  Saving model ...                                                   \n",
      "it: 137, train loss=0.20674607157707214, validation loss=0.2217535525560379, c=0.6163665515540208, ibs=0.22831735118415888, ibnll=0.6736866368552669\n",
      "Validation loss decreased (0.221754 --> 0.221105).  Saving model ...                                                   \n",
      "it: 138, train loss=0.2061470001935959, validation loss=0.22110459208488464, c=0.6135606808090774, ibs=0.22788585051160232, ibnll=0.6720998015626988\n",
      "Validation loss decreased (0.221105 --> 0.220483).  Saving model ...                                                   \n",
      "it: 139, train loss=0.20555448532104492, validation loss=0.22048303484916687, c=0.6140848544647262, ibs=0.2274811171527643, ibnll=0.670611943682042\n",
      "Validation loss decreased (0.220483 --> 0.219842).  Saving model ...                                                   \n",
      "it: 140, train loss=0.20497415959835052, validation loss=0.2198420912027359, c=0.6149173655648742, ibs=0.22704710272198514, ibnll=0.6690432503852096\n",
      "Validation loss decreased (0.219842 --> 0.219244).  Saving model ...                                                   \n",
      "it: 141, train loss=0.20439238846302032, validation loss=0.21924415230751038, c=0.6167057227429699, ibs=0.2266634806069103, ibnll=0.6676526784556268\n",
      "Validation loss decreased (0.219244 --> 0.218602).  Saving model ...                                                   \n",
      "it: 142, train loss=0.20383596420288086, validation loss=0.2186022251844406, c=0.6213616181549088, ibs=0.22619314368868243, ibnll=0.665969640217597\n",
      "Validation loss decreased (0.218602 --> 0.218099).  Saving model ...                                                   \n",
      "it: 143, train loss=0.2032802402973175, validation loss=0.21809938549995422, c=0.6166132215096202, ibs=0.22593196348108616, ibnll=0.6649783593700457\n",
      "Validation loss decreased (0.218099 --> 0.217400).  Saving model ...                                                   \n",
      "it: 144, train loss=0.20276254415512085, validation loss=0.21739988029003143, c=0.6303034040453873, ibs=0.22533812350265095, ibnll=0.6629418818114822\n",
      "Validation loss decreased (0.217400 --> 0.216809).  Saving model ...                                                   \n",
      "it: 145, train loss=0.20225182175636292, validation loss=0.21680915355682373, c=0.6270041933892452, ibs=0.22494441928361486, ibnll=0.6615273819813533\n",
      "Validation loss decreased (0.216809 --> 0.216426).  Saving model ...                                                   \n",
      "it: 146, train loss=0.20168641209602356, validation loss=0.2164260596036911, c=0.6156265416872225, ibs=0.22484037881019395, ibnll=0.6610598326394561\n",
      "Validation loss decreased (0.216426 --> 0.215681).  Saving model ...                                                   \n",
      "it: 147, train loss=0.2012159377336502, validation loss=0.21568061411380768, c=0.6208991119881598, ibs=0.22418239304092613, ibnll=0.6588131837217484\n",
      "Validation loss decreased (0.215681 --> 0.215120).  Saving model ...                                                   \n",
      "it: 148, train loss=0.2006065398454666, validation loss=0.21511968970298767, c=0.6211457819437592, ibs=0.223769959129798, ibnll=0.6573829422340572\n",
      "Validation loss decreased (0.215120 --> 0.214587).  Saving model ...                                                   \n",
      "it: 149, train loss=0.2001343071460724, validation loss=0.21458713710308075, c=0.617908238776517, ibs=0.2234780947671798, ibnll=0.6562864378023008\n",
      "Validation loss decreased (0.214587 --> 0.214124).  Saving model ...                                                   \n",
      "it: 150, train loss=0.19955433905124664, validation loss=0.21412377059459686, c=0.614609028120375, ibs=0.2232275680425846, ibnll=0.6553816448724679\n",
      "Validation loss decreased (0.214124 --> 0.213467).  Saving model ...                                                   \n",
      "it: 151, train loss=0.19909600913524628, validation loss=0.2134668231010437, c=0.6171373951652689, ibs=0.22262935209267495, ibnll=0.653376995274008\n",
      "Validation loss decreased (0.213467 --> 0.212939).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 152, train loss=0.19857542216777802, validation loss=0.21293939650058746, c=0.6173223976319684, ibs=0.22226405268477567, ibnll=0.6520982244285818\n",
      "Validation loss decreased (0.212939 --> 0.212484).  Saving model ...                                                   \n",
      "it: 153, train loss=0.19807714223861694, validation loss=0.21248356997966766, c=0.6172298963986187, ibs=0.22206317672275813, ibnll=0.6513256854067784\n",
      "Validation loss decreased (0.212484 --> 0.211909).  Saving model ...                                                   \n",
      "it: 154, train loss=0.19759540259838104, validation loss=0.21190889179706573, c=0.622903305377405, ibs=0.22160312209194633, ibnll=0.6497566418322646\n",
      "Validation loss decreased (0.211909 --> 0.211362).  Saving model ...                                                   \n",
      "it: 155, train loss=0.19709312915802002, validation loss=0.21136176586151123, c=0.6261408485446472, ibs=0.2211470236706265, ibnll=0.6482194994720126\n",
      "Validation loss decreased (0.211362 --> 0.210882).  Saving model ...                                                   \n",
      "it: 156, train loss=0.19661492109298706, validation loss=0.21088196337223053, c=0.6192649235323138, ibs=0.22085236333883831, ibnll=0.6471711104235709\n",
      "Validation loss decreased (0.210882 --> 0.210439).  Saving model ...                                                   \n",
      "it: 157, train loss=0.1961321383714676, validation loss=0.21043935418128967, c=0.6134990133201776, ibs=0.22058849854319515, ibnll=0.6462134304027093\n",
      "Validation loss decreased (0.210439 --> 0.209883).  Saving model ...                                                   \n",
      "it: 158, train loss=0.19568440318107605, validation loss=0.2098827213048935, c=0.6176924025653675, ibs=0.22009036687415942, ibnll=0.6445683663433778\n",
      "Validation loss decreased (0.209883 --> 0.209399).  Saving model ...                                                   \n",
      "it: 159, train loss=0.19521507620811462, validation loss=0.20939914882183075, c=0.6201899358658115, ibs=0.21975736915339553, ibnll=0.6434410648760538\n",
      "Validation loss decreased (0.209399 --> 0.208969).  Saving model ...                                                   \n",
      "it: 160, train loss=0.19475877285003662, validation loss=0.20896928012371063, c=0.6170140601874692, ibs=0.21953917704472267, ibnll=0.642664078496082\n",
      "Validation loss decreased (0.208969 --> 0.208436).  Saving model ...                                                   \n",
      "it: 161, train loss=0.1943247765302658, validation loss=0.20843644440174103, c=0.6142390231869759, ibs=0.2190599905850644, ibnll=0.641072504532451\n",
      "Validation loss decreased (0.208436 --> 0.207952).  Saving model ...                                                   \n",
      "it: 162, train loss=0.19386959075927734, validation loss=0.20795245468616486, c=0.61667488899852, ibs=0.21868946819791585, ibnll=0.6398130189855986\n",
      "Validation loss decreased (0.207952 --> 0.207518).  Saving model ...                                                   \n",
      "it: 163, train loss=0.19342781603336334, validation loss=0.20751754939556122, c=0.6217624568327578, ibs=0.21846784378290446, ibnll=0.6390175950170635\n",
      "Validation loss decreased (0.207518 --> 0.207036).  Saving model ...                                                   \n",
      "it: 164, train loss=0.1929900348186493, validation loss=0.2070358693599701, c=0.6265108534780464, ibs=0.21806057175857943, ibnll=0.6376790872624509\n",
      "Validation loss decreased (0.207036 --> 0.206576).  Saving model ...                                                   \n",
      "it: 165, train loss=0.19256074726581573, validation loss=0.20657552778720856, c=0.6256475086334484, ibs=0.21768151835269559, ibnll=0.6364221615873943\n",
      "Validation loss decreased (0.206576 --> 0.206147).  Saving model ...                                                   \n",
      "it: 166, train loss=0.19214729964733124, validation loss=0.20614738762378693, c=0.6228416378885052, ibs=0.21744451641623613, ibnll=0.6355831732301259\n",
      "Validation loss decreased (0.206147 --> 0.205677).  Saving model ...                                                   \n",
      "it: 167, train loss=0.1917216181755066, validation loss=0.20567701756954193, c=0.6220399605328071, ibs=0.2170454911523538, ibnll=0.6342803050187555\n",
      "Validation loss decreased (0.205677 --> 0.205238).  Saving model ...                                                   \n",
      "it: 168, train loss=0.1913033276796341, validation loss=0.2052382528781891, c=0.6213924518993587, ibs=0.21669187530153122, ibnll=0.6331216610180207\n",
      "Validation loss decreased (0.205238 --> 0.204844).  Saving model ...                                                   \n",
      "it: 169, train loss=0.19090019166469574, validation loss=0.20484353601932526, c=0.6195424272323631, ibs=0.21649286980544724, ibnll=0.6324044445725556\n",
      "Validation loss decreased (0.204844 --> 0.204382).  Saving model ...                                                   \n",
      "it: 170, train loss=0.19049867987632751, validation loss=0.20438216626644135, c=0.6205907745436606, ibs=0.21606556909273394, ibnll=0.6310290955089412\n",
      "Validation loss decreased (0.204382 --> 0.203951).  Saving model ...                                                   \n",
      "it: 171, train loss=0.19008809328079224, validation loss=0.2039511352777481, c=0.622101628021707, ibs=0.21577243706046392, ibnll=0.6300469673129878\n",
      "Validation loss decreased (0.203951 --> 0.203551).  Saving model ...                                                   \n",
      "it: 172, train loss=0.18966320157051086, validation loss=0.20355069637298584, c=0.6213307844104589, ibs=0.21551023721953638, ibnll=0.6291536184018502\n",
      "Validation loss decreased (0.203551 --> 0.203120).  Saving model ...                                                   \n",
      "it: 173, train loss=0.18927760422229767, validation loss=0.20311994850635529, c=0.6207757770103601, ibs=0.21509426537180323, ibnll=0.627826090263675\n",
      "Validation loss decreased (0.203120 --> 0.202733).  Saving model ...                                                   \n",
      "it: 174, train loss=0.18890245258808136, validation loss=0.20273318886756897, c=0.6192957572767637, ibs=0.21486497702745733, ibnll=0.6270710620971921\n",
      "Validation loss decreased (0.202733 --> 0.202315).  Saving model ...                                                   \n",
      "it: 175, train loss=0.18851368129253387, validation loss=0.20231527090072632, c=0.6202824370991613, ibs=0.21452974152503423, ibnll=0.6259834633359427\n",
      "Validation loss decreased (0.202315 --> 0.201889).  Saving model ...                                                   \n",
      "it: 176, train loss=0.1881297528743744, validation loss=0.20188942551612854, c=0.6194807597434633, ibs=0.21418895980000213, ibnll=0.6248612321568685\n",
      "Validation loss decreased (0.201889 --> 0.201506).  Saving model ...                                                   \n",
      "it: 177, train loss=0.18774741888046265, validation loss=0.20150569081306458, c=0.617507400098668, ibs=0.21398152685483599, ibnll=0.6241428731608304\n",
      "Validation loss decreased (0.201506 --> 0.201093).  Saving model ...                                                   \n",
      "it: 178, train loss=0.18736620247364044, validation loss=0.20109295845031738, c=0.6184632461766156, ibs=0.21359307779590714, ibnll=0.6229082734378137\n",
      "Validation loss decreased (0.201093 --> 0.200717).  Saving model ...                                                   \n",
      "it: 179, train loss=0.18699805438518524, validation loss=0.20071737468242645, c=0.619573260976813, ibs=0.21336219012686758, ibnll=0.622137970229725\n",
      "Validation loss decreased (0.200717 --> 0.200328).  Saving model ...                                                   \n",
      "it: 180, train loss=0.18662238121032715, validation loss=0.20032767951488495, c=0.62083744449926, ibs=0.2130449506509528, ibnll=0.6211341280374287\n",
      "Validation loss decreased (0.200328 --> 0.199941).  Saving model ...                                                   \n",
      "it: 181, train loss=0.18625608086585999, validation loss=0.19994114339351654, c=0.620004933399112, ibs=0.21277058254689546, ibnll=0.6202416224048967\n",
      "Validation loss decreased (0.199941 --> 0.199568).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 182, train loss=0.18587929010391235, validation loss=0.19956772029399872, c=0.6212999506660088, ibs=0.2125085787875206, ibnll=0.6193875673272193\n",
      "Validation loss decreased (0.199568 --> 0.199165).  Saving model ...                                                   \n",
      "it: 183, train loss=0.18551091849803925, validation loss=0.1991645097732544, c=0.6211457819437592, ibs=0.21214414880315527, ibnll=0.6182555790460995\n",
      "Validation loss decreased (0.199165 --> 0.198851).  Saving model ...                                                   \n",
      "it: 184, train loss=0.18514060974121094, validation loss=0.1988510936498642, c=0.6172298963986187, ibs=0.21205253438806348, ibnll=0.6179525703489156\n",
      "Validation loss decreased (0.198851 --> 0.198429).  Saving model ...                                                   \n",
      "it: 185, train loss=0.18479330837726593, validation loss=0.1984289437532425, c=0.6201282683769117, ibs=0.211498705488886, ibnll=0.6162323806225175\n",
      "Validation loss decreased (0.198429 --> 0.198276).  Saving model ...                                                   \n",
      "it: 186, train loss=0.18445263803005219, validation loss=0.19827613234519958, c=0.6192649235323138, ibs=0.21180823151235664, ibnll=0.6171586642875778\n",
      "Validation loss decreased (0.198276 --> 0.197805).  Saving model ...                                                   \n",
      "it: 187, train loss=0.18415838479995728, validation loss=0.1978054642677307, c=0.6169832264430193, ibs=0.21078347800108907, ibnll=0.614103301184875\n",
      "Validation loss decreased (0.197805 --> 0.197635).  Saving model ...                                                   \n",
      "it: 188, train loss=0.1839461624622345, validation loss=0.1976345181465149, c=0.6176615688209176, ibs=0.21135470590027594, ibnll=0.6157271243500558\n",
      "Validation loss decreased (0.197635 --> 0.196986).  Saving model ...                                                   \n",
      "it: 189, train loss=0.18347695469856262, validation loss=0.19698566198349, c=0.62080661075481, ibs=0.21048506538397782, ibnll=0.6129841251034607\n",
      "Validation loss decreased (0.196986 --> 0.196632).  Saving model ...                                                   \n",
      "it: 190, train loss=0.18304350972175598, validation loss=0.19663158059120178, c=0.6200974346324618, ibs=0.2100507537169303, ibnll=0.6116869552834575\n",
      "Validation loss decreased (0.196632 --> 0.196490).  Saving model ...                                                   \n",
      "it: 191, train loss=0.18274632096290588, validation loss=0.19649039208889008, c=0.619604094721263, ibs=0.2104354480617162, ibnll=0.6127890823497082\n",
      "Validation loss decreased (0.196490 --> 0.195916).  Saving model ...                                                   \n",
      "it: 192, train loss=0.18241573870182037, validation loss=0.1959160566329956, c=0.6205599407992106, ibs=0.2096218473515836, ibnll=0.6102129895643975\n",
      "Validation loss decreased (0.195916 --> 0.195571).  Saving model ...                                                   \n",
      "it: 193, train loss=0.1820179969072342, validation loss=0.19557081162929535, c=0.6197582634435126, ibs=0.20936086730450681, ibnll=0.6093710754751215\n",
      "Validation loss decreased (0.195571 --> 0.195420).  Saving model ...                                                   \n",
      "it: 194, train loss=0.18167553842067719, validation loss=0.195420041680336, c=0.6194499259990133, ibs=0.2095704304856605, ibnll=0.6099992964783194\n",
      "Validation loss decreased (0.195420 --> 0.194915).  Saving model ...                                                   \n",
      "it: 195, train loss=0.18136906623840332, validation loss=0.19491547346115112, c=0.6206832757770103, ibs=0.2087533446942692, ibnll=0.6075219783630217\n",
      "Validation loss decreased (0.194915 --> 0.194635).  Saving model ...                                                   \n",
      "it: 196, train loss=0.1810348480939865, validation loss=0.19463470578193665, c=0.6210841144548594, ibs=0.20875399903174555, ibnll=0.6074168121815299\n",
      "Validation loss decreased (0.194635 --> 0.194323).  Saving model ...                                                   \n",
      "it: 197, train loss=0.1806788593530655, validation loss=0.19432324171066284, c=0.6197890971879625, ibs=0.2085864755268565, ibnll=0.6068647640578179\n",
      "Validation loss decreased (0.194323 --> 0.193895).  Saving model ...                                                   \n",
      "it: 198, train loss=0.18035255372524261, validation loss=0.1938948780298233, c=0.618309077454366, ibs=0.2079781463097478, ibnll=0.6050502101685835\n",
      "Validation loss decreased (0.193895 --> 0.193691).  Saving model ...                                                   \n",
      "it: 199, train loss=0.18005993962287903, validation loss=0.19369107484817505, c=0.6188332511100147, ibs=0.208153881890182, ibnll=0.6055293228245445\n",
      "                                                                                                                       \n",
      "Results: 0.6026752785917907, 0.2050486664707753, 0.5970827855639063\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 422, 'lr': 0.0001, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 2, 'num_latent': 158, 'num_odefunc_layers1': 3, 'odefunc_neurons1': 171, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 1.0, 'weight_decay': 0.0001}\n",
      "  3%|                                     | 3/100 [4:04:24<130:34:12, 4845.90s/trial, best loss: 0.16803939640522003]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc6b76ca9c54f3883c61d2de8b509d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.276821).  Saving model ...                                                        \n",
      "it: 0, train loss=0.28474366664886475, validation loss=0.2768207788467407, c=0.572818662198737, ibs=0.2451448393932713, ibnll=0.675386258954228\n",
      "Validation loss decreased (0.276821 --> 0.275750).  Saving model ...                                                   \n",
      "it: 1, train loss=0.2837564945220947, validation loss=0.2757495045661926, c=0.5465588349014048, ibs=0.2442546309719321, ibnll=0.6733873245808549\n",
      "Validation loss decreased (0.275750 --> 0.274663).  Saving model ...                                                   \n",
      "it: 2, train loss=0.28269270062446594, validation loss=0.27466341853141785, c=0.5263242685913133, ibs=0.24336566428274387, ibnll=0.6713956123273848\n",
      "Validation loss decreased (0.274663 --> 0.273545).  Saving model ...                                                   \n",
      "it: 3, train loss=0.2816111445426941, validation loss=0.27354496717453003, c=0.5409846629720325, ibs=0.24244090967784093, ibnll=0.6693280172344843\n",
      "Validation loss decreased (0.273545 --> 0.272395).  Saving model ...                                                   \n",
      "it: 4, train loss=0.28049781918525696, validation loss=0.27239465713500977, c=0.5339283412810929, ibs=0.24149382921064344, ibnll=0.667215094222443\n",
      "Validation loss decreased (0.272395 --> 0.271218).  Saving model ...                                                   \n",
      "it: 5, train loss=0.27934813499450684, validation loss=0.2712180018424988, c=0.5479121020750096, ibs=0.24052955522373046, ibnll=0.6650673526006424\n",
      "Validation loss decreased (0.271218 --> 0.270009).  Saving model ...                                                   \n",
      "it: 6, train loss=0.2781713306903839, validation loss=0.2700091600418091, c=0.5590604459337544, ibs=0.23954050092592025, ibnll=0.6628694877185086\n",
      "Validation loss decreased (0.270009 --> 0.268772).  Saving model ...                                                   \n",
      "it: 7, train loss=0.2769635319709778, validation loss=0.26877227425575256, c=0.5502964299523134, ibs=0.2385256812650019, ibnll=0.6606198424013786\n",
      "Validation loss decreased (0.268772 --> 0.267508).  Saving model ...                                                   \n",
      "it: 8, train loss=0.2757280170917511, validation loss=0.26750844717025757, c=0.5241977058899343, ibs=0.23749027264627975, ibnll=0.6583299536937756\n",
      "Validation loss decreased (0.267508 --> 0.266212).  Saving model ...                                                   \n",
      "it: 9, train loss=0.2744634449481964, validation loss=0.26621201634407043, c=0.5462688490784895, ibs=0.23643222058812727, ibnll=0.6559946287197502\n",
      "Validation loss decreased (0.266212 --> 0.264888).  Saving model ...                                                   \n",
      "it: 10, train loss=0.27316442131996155, validation loss=0.26488813757896423, c=0.5268397989431628, ibs=0.23535223820185566, ibnll=0.6536170083388122\n",
      "Validation loss decreased (0.264888 --> 0.263533).  Saving model ...                                                   \n",
      "it: 11, train loss=0.27183738350868225, validation loss=0.2635330259799957, c=0.5122438458564248, ibs=0.23424551901635743, ibnll=0.6511865143095564\n",
      "Validation loss decreased (0.263533 --> 0.262161).  Saving model ...                                                   \n",
      "it: 12, train loss=0.27048107981681824, validation loss=0.2621608376502991, c=0.4889160974352365, ibs=0.23312542453149085, ibnll=0.6487328893151122\n",
      "Validation loss decreased (0.262161 --> 0.260750).  Saving model ...                                                   \n",
      "it: 13, train loss=0.2691071927547455, validation loss=0.2607504725456238, c=0.572657558963784, ibs=0.23197814119991117, ibnll=0.6462242142512717\n",
      "Validation loss decreased (0.260750 --> 0.259321).  Saving model ...                                                   \n",
      "it: 14, train loss=0.2676977217197418, validation loss=0.2593206763267517, c=0.5655690166258538, ibs=0.23081444215179103, ibnll=0.6436870580430357\n",
      "Validation loss decreased (0.259321 --> 0.257876).  Saving model ...                                                   \n",
      "it: 15, train loss=0.26626870036125183, validation loss=0.25787580013275146, c=0.5270653434720969, ibs=0.22963866015088857, ibnll=0.6411300171527655\n",
      "Validation loss decreased (0.257876 --> 0.256406).  Saving model ...                                                   \n",
      "it: 16, train loss=0.2648250460624695, validation loss=0.25640612840652466, c=0.4922992653692486, ibs=0.22844246958180664, ibnll=0.6385344818002816\n",
      "Validation loss decreased (0.256406 --> 0.254943).  Saving model ...                                                   \n",
      "it: 17, train loss=0.26335418224334717, validation loss=0.25494328141212463, c=0.4506701894574043, ibs=0.22725397142015424, ibnll=0.6359625121849186\n",
      "Validation loss decreased (0.254943 --> 0.253427).  Saving model ...                                                   \n",
      "it: 18, train loss=0.2618906497955322, validation loss=0.25342708826065063, c=0.48946384843407653, ibs=0.2260268092274183, ibnll=0.6333109632108299\n",
      "Validation loss decreased (0.253427 --> 0.251912).  Saving model ...                                                   \n",
      "it: 19, train loss=0.26037800312042236, validation loss=0.2519122064113617, c=0.5121149632684624, ibs=0.22480140213303818, ibnll=0.6306701457782522\n",
      "Validation loss decreased (0.251912 --> 0.250398).  Saving model ...                                                   \n",
      "it: 20, train loss=0.25886520743370056, validation loss=0.25039756298065186, c=0.45965974996777936, ibs=0.22357886600062474, ibnll=0.6280421118542274\n",
      "Validation loss decreased (0.250398 --> 0.248849).  Saving model ...                                                   \n",
      "it: 21, train loss=0.2573557496070862, validation loss=0.24884933233261108, c=0.5049619796365511, ibs=0.22233067125026654, ibnll=0.6253634106197893\n",
      "Validation loss decreased (0.248849 --> 0.247308).  Saving model ...                                                   \n",
      "it: 22, train loss=0.25580862164497375, validation loss=0.2473081648349762, c=0.487852816084547, ibs=0.22109148555623445, ibnll=0.6227102399384316\n",
      "Validation loss decreased (0.247308 --> 0.245754).  Saving model ...                                                   \n",
      "it: 23, train loss=0.25427043437957764, validation loss=0.2457539141178131, c=0.47519010181724447, ibs=0.21984468667120144, ibnll=0.6200462473717341\n",
      "Validation loss decreased (0.245754 --> 0.244190).  Saving model ...                                                   \n",
      "it: 24, train loss=0.25272008776664734, validation loss=0.2441902607679367, c=0.4597886325557417, ibs=0.2185934562657824, ibnll=0.617378274216606\n",
      "Validation loss decreased (0.244190 --> 0.242643).  Saving model ...                                                   \n",
      "it: 25, train loss=0.2511616051197052, validation loss=0.24264343082904816, c=0.48253640933109937, ibs=0.21735898127645925, ibnll=0.6147513705159856\n",
      "Validation loss decreased (0.242643 --> 0.241071).  Saving model ...                                                   \n",
      "it: 26, train loss=0.2496183067560196, validation loss=0.24107065796852112, c=0.49478025518752417, ibs=0.21611092490729084, ibnll=0.6120990823024552\n",
      "Validation loss decreased (0.241071 --> 0.239521).  Saving model ...                                                   \n",
      "it: 27, train loss=0.2480539232492447, validation loss=0.23952074348926544, c=0.4872084031447351, ibs=0.21488480011404487, ibnll=0.6094984171493016\n",
      "Validation loss decreased (0.239521 --> 0.237972).  Saving model ...                                                   \n",
      "it: 28, train loss=0.24650907516479492, validation loss=0.23797187209129333, c=0.49626240494909135, ibs=0.21366369745989222, ibnll=0.6069127903673871\n",
      "Validation loss decreased (0.237972 --> 0.236425).  Saving model ...                                                   \n",
      "it: 29, train loss=0.2449662834405899, validation loss=0.23642510175704956, c=0.504575331872664, ibs=0.21244834435022697, ibnll=0.6043434012428398\n",
      "Validation loss decreased (0.236425 --> 0.234888).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.24342672526836395, validation loss=0.23488834500312805, c=0.4820208789792499, ibs=0.21124520233323862, ibnll=0.6018038968467042\n",
      "Validation loss decreased (0.234888 --> 0.233358).  Saving model ...                                                   \n",
      "it: 31, train loss=0.2418995201587677, validation loss=0.23335768282413483, c=0.5251321046526615, ibs=0.21005314510432563, ibnll=0.5992904032009642\n",
      "Validation loss decreased (0.233358 --> 0.231843).  Saving model ...                                                   \n",
      "it: 32, train loss=0.24037589132785797, validation loss=0.23184259235858917, c=0.5061541435752029, ibs=0.2088783113026842, ibnll=0.5968166203706528\n",
      "Validation loss decreased (0.231843 --> 0.230337).  Saving model ...                                                   \n",
      "it: 33, train loss=0.23886917531490326, validation loss=0.23033717274665833, c=0.4921703827812863, ibs=0.20771640974134956, ibnll=0.5943730768807797\n",
      "Validation loss decreased (0.230337 --> 0.228857).  Saving model ...                                                   \n",
      "it: 34, train loss=0.23737215995788574, validation loss=0.22885741293430328, c=0.4325944064956824, ibs=0.20658094137968075, ibnll=0.5919877920340632\n",
      "Validation loss decreased (0.228857 --> 0.227370).  Saving model ...                                                   \n",
      "it: 35, train loss=0.23590020835399628, validation loss=0.2273700088262558, c=0.5153692486145122, ibs=0.2054425214502857, ibnll=0.5895980311395367\n",
      "Validation loss decreased (0.227370 --> 0.225929).  Saving model ...                                                   \n",
      "it: 36, train loss=0.23442697525024414, validation loss=0.2259286493062973, c=0.497003479829875, ibs=0.20434705187371663, ibnll=0.587300738434846\n",
      "Validation loss decreased (0.225929 --> 0.224486).  Saving model ...                                                   \n",
      "it: 37, train loss=0.23299621045589447, validation loss=0.22448626160621643, c=0.4938780770717876, ibs=0.20326048271434502, ibnll=0.5850230047752828\n",
      "Validation loss decreased (0.224486 --> 0.223073).  Saving model ...                                                   \n",
      "it: 38, train loss=0.23156651854515076, validation loss=0.22307343780994415, c=0.51688361902307, ibs=0.202202890404095, ibnll=0.5828073125607122\n",
      "Validation loss decreased (0.223073 --> 0.221681).  Saving model ...                                                   \n",
      "it: 39, train loss=0.23016373813152313, validation loss=0.22168061137199402, c=0.5194290501353267, ibs=0.20116647319452421, ibnll=0.5806371957881266\n",
      "Validation loss decreased (0.221681 --> 0.220305).  Saving model ...                                                   \n",
      "it: 40, train loss=0.22878232598304749, validation loss=0.220305398106575, c=0.5086673540404691, ibs=0.20014996970459112, ibnll=0.5785096345639049\n",
      "Validation loss decreased (0.220305 --> 0.218962).  Saving model ...                                                   \n",
      "it: 41, train loss=0.22741913795471191, validation loss=0.21896179020404816, c=0.44661038793658975, ibs=0.1991649063207142, ibnll=0.576448461752323\n",
      "Validation loss decreased (0.218962 --> 0.217625).  Saving model ...                                                   \n",
      "it: 42, train loss=0.22608767449855804, validation loss=0.2176254242658615, c=0.49635906689006315, ibs=0.1981901834793259, ibnll=0.5744086897392308\n",
      "Validation loss decreased (0.217625 --> 0.216322).  Saving model ...                                                   \n",
      "it: 43, train loss=0.22476619482040405, validation loss=0.2163219302892685, c=0.530609614641062, ibs=0.1972467336366266, ibnll=0.5724344271661228\n",
      "Validation loss decreased (0.216322 --> 0.215052).  Saving model ...                                                   \n",
      "it: 44, train loss=0.22347861528396606, validation loss=0.21505214273929596, c=0.5166258538471452, ibs=0.19633585430066258, ibnll=0.5705283583447442\n",
      "Validation loss decreased (0.215052 --> 0.213792).  Saving model ...                                                   \n",
      "it: 45, train loss=0.22222261130809784, validation loss=0.21379241347312927, c=0.5192679469003738, ibs=0.19544200423735486, ibnll=0.5686570242716628\n",
      "Validation loss decreased (0.213792 --> 0.212572).  Saving model ...                                                   \n",
      "it: 46, train loss=0.2209782749414444, validation loss=0.21257179975509644, c=0.4704858873566181, ibs=0.19458498516897943, ibnll=0.5668622396813183\n",
      "Validation loss decreased (0.212572 --> 0.211372).  Saving model ...                                                   \n",
      "it: 47, train loss=0.2197714000940323, validation loss=0.2113720327615738, c=0.4932014434849852, ibs=0.1937479941859688, ibnll=0.5651084791049834\n",
      "Validation loss decreased (0.211372 --> 0.210200).  Saving model ...                                                   \n",
      "it: 48, train loss=0.21858733892440796, validation loss=0.21019959449768066, c=0.5228444387163295, ibs=0.19293780010245803, ibnll=0.563409848103538\n",
      "Validation loss decreased (0.210200 --> 0.209052).  Saving model ...                                                   \n",
      "it: 49, train loss=0.21743227541446686, validation loss=0.2090519517660141, c=0.5296107745843537, ibs=0.19215211921019842, ibnll=0.5617614924114702\n",
      "Validation loss decreased (0.209052 --> 0.207935).  Saving model ...                                                   \n",
      "it: 50, train loss=0.21630175411701202, validation loss=0.20793476700782776, c=0.49519912359840185, ibs=0.19139638622986882, ibnll=0.5601748258688364\n",
      "Validation loss decreased (0.207935 --> 0.206847).  Saving model ...                                                   \n",
      "it: 51, train loss=0.21520063281059265, validation loss=0.20684677362442017, c=0.4783155045753319, ibs=0.1906685965076922, ibnll=0.5586451847214694\n",
      "Validation loss decreased (0.206847 --> 0.205782).  Saving model ...                                                   \n",
      "it: 52, train loss=0.21412846446037292, validation loss=0.2057817578315735, c=0.5078296172187138, ibs=0.18996408494942005, ibnll=0.5571627160285144\n",
      "Validation loss decreased (0.205782 --> 0.204747).  Saving model ...                                                   \n",
      "it: 53, train loss=0.2130807638168335, validation loss=0.20474687218666077, c=0.5356360355715942, ibs=0.18928704458481238, ibnll=0.555736465774928\n",
      "Validation loss decreased (0.204747 --> 0.203737).  Saving model ...                                                   \n",
      "it: 54, train loss=0.2120642215013504, validation loss=0.20373718440532684, c=0.5342505477509989, ibs=0.1886347136687204, ibnll=0.5543605666129648\n",
      "Validation loss decreased (0.203737 --> 0.202755).  Saving model ...                                                   \n",
      "it: 55, train loss=0.21107307076454163, validation loss=0.2027549147605896, c=0.5072496455728831, ibs=0.18800824417564432, ibnll=0.5530374532417093\n",
      "Validation loss decreased (0.202755 --> 0.201802).  Saving model ...                                                   \n",
      "it: 56, train loss=0.2101091742515564, validation loss=0.20180168747901917, c=0.48801391931949994, ibs=0.18740754087468423, ibnll=0.5517669034496502\n",
      "Validation loss decreased (0.201802 --> 0.200875).  Saving model ...                                                   \n",
      "it: 57, train loss=0.20917339622974396, validation loss=0.20087459683418274, c=0.5048975383425699, ibs=0.1868308368938506, ibnll=0.5505451227577218\n",
      "Validation loss decreased (0.200875 --> 0.199974).  Saving model ...                                                   \n",
      "it: 58, train loss=0.20826393365859985, validation loss=0.19997404515743256, c=0.5213622889547622, ibs=0.18627868460487923, ibnll=0.5493732898471857\n",
      "Validation loss decreased (0.199974 --> 0.199098).  Saving model ...                                                   \n",
      "it: 59, train loss=0.20738209784030914, validation loss=0.19909802079200745, c=0.5271620054130687, ibs=0.1857496104278137, ibnll=0.5482484169608672\n",
      "Validation loss decreased (0.199098 --> 0.198248).  Saving model ...                                                   \n",
      "it: 60, train loss=0.20652523636817932, validation loss=0.19824816286563873, c=0.5164969712591829, ibs=0.18524391595747355, ibnll=0.547171222797692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.198248 --> 0.197425).  Saving model ...                                                   \n",
      "it: 61, train loss=0.205694779753685, validation loss=0.1974247545003891, c=0.502448769171285, ibs=0.1847612858678079, ibnll=0.5461410920310156\n",
      "Validation loss decreased (0.197425 --> 0.196625).  Saving model ...                                                   \n",
      "it: 62, train loss=0.20489083230495453, validation loss=0.19662532210350037, c=0.514434849851785, ibs=0.1842994056635077, ibnll=0.5451532092260485\n",
      "Validation loss decreased (0.196625 --> 0.195852).  Saving model ...                                                   \n",
      "it: 63, train loss=0.2041102796792984, validation loss=0.1958521008491516, c=0.5138226575589638, ibs=0.1838602008618406, ibnll=0.5442117764235673\n",
      "Validation loss decreased (0.195852 --> 0.195103).  Saving model ...                                                   \n",
      "it: 64, train loss=0.20335568487644196, validation loss=0.19510260224342346, c=0.5104717102719423, ibs=0.18344184691914167, ibnll=0.5433129323309083\n",
      "Validation loss decreased (0.195103 --> 0.194376).  Saving model ...                                                   \n",
      "it: 65, train loss=0.20262520015239716, validation loss=0.19437561929225922, c=0.5081196030416291, ibs=0.18304347450216682, ibnll=0.5424549263826608\n",
      "Validation loss decreased (0.194376 --> 0.193672).  Saving model ...                                                   \n",
      "it: 66, train loss=0.20191770792007446, validation loss=0.1936718374490738, c=0.5174635906689007, ibs=0.18266434346471355, ibnll=0.5416362636338103\n",
      "Validation loss decreased (0.193672 --> 0.192991).  Saving model ...                                                   \n",
      "it: 67, train loss=0.2012336701154709, validation loss=0.19299069046974182, c=0.5176891351978348, ibs=0.18230446030480094, ibnll=0.5408571307000357\n",
      "Validation loss decreased (0.192991 --> 0.192332).  Saving model ...                                                   \n",
      "it: 68, train loss=0.20057234168052673, validation loss=0.19233235716819763, c=0.52232890836448, ibs=0.18196308127944003, ibnll=0.5401160429198637\n",
      "Validation loss decreased (0.192332 --> 0.191695).  Saving model ...                                                   \n",
      "it: 69, train loss=0.1999334692955017, validation loss=0.1916952133178711, c=0.5228122180693389, ibs=0.1816394992073027, ibnll=0.5394115984148353\n",
      "Validation loss decreased (0.191695 --> 0.191080).  Saving model ...                                                   \n",
      "it: 70, train loss=0.19931551814079285, validation loss=0.1910797506570816, c=0.5154981312024746, ibs=0.1813334031488421, ibnll=0.5387432317003009\n",
      "Validation loss decreased (0.191080 --> 0.190485).  Saving model ...                                                   \n",
      "it: 71, train loss=0.1987190991640091, validation loss=0.19048453867435455, c=0.5095695321562057, ibs=0.1810438722001515, ibnll=0.5381090821231324\n",
      "Validation loss decreased (0.190485 --> 0.189909).  Saving model ...                                                   \n",
      "it: 72, train loss=0.19814319908618927, validation loss=0.18990887701511383, c=0.5182691068436655, ibs=0.18077000433976237, ibnll=0.5375072632878606\n",
      "Validation loss decreased (0.189909 --> 0.189353).  Saving model ...                                                   \n",
      "it: 73, train loss=0.19758693873882294, validation loss=0.1893530637025833, c=0.525712076298492, ibs=0.18051165318689885, ibnll=0.5369376129881226\n",
      "Validation loss decreased (0.189353 --> 0.188816).  Saving model ...                                                   \n",
      "it: 74, train loss=0.19705061614513397, validation loss=0.18881575763225555, c=0.5296752158783349, ibs=0.18026824042855843, ibnll=0.536399041781434\n",
      "Validation loss decreased (0.188816 --> 0.188297).  Saving model ...                                                   \n",
      "it: 75, train loss=0.19653259217739105, validation loss=0.18829742074012756, c=0.5226833354813765, ibs=0.18003917232809802, ibnll=0.5358903293091744\n",
      "Validation loss decreased (0.188297 --> 0.187797).  Saving model ...                                                   \n",
      "it: 76, train loss=0.19603347778320312, validation loss=0.1877966821193695, c=0.5204278901920351, ibs=0.17982396821523278, ibnll=0.5354105990775334\n",
      "Validation loss decreased (0.187797 --> 0.187314).  Saving model ...                                                   \n",
      "it: 77, train loss=0.1955518126487732, validation loss=0.18731406331062317, c=0.5219744812475835, ibs=0.17962171525363066, ibnll=0.5349579067625041\n",
      "Validation loss decreased (0.187314 --> 0.186847).  Saving model ...                                                   \n",
      "it: 78, train loss=0.1950879991054535, validation loss=0.1868472397327423, c=0.5258087382394638, ibs=0.17943207947025336, ibnll=0.53453170082842\n",
      "Validation loss decreased (0.186847 --> 0.186398).  Saving model ...                                                   \n",
      "it: 79, train loss=0.19464007019996643, validation loss=0.18639808893203735, c=0.5308029385230055, ibs=0.17925467906192746, ibnll=0.5341311682504689\n",
      "Validation loss decreased (0.186398 --> 0.185964).  Saving model ...                                                   \n",
      "it: 80, train loss=0.19420959055423737, validation loss=0.18596388399600983, c=0.5287730377625983, ibs=0.1790889508313821, ibnll=0.5337552684797648\n",
      "Validation loss decreased (0.185964 --> 0.185545).  Saving model ...                                                   \n",
      "it: 81, train loss=0.19379405677318573, validation loss=0.18554529547691345, c=0.5256798556515014, ibs=0.17893430423133377, ibnll=0.533402758428594\n",
      "Validation loss decreased (0.185545 --> 0.185142).  Saving model ...                                                   \n",
      "it: 82, train loss=0.1933939903974533, validation loss=0.1851416528224945, c=0.5252287665936332, ibs=0.178790365106327, ibnll=0.5330729669808206\n",
      "Validation loss decreased (0.185142 --> 0.184752).  Saving model ...                                                   \n",
      "it: 83, train loss=0.19300876557826996, validation loss=0.1847524344921112, c=0.5286441551746359, ibs=0.178656576093416, ibnll=0.5327647181582252\n",
      "Validation loss decreased (0.184752 --> 0.184377).  Saving model ...                                                   \n",
      "it: 84, train loss=0.1926378458738327, validation loss=0.18437731266021729, c=0.529739657172316, ibs=0.17853248773381827, ibnll=0.5324771421758301\n",
      "Validation loss decreased (0.184377 --> 0.184016).  Saving model ...                                                   \n",
      "it: 85, train loss=0.1922808736562729, validation loss=0.18401572108268738, c=0.5382459079778322, ibs=0.17841767064336436, ibnll=0.5322093490818955\n",
      "Validation loss decreased (0.184016 --> 0.183667).  Saving model ...                                                   \n",
      "it: 86, train loss=0.19193719327449799, validation loss=0.1836673766374588, c=0.5379237015079262, ibs=0.17831179587773363, ibnll=0.531960713023266\n",
      "Validation loss decreased (0.183667 --> 0.183332).  Saving model ...                                                   \n",
      "it: 87, train loss=0.19160661101341248, validation loss=0.18333172798156738, c=0.5360871246294625, ibs=0.17821443145699187, ibnll=0.5317303744641084\n",
      "Validation loss decreased (0.183332 --> 0.183008).  Saving model ...                                                   \n",
      "it: 88, train loss=0.1912885457277298, validation loss=0.18300843238830566, c=0.5349594019847919, ibs=0.17812511999484867, ibnll=0.531517413336948\n",
      "Validation loss decreased (0.183008 --> 0.182697).  Saving model ...                                                   \n",
      "it: 89, train loss=0.1909826546907425, validation loss=0.18269674479961395, c=0.5383747905657945, ibs=0.1780434594850804, ibnll=0.5313209894560159\n",
      "Validation loss decreased (0.182697 --> 0.182397).  Saving model ...                                                   \n",
      "it: 90, train loss=0.19068826735019684, validation loss=0.18239696323871613, c=0.5409846629720325, ibs=0.17796908498743907, ibnll=0.5311403635285515\n",
      "Validation loss decreased (0.182397 --> 0.182108).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.19040557742118835, validation loss=0.18210755288600922, c=0.5440456244361387, ibs=0.17790169000426515, ibnll=0.530974927819525\n",
      "Validation loss decreased (0.182108 --> 0.181830).  Saving model ...                                                   \n",
      "it: 92, train loss=0.190133199095726, validation loss=0.18182975053787231, c=0.5489753834256992, ibs=0.1778409289902771, ibnll=0.530823963383644\n",
      "Validation loss decreased (0.181830 --> 0.181561).  Saving model ...                                                   \n",
      "it: 93, train loss=0.18987222015857697, validation loss=0.18156149983406067, c=0.5461721871375177, ibs=0.17778652004444018, ibnll=0.5306869148319683\n",
      "Validation loss decreased (0.181561 --> 0.181304).  Saving model ...                                                   \n",
      "it: 94, train loss=0.18962058424949646, validation loss=0.18130375444889069, c=0.5458177600206212, ibs=0.17773808655043494, ibnll=0.5305630029237981\n",
      "Validation loss decreased (0.181304 --> 0.181056).  Saving model ...                                                   \n",
      "it: 95, train loss=0.1893792301416397, validation loss=0.18105551600456238, c=0.5475254543111225, ibs=0.17769527358290169, ibnll=0.5304514957266099\n",
      "Validation loss decreased (0.181056 --> 0.180816).  Saving model ...                                                   \n",
      "it: 96, train loss=0.18914714455604553, validation loss=0.18081626296043396, c=0.5529385230055419, ibs=0.1776578188619004, ibnll=0.5303518830449191\n",
      "Validation loss decreased (0.180816 --> 0.180587).  Saving model ...                                                   \n",
      "it: 97, train loss=0.18892395496368408, validation loss=0.18058663606643677, c=0.5552906302358551, ibs=0.17762542902822132, ibnll=0.530263466043883\n",
      "Validation loss decreased (0.180587 --> 0.180365).  Saving model ...                                                   \n",
      "it: 98, train loss=0.1887100636959076, validation loss=0.18036508560180664, c=0.5566116767624694, ibs=0.17759786940424505, ibnll=0.5301858471450154\n",
      "Validation loss decreased (0.180365 --> 0.180152).  Saving model ...                                                   \n",
      "it: 99, train loss=0.188504159450531, validation loss=0.18015216290950775, c=0.5580293852300554, ibs=0.17757491096850578, ibnll=0.5301185005026783\n",
      "Validation loss decreased (0.180152 --> 0.179947).  Saving model ...                                                   \n",
      "it: 100, train loss=0.18830670416355133, validation loss=0.17994731664657593, c=0.5593826524036603, ibs=0.17755627857852968, ibnll=0.5300608617065761\n",
      "Validation loss decreased (0.179947 --> 0.179750).  Saving model ...                                                   \n",
      "it: 101, train loss=0.18811707198619843, validation loss=0.17974995076656342, c=0.5578038407011213, ibs=0.1775417320767152, ibnll=0.530012445785778\n",
      "Validation loss decreased (0.179750 --> 0.179561).  Saving model ...                                                   \n",
      "it: 102, train loss=0.18793481588363647, validation loss=0.17956075072288513, c=0.5591893285217168, ibs=0.1775310029360976, ibnll=0.5299727008596229\n",
      "Validation loss decreased (0.179561 --> 0.179378).  Saving model ...                                                   \n",
      "it: 103, train loss=0.18776045739650726, validation loss=0.17937807738780975, c=0.5613158912230958, ibs=0.17752387930656593, ibnll=0.5299411535411571\n",
      "Validation loss decreased (0.179378 --> 0.179203).  Saving model ...                                                   \n",
      "it: 104, train loss=0.18759240210056305, validation loss=0.17920267581939697, c=0.5667934012114964, ibs=0.17752014254940204, ibnll=0.529917373769617\n",
      "Validation loss decreased (0.179203 --> 0.179034).  Saving model ...                                                   \n",
      "it: 105, train loss=0.1874314844608307, validation loss=0.17903394997119904, c=0.5669867250934398, ibs=0.17751965221972735, ibnll=0.5299010173361135\n",
      "Validation loss decreased (0.179034 --> 0.178872).  Saving model ...                                                   \n",
      "it: 106, train loss=0.18727697432041168, validation loss=0.1788717657327652, c=0.5699188039695837, ibs=0.1775221691498638, ibnll=0.5298915741806801\n",
      "Validation loss decreased (0.178872 --> 0.178716).  Saving model ...                                                   \n",
      "it: 107, train loss=0.18712882697582245, validation loss=0.1787155419588089, c=0.571239850496198, ibs=0.17752755031109776, ibnll=0.5298887754431214\n",
      "Validation loss decreased (0.178716 --> 0.178566).  Saving model ...                                                   \n",
      "it: 108, train loss=0.18698646128177643, validation loss=0.17856569588184357, c=0.5666967392705246, ibs=0.1775355859604224, ibnll=0.5298921241078277\n",
      "Validation loss decreased (0.178566 --> 0.178421).  Saving model ...                                                   \n",
      "it: 109, train loss=0.18685024976730347, validation loss=0.1784212589263916, c=0.5669545044464492, ibs=0.1775460904521081, ibnll=0.5299012646455602\n",
      "Validation loss decreased (0.178421 --> 0.178283).  Saving model ...                                                   \n",
      "it: 110, train loss=0.18671929836273193, validation loss=0.1782827079296112, c=0.5694354942647248, ibs=0.17755889847331835, ibnll=0.5299158408091963\n",
      "Validation loss decreased (0.178283 --> 0.178149).  Saving model ...                                                   \n",
      "it: 111, train loss=0.18659406900405884, validation loss=0.17814935743808746, c=0.571787601495038, ibs=0.17757389231273923, ibnll=0.5299355861386038\n",
      "Validation loss decreased (0.178149 --> 0.178021).  Saving model ...                                                   \n",
      "it: 112, train loss=0.18647369742393494, validation loss=0.17802117764949799, c=0.5734952957855394, ibs=0.17759090337614164, ibnll=0.5299601180138848\n",
      "Validation loss decreased (0.178021 --> 0.177898).  Saving model ...                                                   \n",
      "it: 113, train loss=0.18635834753513336, validation loss=0.17789818346500397, c=0.5718198221420286, ibs=0.17760982229024636, ibnll=0.52998921088925\n",
      "Validation loss decreased (0.177898 --> 0.177779).  Saving model ...                                                   \n",
      "it: 114, train loss=0.18624800443649292, validation loss=0.17777948081493378, c=0.5735919577265112, ibs=0.1776305245917551, ibnll=0.5300225964272828\n",
      "Validation loss decreased (0.177779 --> 0.177666).  Saving model ...                                                   \n",
      "it: 115, train loss=0.1861417591571808, validation loss=0.17766603827476501, c=0.5751707694290501, ibs=0.17765283670905643, ibnll=0.5300599060595476\n",
      "Validation loss decreased (0.177666 --> 0.177556).  Saving model ...                                                   \n",
      "it: 116, train loss=0.18604061007499695, validation loss=0.17755644023418427, c=0.5748485629591442, ibs=0.17767664767620986, ibnll=0.5301008621113016\n",
      "Validation loss decreased (0.177556 --> 0.177451).  Saving model ...                                                   \n",
      "it: 117, train loss=0.18594305217266083, validation loss=0.17745132744312286, c=0.5761373888387679, ibs=0.17770180460296603, ibnll=0.5301451475886786\n",
      "Validation loss decreased (0.177451 --> 0.177350).  Saving model ...                                                   \n",
      "it: 118, train loss=0.1858498454093933, validation loss=0.17735038697719574, c=0.579198350302874, ibs=0.17772825347468707, ibnll=0.5301926410761343\n",
      "Validation loss decreased (0.177350 --> 0.177253).  Saving model ...                                                   \n",
      "it: 119, train loss=0.18576061725616455, validation loss=0.17725317180156708, c=0.5800683077716201, ibs=0.1777558705838512, ibnll=0.5302430325603025\n",
      "Validation loss decreased (0.177253 --> 0.177160).  Saving model ...                                                   \n",
      "it: 120, train loss=0.18567483127117157, validation loss=0.17716000974178314, c=0.5817115607681402, ibs=0.17778457282455007, ibnll=0.5302962189806583\n",
      "Validation loss decreased (0.177160 --> 0.177070).  Saving model ...                                                   \n",
      "it: 121, train loss=0.18559305369853973, validation loss=0.17707037925720215, c=0.581808222709112, ibs=0.17781424052038988, ibnll=0.5303518083988114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.177070 --> 0.176984).  Saving model ...                                                   \n",
      "it: 122, train loss=0.18551449477672577, validation loss=0.17698420584201813, c=0.581421574945225, ibs=0.17784480534939665, ibnll=0.5304097553008725\n",
      "Validation loss decreased (0.176984 --> 0.176902).  Saving model ...                                                   \n",
      "it: 123, train loss=0.18543925881385803, validation loss=0.17690160870552063, c=0.5842892125273875, ibs=0.1778761351883181, ibnll=0.5304697068859607\n",
      "Validation loss decreased (0.176902 --> 0.176822).  Saving model ...                                                   \n",
      "it: 124, train loss=0.18536745011806488, validation loss=0.17682193219661713, c=0.5832581518236886, ibs=0.1779081901926998, ibnll=0.5305316037109944\n",
      "Validation loss decreased (0.176822 --> 0.176746).  Saving model ...                                                   \n",
      "it: 125, train loss=0.1852983683347702, validation loss=0.17674578726291656, c=0.5833548137646604, ibs=0.17794084331666474, ibnll=0.5305951178405152\n",
      "Validation loss decreased (0.176746 --> 0.176672).  Saving model ...                                                   \n",
      "it: 126, train loss=0.18523259460926056, validation loss=0.17667213082313538, c=0.5858035829359454, ibs=0.1779740570569636, ibnll=0.5306601879013579\n",
      "Validation loss decreased (0.176672 --> 0.176602).  Saving model ...                                                   \n",
      "it: 127, train loss=0.18516917526721954, validation loss=0.1766018271446228, c=0.5881556901662586, ibs=0.17800773657167948, ibnll=0.5307265876127657\n",
      "Validation loss decreased (0.176602 --> 0.176534).  Saving model ...                                                   \n",
      "it: 128, train loss=0.1851089596748352, validation loss=0.17653369903564453, c=0.5868990849336254, ibs=0.17804180605823136, ibnll=0.5307941714722422\n",
      "Validation loss decreased (0.176534 --> 0.176469).  Saving model ...                                                   \n",
      "it: 129, train loss=0.18505074083805084, validation loss=0.17646904289722443, c=0.5874146152854749, ibs=0.17807618721604057, ibnll=0.5308626711821263\n",
      "Validation loss decreased (0.176469 --> 0.176406).  Saving model ...                                                   \n",
      "it: 130, train loss=0.18499580025672913, validation loss=0.1764059215784073, c=0.5886389998711175, ibs=0.17811073785486606, ibnll=0.5309318288380159\n",
      "Validation loss decreased (0.176406 --> 0.176346).  Saving model ...                                                   \n",
      "it: 131, train loss=0.18494226038455963, validation loss=0.17634595930576324, c=0.5885423379301457, ibs=0.17814531765583344, ibnll=0.531001276146007\n",
      "Validation loss decreased (0.176346 --> 0.176287).  Saving model ...                                                   \n",
      "it: 132, train loss=0.18489177525043488, validation loss=0.1762874871492386, c=0.5907333419255059, ibs=0.178179728976321, ibnll=0.53107049293981\n",
      "Validation loss decreased (0.176287 --> 0.176231).  Saving model ...                                                   \n",
      "it: 133, train loss=0.18484267592430115, validation loss=0.17623107135295868, c=0.5898311638097693, ibs=0.1782135178452069, ibnll=0.5311384026986377\n",
      "Validation loss decreased (0.176231 --> 0.176175).  Saving model ...                                                   \n",
      "it: 134, train loss=0.18479563295841217, validation loss=0.17617525160312653, c=0.5903144735146282, ibs=0.17824559152643435, ibnll=0.5312023023831207\n",
      "Validation loss decreased (0.176175 --> 0.176119).  Saving model ...                                                   \n",
      "it: 135, train loss=0.1847492903470993, validation loss=0.1761186122894287, c=0.5914744168062894, ibs=0.17827366164323927, ibnll=0.5312566718182317\n",
      "Validation loss decreased (0.176119 --> 0.176063).  Saving model ...                                                   \n",
      "it: 136, train loss=0.1847023218870163, validation loss=0.17606285214424133, c=0.5927310220389226, ibs=0.17829721069831347, ibnll=0.5313009935425622\n",
      "Validation loss decreased (0.176063 --> 0.176021).  Saving model ...                                                   \n",
      "it: 137, train loss=0.18465332686901093, validation loss=0.1760205328464508, c=0.5924410362160072, ibs=0.1783209917632183, ibnll=0.5313483416125673\n",
      "Validation loss decreased (0.176021 --> 0.175984).  Saving model ...                                                   \n",
      "it: 138, train loss=0.18461114168167114, validation loss=0.17598430812358856, c=0.593794303389612, ibs=0.1783544549984414, ibnll=0.5314192497063298\n",
      "Validation loss decreased (0.175984 --> 0.175934).  Saving model ...                                                   \n",
      "it: 139, train loss=0.1845802217721939, validation loss=0.17593392729759216, c=0.5980796494393608, ibs=0.17838518365670594, ibnll=0.5314808115627117\n",
      "Validation loss decreased (0.175934 --> 0.175882).  Saving model ...                                                   \n",
      "it: 140, train loss=0.18453766405582428, validation loss=0.17588235437870026, c=0.5975641190875113, ibs=0.1784206222698182, ibnll=0.5315534808059695\n",
      "Validation loss decreased (0.175882 --> 0.175840).  Saving model ...                                                   \n",
      "it: 141, train loss=0.18449737131595612, validation loss=0.17583999037742615, c=0.601527258667354, ibs=0.1784572614158271, ibnll=0.5316306310663733\n",
      "Validation loss decreased (0.175840 --> 0.175800).  Saving model ...                                                   \n",
      "it: 142, train loss=0.18446484208106995, validation loss=0.1757996380329132, c=0.605554839541178, ibs=0.1784880622650845, ibnll=0.5316948940561368\n",
      "Validation loss decreased (0.175800 --> 0.175758).  Saving model ...                                                   \n",
      "it: 143, train loss=0.18443110585212708, validation loss=0.17575767636299133, c=0.6031382910168837, ibs=0.17851652199516596, ibnll=0.5317528097239764\n",
      "Validation loss decreased (0.175758 --> 0.175718).  Saving model ...                                                   \n",
      "it: 144, train loss=0.18439540266990662, validation loss=0.1757175773382187, c=0.5999806676118057, ibs=0.1785426325526333, ibnll=0.5318051180709791\n",
      "Validation loss decreased (0.175718 --> 0.175683).  Saving model ...                                                   \n",
      "it: 145, train loss=0.18435974419116974, validation loss=0.1756831556558609, c=0.5997873437298621, ibs=0.17856533258255577, ibnll=0.5318507197873208\n",
      "Validation loss decreased (0.175683 --> 0.175651).  Saving model ...                                                   \n",
      "it: 146, train loss=0.18432748317718506, validation loss=0.1756511926651001, c=0.6001739914937492, ibs=0.17858820503412343, ibnll=0.5318978213666085\n",
      "Validation loss decreased (0.175651 --> 0.175614).  Saving model ...                                                   \n",
      "it: 147, train loss=0.1842980980873108, validation loss=0.17561396956443787, c=0.5999162263178245, ibs=0.17861465361834308, ibnll=0.5319515835679409\n",
      "Validation loss decreased (0.175614 --> 0.175573).  Saving model ...                                                   \n",
      "it: 148, train loss=0.1842673122882843, validation loss=0.17557305097579956, c=0.6005284186106457, ibs=0.17864694587340166, ibnll=0.5320173425985123\n",
      "Validation loss decreased (0.175573 --> 0.175536).  Saving model ...                                                   \n",
      "it: 149, train loss=0.18423709273338318, validation loss=0.17553550004959106, c=0.5990784894960691, ibs=0.17867754997307397, ibnll=0.5320789720722323\n",
      "Validation loss decreased (0.175536 --> 0.175501).  Saving model ...                                                   \n",
      "it: 150, train loss=0.1842096745967865, validation loss=0.17550130188465118, c=0.596243072560897, ibs=0.17870286439282057, ibnll=0.5321287777586763\n",
      "Validation loss decreased (0.175501 --> 0.175466).  Saving model ...                                                   \n",
      "it: 151, train loss=0.18418283760547638, validation loss=0.17546649277210236, c=0.5945031576234051, ibs=0.1787218005296884, ibnll=0.5321646088597923\n",
      "Validation loss decreased (0.175466 --> 0.175432).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 152, train loss=0.18415379524230957, validation loss=0.17543204128742218, c=0.5959530867379818, ibs=0.17873476457664356, ibnll=0.5321892200490758\n",
      "Validation loss decreased (0.175432 --> 0.175415).  Saving model ...                                                   \n",
      "it: 153, train loss=0.18412259221076965, validation loss=0.17541486024856567, c=0.5987562830261631, ibs=0.1787529491339194, ibnll=0.5322274932467298\n",
      "Validation loss decreased (0.175415 --> 0.175390).  Saving model ...                                                   \n",
      "it: 154, train loss=0.1841035932302475, validation loss=0.1753903180360794, c=0.598530738497229, ibs=0.17877787659070826, ibnll=0.5322788878767427\n",
      "Validation loss decreased (0.175390 --> 0.175352).  Saving model ...                                                   \n",
      "it: 155, train loss=0.1840839982032776, validation loss=0.17535223066806793, c=0.5982085320273232, ibs=0.17880453503547852, ibnll=0.532331456530579\n",
      "Validation loss decreased (0.175352 --> 0.175318).  Saving model ...                                                   \n",
      "it: 156, train loss=0.1840560883283615, validation loss=0.17531795799732208, c=0.5942131718004897, ibs=0.17883521294528074, ibnll=0.5323935877064114\n",
      "Validation loss decreased (0.175318 --> 0.175291).  Saving model ...                                                   \n",
      "it: 157, train loss=0.18403451144695282, validation loss=0.17529112100601196, c=0.5943098337414615, ibs=0.1788619535366409, ibnll=0.5324480092935711\n",
      "Validation loss decreased (0.175291 --> 0.175267).  Saving model ...                                                   \n",
      "it: 158, train loss=0.18401534855365753, validation loss=0.17526660859584808, c=0.5956308802680758, ibs=0.17887995911607624, ibnll=0.5324838922605374\n",
      "Validation loss decreased (0.175267 --> 0.175245).  Saving model ...                                                   \n",
      "it: 159, train loss=0.18399345874786377, validation loss=0.17524509131908417, c=0.5989496069081067, ibs=0.17889185180554837, ibnll=0.5325066876846605\n",
      "Validation loss decreased (0.175245 --> 0.175222).  Saving model ...                                                   \n",
      "it: 160, train loss=0.18397246301174164, validation loss=0.17522192001342773, c=0.6012372728444387, ibs=0.17890191491571103, ibnll=0.5325244413334004\n",
      "Validation loss decreased (0.175222 --> 0.175188).  Saving model ...                                                   \n",
      "it: 161, train loss=0.1839507520198822, validation loss=0.17518794536590576, c=0.6038793658976672, ibs=0.17890664768597922, ibnll=0.5325271779839948\n",
      "Validation loss decreased (0.175188 --> 0.175152).  Saving model ...                                                   \n",
      "it: 162, train loss=0.18392108380794525, validation loss=0.1751515120267868, c=0.6032671736048459, ibs=0.1788948279572523, ibnll=0.5324937825605723\n",
      "Validation loss decreased (0.175152 --> 0.175138).  Saving model ...                                                   \n",
      "it: 163, train loss=0.18388105928897858, validation loss=0.1751376986503601, c=0.600270653434721, ibs=0.17891020301401064, ibnll=0.532527805720215\n",
      "Validation loss decreased (0.175138 --> 0.175112).  Saving model ...                                                   \n",
      "it: 164, train loss=0.18387311697006226, validation loss=0.17511169612407684, c=0.5996262404949091, ibs=0.17892479486314572, ibnll=0.5325559471617087\n",
      "Validation loss decreased (0.175112 --> 0.175084).  Saving model ...                                                   \n",
      "it: 165, train loss=0.18385343253612518, validation loss=0.17508412897586823, c=0.6021716716071659, ibs=0.1789313450822501, ibnll=0.5325653973658842\n",
      "Validation loss decreased (0.175084 --> 0.175048).  Saving model ...                                                   \n",
      "it: 166, train loss=0.18382616341114044, validation loss=0.17504750192165375, c=0.6049426472483568, ibs=0.1789344720085849, ibnll=0.5325626327132182\n",
      "Validation loss decreased (0.175048 --> 0.175025).  Saving model ...                                                   \n",
      "it: 167, train loss=0.18378852307796478, validation loss=0.17502471804618835, c=0.6059092666580745, ibs=0.17896308292601698, ibnll=0.5326203661844822\n",
      "Validation loss decreased (0.175025 --> 0.174997).  Saving model ...                                                   \n",
      "it: 168, train loss=0.18377870321273804, validation loss=0.17499704658985138, c=0.604684882072432, ibs=0.17896378143486066, ibnll=0.5326157360602631\n",
      "Validation loss decreased (0.174997 --> 0.174969).  Saving model ...                                                   \n",
      "it: 169, train loss=0.1837512105703354, validation loss=0.17496925592422485, c=0.6041371310735919, ibs=0.17894819591965153, ibnll=0.5325764052233847\n",
      "Validation loss decreased (0.174969 --> 0.174944).  Saving model ...                                                   \n",
      "it: 170, train loss=0.18371520936489105, validation loss=0.17494411766529083, c=0.6040082484856296, ibs=0.17895476541113708, ibnll=0.5325879776931661\n",
      "Validation loss decreased (0.174944 --> 0.174905).  Saving model ...                                                   \n",
      "it: 171, train loss=0.18369673192501068, validation loss=0.17490504682064056, c=0.6035893800747519, ibs=0.17895396263859825, ibnll=0.532576015427042\n",
      "Validation loss decreased (0.174905 --> 0.174875).  Saving model ...                                                   \n",
      "it: 172, train loss=0.1836635321378708, validation loss=0.174874946475029, c=0.6039115865446578, ibs=0.1789437124360462, ibnll=0.5325441087637649\n",
      "Validation loss decreased (0.174875 --> 0.174856).  Saving model ...                                                   \n",
      "it: 173, train loss=0.18363423645496368, validation loss=0.17485567927360535, c=0.6032671736048459, ibs=0.17891387641466994, ibnll=0.5324703086449809\n",
      "Validation loss decreased (0.174856 --> 0.174826).  Saving model ...                                                   \n",
      "it: 174, train loss=0.18360121548175812, validation loss=0.17482587695121765, c=0.601527258667354, ibs=0.17888571248019144, ibnll=0.5323983514636413\n",
      "Validation loss decreased (0.174826 --> 0.174768).  Saving model ...                                                   \n",
      "it: 175, train loss=0.18356144428253174, validation loss=0.17476801574230194, c=0.6004639773166646, ibs=0.17884946850760536, ibnll=0.5323067755892782\n",
      "Validation loss decreased (0.174768 --> 0.174688).  Saving model ...                                                   \n",
      "it: 176, train loss=0.18350696563720703, validation loss=0.17468804121017456, c=0.6037182626627142, ibs=0.17878421142736659, ibnll=0.5321425552904319\n",
      "Validation loss decreased (0.174688 --> 0.174650).  Saving model ...                                                   \n",
      "it: 177, train loss=0.18344278633594513, validation loss=0.17465049028396606, c=0.6051359711303003, ibs=0.17871719493495178, ibnll=0.5319788028178559\n",
      "Validation loss decreased (0.174650 --> 0.174630).  Saving model ...                                                   \n",
      "it: 178, train loss=0.1833898425102234, validation loss=0.174630269408226, c=0.6047171027194226, ibs=0.1786629227977685, ibnll=0.531847610986427\n",
      "Validation loss decreased (0.174630 --> 0.174567).  Saving model ...                                                   \n",
      "it: 179, train loss=0.18334783613681793, validation loss=0.1745673567056656, c=0.6026549813120248, ibs=0.17864955446598002, ibnll=0.5318052416399264\n",
      "Validation loss decreased (0.174567 --> 0.174510).  Saving model ...                                                   \n",
      "it: 180, train loss=0.18329450488090515, validation loss=0.17451037466526031, c=0.6037504833097048, ibs=0.17859891267597686, ibnll=0.5316828599964599\n",
      "Validation loss decreased (0.174510 --> 0.174470).  Saving model ...                                                   \n",
      "it: 181, train loss=0.18323568999767303, validation loss=0.17447003722190857, c=0.6049748678953474, ibs=0.17850599188210126, ibnll=0.5314637293958115\n",
      "Validation loss decreased (0.174470 --> 0.174400).  Saving model ...                                                   \n",
      "it: 182, train loss=0.1831744909286499, validation loss=0.17440026998519897, c=0.6054903982471967, ibs=0.17845556397364018, ibnll=0.5313291378906034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.174400 --> 0.174346).  Saving model ...                                                   \n",
      "it: 183, train loss=0.18310748040676117, validation loss=0.17434588074684143, c=0.6045559994844697, ibs=0.17839229546682167, ibnll=0.531181691479687\n",
      "Validation loss decreased (0.174346 --> 0.174269).  Saving model ...                                                   \n",
      "it: 184, train loss=0.1830366849899292, validation loss=0.17426873743534088, c=0.6058448253640933, ibs=0.1783121303464875, ibnll=0.5309838113442898\n",
      "Validation loss decreased (0.174269 --> 0.174237).  Saving model ...                                                   \n",
      "it: 185, train loss=0.18295246362686157, validation loss=0.17423690855503082, c=0.6077458435365383, ibs=0.1781573074401097, ibnll=0.5306309777256222\n",
      "Validation loss decreased (0.174237 --> 0.174113).  Saving model ...                                                   \n",
      "it: 186, train loss=0.18287187814712524, validation loss=0.1741131842136383, c=0.6049748678953474, ibs=0.1781714665146126, ibnll=0.5306346953374875\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 187, train loss=0.18279603123664856, validation loss=0.17411763966083527, c=0.6077780641835288, ibs=0.17797704066520592, ibnll=0.5302081618883095\n",
      "Validation loss decreased (0.174113 --> 0.173969).  Saving model ...                                                   \n",
      "it: 188, train loss=0.1827162653207779, validation loss=0.17396898567676544, c=0.6074880783606135, ibs=0.17790281372732702, ibnll=0.5300011448141388\n",
      "Validation loss decreased (0.173969 --> 0.173881).  Saving model ...                                                   \n",
      "it: 189, train loss=0.18258748948574066, validation loss=0.17388135194778442, c=0.610677922412682, ibs=0.17767508298272652, ibnll=0.5294826380414084\n",
      "Validation loss decreased (0.173881 --> 0.173751).  Saving model ...                                                   \n",
      "it: 190, train loss=0.18244317173957825, validation loss=0.17375147342681885, c=0.6093568758860678, ibs=0.17758633942927118, ibnll=0.5292595437157145\n",
      "Validation loss decreased (0.173751 --> 0.173676).  Saving model ...                                                   \n",
      "it: 191, train loss=0.1823093742132187, validation loss=0.17367590963840485, c=0.6120956308802681, ibs=0.17733475769004853, ibnll=0.5286682448660448\n",
      "Validation loss decreased (0.173676 --> 0.173530).  Saving model ...                                                   \n",
      "it: 192, train loss=0.18218094110488892, validation loss=0.17353041470050812, c=0.6090346694161619, ibs=0.1772907815095847, ibnll=0.5285721086226318\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 193, train loss=0.18205596506595612, validation loss=0.17354215681552887, c=0.6141255316406753, ibs=0.1768877275278471, ibnll=0.5276360311621346\n",
      "Validation loss decreased (0.173530 --> 0.173263).  Saving model ...                                                   \n",
      "it: 194, train loss=0.18194490671157837, validation loss=0.17326349020004272, c=0.6053937363062251, ibs=0.17711340235252712, ibnll=0.528140577395974\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 195, train loss=0.18186046183109283, validation loss=0.17331776022911072, c=0.6119667482923057, ibs=0.1765043627090424, ibnll=0.5267580563808847\n",
      "Validation loss decreased (0.173263 --> 0.173155).  Saving model ...                                                   \n",
      "it: 196, train loss=0.18165549635887146, validation loss=0.17315460741519928, c=0.6083580358293594, ibs=0.17650406900746743, ibnll=0.5267030046810289\n",
      "Validation loss decreased (0.173155 --> 0.172836).  Saving model ...                                                   \n",
      "it: 197, train loss=0.1815812736749649, validation loss=0.17283621430397034, c=0.6077458435365383, ibs=0.17632542578487578, ibnll=0.5263133973221515\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 198, train loss=0.1812821477651596, validation loss=0.1730968952178955, c=0.6100012888258797, ibs=0.17587481409502376, ibnll=0.5253423192376016\n",
      "Validation loss decreased (0.172836 --> 0.172649).  Saving model ...                                                   \n",
      "it: 199, train loss=0.18134574592113495, validation loss=0.17264921963214874, c=0.6093890965330584, ibs=0.17572411423160103, ibnll=0.5249100442492604\n",
      "                                                                                                                       \n",
      "Results: 0.6033331339832148, 0.18459333284807392, 0.5447963418998149\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 670, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 4, 'num_latent': 95, 'num_odefunc_layers1': 4, 'odefunc_neurons1': 117, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.0001}\n",
      "  4%|                                     | 4/100 [6:24:50<166:54:42, 6259.19s/trial, best loss: 0.16803939640522003]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a928de2f57e43ab994e184c4e64ad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.457116).  Saving model ...                                                        \n",
      "it: 0, train loss=0.47485142946243286, validation loss=0.4571162164211273, c=0.5072620185636313, ibs=0.3254776264875189, ibnll=1.2458792524841242\n",
      "Validation loss decreased (0.457116 --> 0.456174).  Saving model ...                                                   \n",
      "it: 1, train loss=0.47392356395721436, validation loss=0.45617443323135376, c=0.5040550124888218, ibs=0.3252760907626198, ibnll=1.2439282439607586\n",
      "Validation loss decreased (0.456174 --> 0.455063).  Saving model ...                                                   \n",
      "it: 2, train loss=0.47295302152633667, validation loss=0.4550629258155823, c=0.392118165839218, ibs=0.32503133631149966, ibnll=1.2416294926213296\n",
      "Validation loss decreased (0.455063 --> 0.453747).  Saving model ...                                                   \n",
      "it: 3, train loss=0.47180652618408203, validation loss=0.4537470042705536, c=0.4801566499121157, ibs=0.32474310469857404, ibnll=1.2389063569665493\n",
      "Validation loss decreased (0.453747 --> 0.452470).  Saving model ...                                                   \n",
      "it: 4, train loss=0.47044241428375244, validation loss=0.452470064163208, c=0.4597428227820778, ibs=0.32446363734444444, ibnll=1.2362839092760378\n",
      "Validation loss decreased (0.452470 --> 0.450830).  Saving model ...                                                   \n",
      "it: 5, train loss=0.4691300392150879, validation loss=0.4508298933506012, c=0.5319929692559129, ibs=0.32410252562355696, ibnll=1.2329026605059228\n",
      "Validation loss decreased (0.450830 --> 0.449049).  Saving model ...                                                   \n",
      "it: 6, train loss=0.467441588640213, validation loss=0.4490489363670349, c=0.48829751148663236, ibs=0.323708303054778, ibnll=1.229223576050935\n",
      "Validation loss decreased (0.449049 --> 0.447112).  Saving model ...                                                   \n",
      "it: 7, train loss=0.46559664607048035, validation loss=0.44711238145828247, c=0.5723889111597644, ibs=0.3232756605068881, ibnll=1.225196619616951\n",
      "Validation loss decreased (0.447112 --> 0.445073).  Saving model ...                                                   \n",
      "it: 8, train loss=0.4635920524597168, validation loss=0.44507256150245667, c=0.08039100804835178, ibs=0.3228182458957775, ibnll=1.220968685683475\n",
      "Validation loss decreased (0.445073 --> 0.442991).  Saving model ...                                                   \n",
      "it: 9, train loss=0.4614834785461426, validation loss=0.44299131631851196, c=0.5108390638009189, ibs=0.3223495485903489, ibnll=1.2166699264746597\n",
      "Validation loss decreased (0.442991 --> 0.440825).  Saving model ...                                                   \n",
      "it: 10, train loss=0.4593362510204315, validation loss=0.44082456827163696, c=0.31786364056862687, ibs=0.32185801136395376, ibnll=1.212156790396847\n",
      "Validation loss decreased (0.440825 --> 0.438502).  Saving model ...                                                   \n",
      "it: 11, train loss=0.457099050283432, validation loss=0.4385017454624176, c=0.2694193468808782, ibs=0.3213275938812669, ibnll=1.2072959218255133\n",
      "Validation loss decreased (0.438502 --> 0.436159).  Saving model ...                                                   \n",
      "it: 12, train loss=0.45469650626182556, validation loss=0.4361588656902313, c=0.22788245089271947, ibs=0.32078827168778584, ibnll=1.202405694211049\n",
      "Validation loss decreased (0.436159 --> 0.433761).  Saving model ...                                                   \n",
      "it: 13, train loss=0.4522726833820343, validation loss=0.43376123905181885, c=0.3359955595300503, ibs=0.32023156644840145, ibnll=1.1973964378058652\n",
      "Validation loss decreased (0.433761 --> 0.431318).  Saving model ...                                                   \n",
      "it: 14, train loss=0.44979268312454224, validation loss=0.43131786584854126, c=0.16525332264331308, ibs=0.3196590267810035, ibnll=1.192285540576721\n",
      "Validation loss decreased (0.431318 --> 0.428833).  Saving model ...                                                   \n",
      "it: 15, train loss=0.44726431369781494, validation loss=0.42883265018463135, c=0.13006876561102718, ibs=0.3190729689776964, ibnll=1.1870913755913903\n",
      "Validation loss decreased (0.428833 --> 0.426317).  Saving model ...                                                   \n",
      "it: 16, train loss=0.44469520449638367, validation loss=0.4263167679309845, c=0.08550988312929785, ibs=0.31847459167952, ibnll=1.1818210033561865\n",
      "Validation loss decreased (0.426317 --> 0.423778).  Saving model ...                                                   \n",
      "it: 17, train loss=0.442092627286911, validation loss=0.4237784445285797, c=0.08483147799808813, ibs=0.3178653641213818, ibnll=1.1764957544650532\n",
      "Validation loss decreased (0.423778 --> 0.421220).  Saving model ...                                                   \n",
      "it: 18, train loss=0.4394663870334625, validation loss=0.42121997475624084, c=0.07076998982392303, ibs=0.31724653391163454, ibnll=1.1711268439923543\n",
      "Validation loss decreased (0.421220 --> 0.418652).  Saving model ...                                                   \n",
      "it: 19, train loss=0.43682000041007996, validation loss=0.4186519980430603, c=0.2125258256498813, ibs=0.31661943783876734, ibnll=1.165726776451703\n",
      "Validation loss decreased (0.418652 --> 0.416075).  Saving model ...                                                   \n",
      "it: 20, train loss=0.4341629445552826, validation loss=0.416075199842453, c=0.15134601745351384, ibs=0.31598513203106227, ibnll=1.1603036466900927\n",
      "Validation loss decreased (0.416075 --> 0.413497).  Saving model ...                                                   \n",
      "it: 21, train loss=0.4314972758293152, validation loss=0.4134965240955353, c=0.026581146504671745, ibs=0.3153447590217643, ibnll=1.1548700716888634\n",
      "Validation loss decreased (0.413497 --> 0.410921).  Saving model ...                                                   \n",
      "it: 22, train loss=0.42882922291755676, validation loss=0.4109208285808563, c=0.001202627278053594, ibs=0.31469930536955015, ibnll=1.1494347585432207\n",
      "Validation loss decreased (0.410921 --> 0.408351).  Saving model ...                                                   \n",
      "it: 23, train loss=0.4261641800403595, validation loss=0.40835070610046387, c=3.0836596873169075e-05, ibs=0.31404977943150375, ibnll=1.1440059851713167\n",
      "Validation loss decreased (0.408351 --> 0.405793).  Saving model ...                                                   \n",
      "it: 24, train loss=0.42350468039512634, validation loss=0.40579280257225037, c=6.167319374633815e-05, ibs=0.31339722920989177, ibnll=1.1385923147707924\n",
      "Validation loss decreased (0.405793 --> 0.403248).  Saving model ...                                                   \n",
      "it: 25, train loss=0.4208575487136841, validation loss=0.4032479226589203, c=0.0001541829843658454, ibs=0.3127424470950708, ibnll=1.1332008840318177\n",
      "Validation loss decreased (0.403248 --> 0.400721).  Saving model ...                                                   \n",
      "it: 26, train loss=0.4182232618331909, validation loss=0.4007209539413452, c=0.0001233463874926763, ibs=0.3120862414422605, ibnll=1.1278383745109117\n",
      "Validation loss decreased (0.400721 --> 0.398213).  Saving model ...                                                   \n",
      "it: 27, train loss=0.4156078100204468, validation loss=0.3982127010822296, c=3.0836596873169075e-05, ibs=0.31142928898178324, ibnll=1.1225102650178285\n",
      "Validation loss decreased (0.398213 --> 0.395728).  Saving model ...                                                   \n",
      "it: 28, train loss=0.4130115211009979, validation loss=0.39572760462760925, c=9.250979061950723e-05, ibs=0.3107723400085103, ibnll=1.1172218686765913\n",
      "Validation loss decreased (0.395728 --> 0.393265).  Saving model ...                                                   \n",
      "it: 29, train loss=0.4104386568069458, validation loss=0.39326462149620056, c=0.0001233463874926763, ibs=0.31011571298878965, ibnll=1.1119750734517093\n",
      "Validation loss decreased (0.393265 --> 0.390828).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.40788838267326355, validation loss=0.390828013420105, c=9.250979061950723e-05, ibs=0.30945984395985254, ibnll=1.106773119003031\n",
      "Validation loss decreased (0.390828 --> 0.388414).  Saving model ...                                                   \n",
      "it: 31, train loss=0.40536507964134216, validation loss=0.3884141743183136, c=9.250979061950723e-05, ibs=0.3088048864895481, ibnll=1.1016171020675534\n",
      "Validation loss decreased (0.388414 --> 0.386028).  Saving model ...                                                   \n",
      "it: 32, train loss=0.40286552906036377, validation loss=0.386027991771698, c=0.022942428073637795, ibs=0.3081511739101885, ibnll=1.0965082277277292\n",
      "Validation loss decreased (0.386028 --> 0.383666).  Saving model ...                                                   \n",
      "it: 33, train loss=0.4003942608833313, validation loss=0.38366588950157166, c=0.07696814579543002, ibs=0.30749873694999974, ibnll=1.0914467903397318\n",
      "Validation loss decreased (0.383666 --> 0.381329).  Saving model ...                                                   \n",
      "it: 34, train loss=0.3979474604129791, validation loss=0.38132864236831665, c=0.07002991149896698, ibs=0.30684758060188777, ibnll=1.0864319771734599\n",
      "Validation loss decreased (0.381329 --> 0.379017).  Saving model ...                                                   \n",
      "it: 35, train loss=0.395526260137558, validation loss=0.37901708483695984, c=0.09306484936322428, ibs=0.30619772442706694, ibnll=1.0814634197609507\n",
      "Validation loss decreased (0.379017 --> 0.376729).  Saving model ...                                                   \n",
      "it: 36, train loss=0.3931314945220947, validation loss=0.3767291009426117, c=0.04684079065034383, ibs=0.305549152026713, ibnll=1.0765402259714585\n",
      "Validation loss decreased (0.376729 --> 0.374467).  Saving model ...                                                   \n",
      "it: 37, train loss=0.3907606899738312, validation loss=0.3744666874408722, c=0.026581146504671745, ibs=0.30490182884143, ibnll=1.0716617737674825\n",
      "Validation loss decreased (0.374467 --> 0.372226).  Saving model ...                                                   \n",
      "it: 38, train loss=0.3884164094924927, validation loss=0.37222597002983093, c=0.04378796755990009, ibs=0.30425572264980066, ibnll=1.0668272123456046\n",
      "Validation loss decreased (0.372226 --> 0.370012).  Saving model ...                                                   \n",
      "it: 39, train loss=0.38609424233436584, validation loss=0.3700115978717804, c=0.024885133676647447, ibs=0.3036108779277961, ibnll=1.062036401671053\n",
      "Validation loss decreased (0.370012 --> 0.367818).  Saving model ...                                                   \n",
      "it: 40, train loss=0.3837990462779999, validation loss=0.3678179681301117, c=0.03462949828856887, ibs=0.3029673719139677, ibnll=1.0572890810582012\n",
      "Validation loss decreased (0.367818 --> 0.365649).  Saving model ...                                                   \n",
      "it: 41, train loss=0.3815251588821411, validation loss=0.36564934253692627, c=0.05791112892781153, ibs=0.30232522532640593, ibnll=1.0525851822333934\n",
      "Validation loss decreased (0.365649 --> 0.363503).  Saving model ...                                                   \n",
      "it: 42, train loss=0.3792768120765686, validation loss=0.3635026812553406, c=0.03906996823830522, ibs=0.3016845148152638, ibnll=1.0479245525642766\n",
      "Validation loss decreased (0.363503 --> 0.361379).  Saving model ...                                                   \n",
      "it: 43, train loss=0.3770512640476227, validation loss=0.36137935519218445, c=0.3020136297758179, ibs=0.3010453241252834, ibnll=1.0433074754587142\n",
      "Validation loss decreased (0.361379 --> 0.359279).  Saving model ...                                                   \n",
      "it: 44, train loss=0.37484949827194214, validation loss=0.35927945375442505, c=0.08372136051065404, ibs=0.30040778584525896, ibnll=1.0387341910628123\n",
      "Validation loss decreased (0.359279 --> 0.357203).  Saving model ...                                                   \n",
      "it: 45, train loss=0.3726717233657837, validation loss=0.35720258951187134, c=0.04745752258780721, ibs=0.2997719469484961, ibnll=1.0342044238630992\n",
      "Validation loss decreased (0.357203 --> 0.355149).  Saving model ...                                                   \n",
      "it: 46, train loss=0.3705176115036011, validation loss=0.3551487326622009, c=0.07021493108020599, ibs=0.2991379142637092, ibnll=1.029718591843204\n",
      "Validation loss decreased (0.355149 --> 0.353119).  Saving model ...                                                   \n",
      "it: 47, train loss=0.36838704347610474, validation loss=0.35311850905418396, c=0.1616454408091523, ibs=0.29850576924354616, ibnll=1.0252763273299692\n",
      "Validation loss decreased (0.353119 --> 0.351110).  Saving model ...                                                   \n",
      "it: 48, train loss=0.3662808835506439, validation loss=0.3511102795600891, c=0.12627586419562736, ibs=0.2978755282530145, ibnll=1.0208776587083428\n",
      "Validation loss decreased (0.351110 --> 0.349126).  Saving model ...                                                   \n",
      "it: 49, train loss=0.3641971945762634, validation loss=0.3491261601448059, c=0.26204940022819084, ibs=0.29724730087348733, ibnll=1.0165222995818055\n",
      "Validation loss decreased (0.349126 --> 0.347163).  Saving model ...                                                   \n",
      "it: 50, train loss=0.36213821172714233, validation loss=0.3471631109714508, c=0.3132073144407783, ibs=0.29662100219964954, ibnll=1.0122097806859074\n",
      "Validation loss decreased (0.347163 --> 0.345224).  Saving model ...                                                   \n",
      "it: 51, train loss=0.3601011037826538, validation loss=0.3452235162258148, c=0.33371365136143577, ibs=0.29599682472122135, ibnll=1.007940082903789\n",
      "Validation loss decreased (0.345224 --> 0.343305).  Saving model ...                                                   \n",
      "it: 52, train loss=0.35808783769607544, validation loss=0.3433053493499756, c=0.11841253199296925, ibs=0.29537468212900225, ibnll=1.003712650641889\n",
      "Validation loss decreased (0.343305 --> 0.341409).  Saving model ...                                                   \n",
      "it: 53, train loss=0.35609668493270874, validation loss=0.3414091169834137, c=0.16013444756236703, ibs=0.29475458550857475, ibnll=0.9995269541399294\n",
      "Validation loss decreased (0.341409 --> 0.339535).  Saving model ...                                                   \n",
      "it: 54, train loss=0.35412803292274475, validation loss=0.3395349383354187, c=0.11246106879644763, ibs=0.29413663874176865, ibnll=0.9953828276822986\n",
      "Validation loss decreased (0.339535 --> 0.337681).  Saving model ...                                                   \n",
      "it: 55, train loss=0.35218191146850586, validation loss=0.33768120408058167, c=0.1248573807394616, ibs=0.29352074260441297, ibnll=0.9912794372319995\n",
      "Validation loss decreased (0.337681 --> 0.335850).  Saving model ...                                                   \n",
      "it: 56, train loss=0.35025694966316223, validation loss=0.3358500301837921, c=0.13648277776064632, ibs=0.29290698455896613, ibnll=0.9872165370362144\n",
      "Validation loss decreased (0.335850 --> 0.334038).  Saving model ...                                                   \n",
      "it: 57, train loss=0.3483550250530243, validation loss=0.33403754234313965, c=0.24845046100712326, ibs=0.2922952745015936, ibnll=0.983193485479337\n",
      "Validation loss decreased (0.334038 --> 0.332248).  Saving model ...                                                   \n",
      "it: 58, train loss=0.3464723527431488, validation loss=0.33224809169769287, c=0.4305097289463135, ibs=0.2916856489353045, ibnll=0.9792097185329276\n",
      "Validation loss decreased (0.332248 --> 0.330477).  Saving model ...                                                   \n",
      "it: 59, train loss=0.34461331367492676, validation loss=0.3304767608642578, c=0.5317154398840543, ibs=0.2910782506972635, ibnll=0.975265916950134\n",
      "Validation loss decreased (0.330477 --> 0.328727).  Saving model ...                                                   \n",
      "it: 60, train loss=0.34277305006980896, validation loss=0.3287266492843628, c=0.11295445434641833, ibs=0.29047285653591093, ibnll=0.9713595086778305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.328727 --> 0.326996).  Saving model ...                                                   \n",
      "it: 61, train loss=0.34095439314842224, validation loss=0.3269955813884735, c=0.1537821086064942, ibs=0.28986960723875677, ibnll=0.9674917427357528\n",
      "Validation loss decreased (0.326996 --> 0.325284).  Saving model ...                                                   \n",
      "it: 62, train loss=0.3391551375389099, validation loss=0.32528379559516907, c=0.18292269265163896, ibs=0.28926851276842636, ibnll=0.9636616062874855\n",
      "Validation loss decreased (0.325284 --> 0.323592).  Saving model ...                                                   \n",
      "it: 63, train loss=0.33737602829933167, validation loss=0.32359206676483154, c=0.1441919269789386, ibs=0.28866947263209836, ibnll=0.9598684833316281\n",
      "Validation loss decreased (0.323592 --> 0.321918).  Saving model ...                                                   \n",
      "it: 64, train loss=0.33561739325523376, validation loss=0.32191795110702515, c=0.2944895001387647, ibs=0.2880725877115933, ibnll=0.9561121270248011\n",
      "Validation loss decreased (0.321918 --> 0.320264).  Saving model ...                                                   \n",
      "it: 65, train loss=0.33387690782546997, validation loss=0.32026413083076477, c=0.21508526319035431, ibs=0.2874777932626559, ibnll=0.952392018487533\n",
      "Validation loss decreased (0.320264 --> 0.318627).  Saving model ...                                                   \n",
      "it: 66, train loss=0.3321572244167328, validation loss=0.31862732768058777, c=0.1420950383915631, ibs=0.2868851206724753, ibnll=0.948707823837564\n",
      "Validation loss decreased (0.318627 --> 0.317010).  Saving model ...                                                   \n",
      "it: 67, train loss=0.33045506477355957, validation loss=0.31701040267944336, c=0.0849548243855808, ibs=0.28629459836966825, ibnll=0.9450590743547748\n",
      "Validation loss decreased (0.317010 --> 0.315410).  Saving model ...                                                   \n",
      "it: 68, train loss=0.32877317070961, validation loss=0.3154098689556122, c=0.11690153874618398, ibs=0.28570616431502494, ibnll=0.9414453539249159\n",
      "Validation loss decreased (0.315410 --> 0.313828).  Saving model ...                                                   \n",
      "it: 69, train loss=0.3271084129810333, validation loss=0.31382831931114197, c=0.26322119090937124, ibs=0.28511979975479196, ibnll=0.9378660656022099\n",
      "Validation loss decreased (0.313828 --> 0.312263).  Saving model ...                                                   \n",
      "it: 70, train loss=0.32546302676200867, validation loss=0.31226348876953125, c=0.10472108298128219, ibs=0.28453564541679427, ibnll=0.9343212837971109\n",
      "Validation loss decreased (0.312263 --> 0.310717).  Saving model ...                                                   \n",
      "it: 71, train loss=0.3238348066806793, validation loss=0.3107165992259979, c=0.12491905393320793, ibs=0.28395356900676066, ibnll=0.9308101904456284\n",
      "Validation loss decreased (0.310717 --> 0.309186).  Saving model ...                                                   \n",
      "it: 72, train loss=0.322224885225296, validation loss=0.30918633937835693, c=0.09753615590983379, ibs=0.28337366495821714, ibnll=0.9273325949141544\n",
      "Validation loss decreased (0.309186 --> 0.307674).  Saving model ...                                                   \n",
      "it: 73, train loss=0.3206322193145752, validation loss=0.30767354369163513, c=0.14625797896944093, ibs=0.28279583459639557, ibnll=0.923887995589158\n",
      "Validation loss decreased (0.307674 --> 0.306177).  Saving model ...                                                   \n",
      "it: 74, train loss=0.3190575838088989, validation loss=0.30617693066596985, c=0.30700915846927135, ibs=0.28222012080952014, ibnll=0.9204760665168703\n",
      "Validation loss decreased (0.306177 --> 0.304697).  Saving model ...                                                   \n",
      "it: 75, train loss=0.3174993693828583, validation loss=0.3046974837779999, c=0.22939344413950477, ibs=0.28164657563691764, ibnll=0.91709650601835\n",
      "Validation loss decreased (0.304697 --> 0.303234).  Saving model ...                                                   \n",
      "it: 76, train loss=0.31595882773399353, validation loss=0.3032337427139282, c=0.11631564340559376, ibs=0.28107513863384165, ibnll=0.9137488119617038\n",
      "Validation loss decreased (0.303234 --> 0.301786).  Saving model ...                                                   \n",
      "it: 77, train loss=0.3144344091415405, validation loss=0.3017861843109131, c=0.1569582780844306, ibs=0.28050581606628455, ibnll=0.910432590617817\n",
      "Validation loss decreased (0.301786 --> 0.300354).  Saving model ...                                                   \n",
      "it: 78, train loss=0.31292685866355896, validation loss=0.30035439133644104, c=0.11989268864288137, ibs=0.27993860533631526, ibnll=0.9071475803897983\n",
      "Validation loss decreased (0.300354 --> 0.298939).  Saving model ...                                                   \n",
      "it: 79, train loss=0.3114352822303772, validation loss=0.2989387810230255, c=0.39831632181072496, ibs=0.2793735482271263, ibnll=0.903893565207329\n",
      "Validation loss decreased (0.298939 --> 0.297538).  Saving model ...                                                   \n",
      "it: 80, train loss=0.3099604547023773, validation loss=0.2975381910800934, c=0.26759998766536125, ibs=0.2788105933285035, ibnll=0.9006699810005577\n",
      "Validation loss decreased (0.297538 --> 0.296153).  Saving model ...                                                   \n",
      "it: 81, train loss=0.30850109457969666, validation loss=0.2961534261703491, c=0.28835301736100405, ibs=0.27824977787128025, ibnll=0.8974765853806226\n",
      "Validation loss decreased (0.296153 --> 0.294783).  Saving model ...                                                   \n",
      "it: 82, train loss=0.30705782771110535, validation loss=0.294783353805542, c=0.19510314841654075, ibs=0.27769115079262424, ibnll=0.8943131839459303\n",
      "Validation loss decreased (0.294783 --> 0.293429).  Saving model ...                                                   \n",
      "it: 83, train loss=0.3056298494338989, validation loss=0.2934291660785675, c=0.30349378642573005, ibs=0.27713461825537333, ibnll=0.8911792710052621\n",
      "Validation loss decreased (0.293429 --> 0.292088).  Saving model ...                                                   \n",
      "it: 84, train loss=0.304218053817749, validation loss=0.2920882999897003, c=0.17413426254278577, ibs=0.2765801820457613, ibnll=0.8880744619883846\n",
      "Validation loss decreased (0.292088 --> 0.290763).  Saving model ...                                                   \n",
      "it: 85, train loss=0.3028199374675751, validation loss=0.2907629609107971, c=0.3604181442536002, ibs=0.2760279007375301, ibnll=0.8849985007616575\n",
      "Validation loss decreased (0.290763 --> 0.289452).  Saving model ...                                                   \n",
      "it: 86, train loss=0.30143800377845764, validation loss=0.28945204615592957, c=0.27756020845539486, ibs=0.27547779575901155, ibnll=0.8819513064560288\n",
      "Validation loss decreased (0.289452 --> 0.288155).  Saving model ...                                                   \n",
      "it: 87, train loss=0.3000706732273102, validation loss=0.2881549894809723, c=0.3331894292145919, ibs=0.2749297887418834, ibnll=0.8789322687448873\n",
      "Validation loss decreased (0.288155 --> 0.286873).  Saving model ...                                                   \n",
      "it: 88, train loss=0.29871776700019836, validation loss=0.2868725061416626, c=0.1482931943630701, ibs=0.27438392840076364, ibnll=0.8759413151004091\n",
      "Validation loss decreased (0.286873 --> 0.285604).  Saving model ...                                                   \n",
      "it: 89, train loss=0.2973797619342804, validation loss=0.28560370206832886, c=0.37340035153720436, ibs=0.2738402529299104, ibnll=0.8729781100142819\n",
      "Validation loss decreased (0.285604 --> 0.284349).  Saving model ...                                                   \n",
      "it: 90, train loss=0.29605570435523987, validation loss=0.2843491733074188, c=0.23861358660458232, ibs=0.27329871350095075, ibnll=0.8700422915684355\n",
      "Validation loss decreased (0.284349 --> 0.283107).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.2947465181350708, validation loss=0.28310710191726685, c=0.10302507015325789, ibs=0.2727593360198529, ibnll=0.8671337115933505\n",
      "Validation loss decreased (0.283107 --> 0.281881).  Saving model ...                                                   \n",
      "it: 92, train loss=0.2934500575065613, validation loss=0.28188079595565796, c=0.28992568380153566, ibs=0.27222206885854316, ibnll=0.8642517741414146\n",
      "Validation loss decreased (0.281881 --> 0.280665).  Saving model ...                                                   \n",
      "it: 93, train loss=0.29216986894607544, validation loss=0.2806648313999176, c=0.2937802584106818, ibs=0.27168700484376224, ibnll=0.8613967012360277\n",
      "Validation loss decreased (0.280665 --> 0.279466).  Saving model ...                                                   \n",
      "it: 94, train loss=0.29090023040771484, validation loss=0.2794656753540039, c=0.11634648000246693, ibs=0.2711540765728264, ibnll=0.8585678012380427\n",
      "Validation loss decreased (0.279466 --> 0.278277).  Saving model ...                                                   \n",
      "it: 95, train loss=0.2896479666233063, validation loss=0.2782767713069916, c=0.1700329951586543, ibs=0.2706233423405794, ibnll=0.8557651054786946\n",
      "Validation loss decreased (0.278277 --> 0.277101).  Saving model ...                                                   \n",
      "it: 96, train loss=0.28840625286102295, validation loss=0.2771013379096985, c=0.07197261710197662, ibs=0.2700947454149412, ibnll=0.8529880822993567\n",
      "Validation loss decreased (0.277101 --> 0.275939).  Saving model ...                                                   \n",
      "it: 97, train loss=0.2871783673763275, validation loss=0.2759392559528351, c=0.20296648061919886, ibs=0.2695683299964571, ibnll=0.8502366815850728\n",
      "Validation loss decreased (0.275939 --> 0.274788).  Saving model ...                                                   \n",
      "it: 98, train loss=0.28596410155296326, validation loss=0.27478837966918945, c=0.17160566159918592, ibs=0.26904410670230233, ibnll=0.8475106505362386\n",
      "Validation loss decreased (0.274788 --> 0.273653).  Saving model ...                                                   \n",
      "it: 99, train loss=0.2847614884376526, validation loss=0.273652583360672, c=0.2774985352616485, ibs=0.26852207456374455, ibnll=0.8448096658697404\n",
      "Validation loss decreased (0.273653 --> 0.272527).  Saving model ...                                                   \n",
      "it: 100, train loss=0.2835744023323059, validation loss=0.27252712845802307, c=0.21073730303123747, ibs=0.26800219907618983, ibnll=0.8421335706543889\n",
      "Validation loss decreased (0.272527 --> 0.271414).  Saving model ...                                                   \n",
      "it: 101, train loss=0.28239792585372925, validation loss=0.27141377329826355, c=0.20435412747849147, ibs=0.2674845397624148, ibnll=0.8394820215679871\n",
      "Validation loss decreased (0.271414 --> 0.270313).  Saving model ...                                                   \n",
      "it: 102, train loss=0.2812339961528778, validation loss=0.2703133225440979, c=0.31394739276573436, ibs=0.26696908069863, ibnll=0.8368549634078044\n",
      "Validation loss decreased (0.270313 --> 0.269224).  Saving model ...                                                   \n",
      "it: 103, train loss=0.2800831198692322, validation loss=0.2692243158817291, c=0.35317154398840545, ibs=0.2664557933470773, ibnll=0.8342519328558355\n",
      "Validation loss decreased (0.269224 --> 0.268147).  Saving model ...                                                   \n",
      "it: 104, train loss=0.27894413471221924, validation loss=0.26814717054367065, c=0.2870578802923309, ibs=0.2659447058878169, ibnll=0.8316728563986466\n",
      "Validation loss decreased (0.268147 --> 0.267082).  Saving model ...                                                   \n",
      "it: 105, train loss=0.27781742811203003, validation loss=0.26708221435546875, c=0.40451447778223193, ibs=0.26543589380082905, ibnll=0.8291176693131309\n",
      "Validation loss decreased (0.267082 --> 0.266028).  Saving model ...                                                   \n",
      "it: 106, train loss=0.2767030596733093, validation loss=0.2660280168056488, c=0.4123161367911437, ibs=0.26492925129252304, ibnll=0.8265858417693195\n",
      "Validation loss decreased (0.266028 --> 0.264987).  Saving model ...                                                   \n",
      "it: 107, train loss=0.27559995651245117, validation loss=0.26498663425445557, c=0.47620956551235005, ibs=0.2644248373987306, ibnll=0.8240774235073663\n",
      "Validation loss decreased (0.264987 --> 0.263955).  Saving model ...                                                   \n",
      "it: 108, train loss=0.274509996175766, validation loss=0.263954758644104, c=0.486416479077369, ibs=0.263922598385612, ibnll=0.8215917247160053\n",
      "Validation loss decreased (0.263955 --> 0.262937).  Saving model ...                                                   \n",
      "it: 109, train loss=0.273429811000824, validation loss=0.26293691992759705, c=0.38820191803632553, ibs=0.26342261441460596, ibnll=0.8191288903031442\n",
      "Validation loss decreased (0.262937 --> 0.261928).  Saving model ...                                                   \n",
      "it: 110, train loss=0.2723641097545624, validation loss=0.26192763447761536, c=0.33013660612414814, ibs=0.2629248323521619, ibnll=0.8166886727852746\n",
      "Validation loss decreased (0.261928 --> 0.260931).  Saving model ...                                                   \n",
      "it: 111, train loss=0.27130720019340515, validation loss=0.2609308958053589, c=0.3377532455518209, ibs=0.26242932160147525, ibnll=0.8142710483600855\n",
      "Validation loss decreased (0.260931 --> 0.259944).  Saving model ...                                                   \n",
      "it: 112, train loss=0.27026301622390747, validation loss=0.25994381308555603, c=0.42619260538406983, ibs=0.2619359935493333, ibnll=0.8118754677704499\n",
      "Validation loss decreased (0.259944 --> 0.258968).  Saving model ...                                                   \n",
      "it: 113, train loss=0.26922911405563354, validation loss=0.25896844267845154, c=0.3544975176539517, ibs=0.26144499706047086, ibnll=0.8095021853745773\n",
      "Validation loss decreased (0.258968 --> 0.258003).  Saving model ...                                                   \n",
      "it: 114, train loss=0.2682071030139923, validation loss=0.2580025792121887, c=0.43146566344938175, ibs=0.2609561988845672, ibnll=0.8071506386482723\n",
      "Validation loss decreased (0.258003 --> 0.257049).  Saving model ...                                                   \n",
      "it: 115, train loss=0.26719480752944946, validation loss=0.25704917311668396, c=0.45181781738567334, ibs=0.26046965217544127, ibnll=0.8048206870418446\n",
      "Validation loss decreased (0.257049 --> 0.256104).  Saving model ...                                                   \n",
      "it: 116, train loss=0.2661953866481781, validation loss=0.2561040222644806, c=0.426408461562182, ibs=0.25998536085877716, ibnll=0.8025121544362197\n",
      "Validation loss decreased (0.256104 --> 0.255172).  Saving model ...                                                   \n",
      "it: 117, train loss=0.26520460844039917, validation loss=0.2551717162132263, c=0.46732862561287736, ibs=0.25950328695982194, ibnll=0.800224637707806\n",
      "Validation loss decreased (0.255172 --> 0.254248).  Saving model ...                                                   \n",
      "it: 118, train loss=0.2642268240451813, validation loss=0.25424784421920776, c=0.42474328533103084, ibs=0.2590235290562526, ibnll=0.7979583455945903\n",
      "Validation loss decreased (0.254248 --> 0.253334).  Saving model ...                                                   \n",
      "it: 119, train loss=0.2632578909397125, validation loss=0.2533339560031891, c=0.4090166209257146, ibs=0.2585459798986991, ibnll=0.795712778047855\n",
      "Validation loss decreased (0.253334 --> 0.252431).  Saving model ...                                                   \n",
      "it: 120, train loss=0.2622992992401123, validation loss=0.2524309754371643, c=0.46214807733818497, ibs=0.2580707512886821, ibnll=0.7934880863343804\n",
      "Validation loss decreased (0.252431 --> 0.251537).  Saving model ...                                                   \n",
      "it: 121, train loss=0.26135173439979553, validation loss=0.251537024974823, c=0.45727589503222427, ibs=0.2575978004040882, ibnll=0.7912838956040688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.251537 --> 0.250654).  Saving model ...                                                   \n",
      "it: 122, train loss=0.26041358709335327, validation loss=0.2506539821624756, c=0.43572111381787904, ibs=0.257127095979917, ibnll=0.7890999297335682\n",
      "Validation loss decreased (0.250654 --> 0.249780).  Saving model ...                                                   \n",
      "it: 123, train loss=0.25948670506477356, validation loss=0.24977964162826538, c=0.4907336026396127, ibs=0.25665869137794006, ibnll=0.7869361541636912\n",
      "Validation loss decreased (0.249780 --> 0.248916).  Saving model ...                                                   \n",
      "it: 124, train loss=0.25856876373291016, validation loss=0.2489161491394043, c=0.5055660057356071, ibs=0.2561925533552937, ibnll=0.7847922959583358\n",
      "Validation loss decreased (0.248916 --> 0.248061).  Saving model ...                                                   \n",
      "it: 125, train loss=0.25766199827194214, validation loss=0.24806095659732819, c=0.45804680995405345, ibs=0.2557286819766528, ibnll=0.782668092364972\n",
      "Validation loss decreased (0.248061 --> 0.247216).  Saving model ...                                                   \n",
      "it: 126, train loss=0.25676390528678894, validation loss=0.24721582233905792, c=0.4578309537759413, ibs=0.2552670929750282, ibnll=0.780563539454661\n",
      "Validation loss decreased (0.247216 --> 0.246380).  Saving model ...                                                   \n",
      "it: 127, train loss=0.2558760941028595, validation loss=0.24637986719608307, c=0.4918128835301736, ibs=0.2548077707833523, ibnll=0.7784783514190249\n",
      "Validation loss decreased (0.246380 --> 0.245553).  Saving model ...                                                   \n",
      "it: 128, train loss=0.25499776005744934, validation loss=0.24555331468582153, c=0.4844429368774862, ibs=0.2543507720975801, ibnll=0.7764125433521744\n",
      "Validation loss decreased (0.245553 --> 0.244736).  Saving model ...                                                   \n",
      "it: 129, train loss=0.25412917137145996, validation loss=0.24473606050014496, c=0.4746677356686916, ibs=0.25389608525208224, ibnll=0.7743658838035922\n",
      "Validation loss decreased (0.244736 --> 0.243927).  Saving model ...                                                   \n",
      "it: 130, train loss=0.2532700002193451, validation loss=0.2439272701740265, c=0.505380986154368, ibs=0.2534437021669872, ibnll=0.7723382638631474\n",
      "Validation loss decreased (0.243927 --> 0.243128).  Saving model ...                                                   \n",
      "it: 131, train loss=0.2524195909500122, validation loss=0.24312825500965118, c=0.507755404113602, ibs=0.25299358224228496, ibnll=0.7703292664696902\n",
      "Validation loss decreased (0.243128 --> 0.242338).  Saving model ...                                                   \n",
      "it: 132, train loss=0.2515793740749359, validation loss=0.24233753979206085, c=0.5171605661599186, ibs=0.2525457582371548, ibnll=0.7683387996110459\n",
      "Validation loss decreased (0.242338 --> 0.241556).  Saving model ...                                                   \n",
      "it: 133, train loss=0.2507476508617401, validation loss=0.24155594408512115, c=0.55006321502359, ibs=0.25210017090807907, ibnll=0.7663664400330689\n",
      "Validation loss decreased (0.241556 --> 0.240782).  Saving model ...                                                   \n",
      "it: 134, train loss=0.24992527067661285, validation loss=0.2407824546098709, c=0.48839002127725184, ibs=0.25165700948955694, ibnll=0.7644130397254311\n",
      "Validation loss decreased (0.240782 --> 0.240018).  Saving model ...                                                   \n",
      "it: 135, train loss=0.24911142885684967, validation loss=0.24001817405223846, c=0.5087730118104166, ibs=0.25121613182631036, ibnll=0.7624775797921913\n",
      "Validation loss decreased (0.240018 --> 0.239263).  Saving model ...                                                   \n",
      "it: 136, train loss=0.2483070343732834, validation loss=0.23926255106925964, c=0.5287859631811034, ibs=0.2507775288693118, ibnll=0.7605599194575065\n",
      "Validation loss decreased (0.239263 --> 0.238515).  Saving model ...                                                   \n",
      "it: 137, train loss=0.24751141667366028, validation loss=0.23851467669010162, c=0.5153720435412747, ibs=0.2503413268539687, ibnll=0.7586605379774706\n",
      "Validation loss decreased (0.238515 --> 0.237777).  Saving model ...                                                   \n",
      "it: 138, train loss=0.2467239797115326, validation loss=0.2377765029668808, c=0.5423232292084246, ibs=0.24990735134808958, ibnll=0.7567784479125078\n",
      "Validation loss decreased (0.237777 --> 0.237045).  Saving model ...                                                   \n",
      "it: 139, train loss=0.24594643712043762, validation loss=0.23704476654529572, c=0.5743007801659009, ibs=0.24947563088025496, ibnll=0.7549136263673927\n",
      "Validation loss decreased (0.237045 --> 0.236323).  Saving model ...                                                   \n",
      "it: 140, train loss=0.2451755553483963, validation loss=0.23632293939590454, c=0.5142927626507139, ibs=0.2490462127055513, ibnll=0.753066085019294\n",
      "Validation loss decreased (0.236323 --> 0.235608).  Saving model ...                                                   \n",
      "it: 141, train loss=0.24441492557525635, validation loss=0.23560795187950134, c=0.48120509420580343, ibs=0.2486192174948299, ibnll=0.7512363006933613\n",
      "Validation loss decreased (0.235608 --> 0.234902).  Saving model ...                                                   \n",
      "it: 142, train loss=0.24366147816181183, validation loss=0.234901562333107, c=0.4717999321594869, ibs=0.24819445095619772, ibnll=0.7494231519057826\n",
      "Validation loss decreased (0.234902 --> 0.234203).  Saving model ...                                                   \n",
      "it: 143, train loss=0.24291682243347168, validation loss=0.23420287668704987, c=0.5146628018131919, ibs=0.24777204829150545, ibnll=0.7476270621289363\n",
      "Validation loss decreased (0.234203 --> 0.233512).  Saving model ...                                                   \n",
      "it: 144, train loss=0.24218004941940308, validation loss=0.23351244628429413, c=0.528724289987357, ibs=0.24735200863642673, ibnll=0.7458479694494862\n",
      "Validation loss decreased (0.233512 --> 0.232829).  Saving model ...                                                   \n",
      "it: 145, train loss=0.24145178496837616, validation loss=0.23282940685749054, c=0.537111844336859, ibs=0.24693417031572476, ibnll=0.7440851027643427\n",
      "Validation loss decreased (0.232829 --> 0.232155).  Saving model ...                                                   \n",
      "it: 146, train loss=0.24073123931884766, validation loss=0.2321545034646988, c=0.5169755465786796, ibs=0.2465186568423308, ibnll=0.7423386897907505\n",
      "Validation loss decreased (0.232155 --> 0.231486).  Saving model ...                                                   \n",
      "it: 147, train loss=0.2400190681219101, validation loss=0.2314864844083786, c=0.44808658916401983, ibs=0.24610548208932814, ibnll=0.7406087869436713\n",
      "Validation loss decreased (0.231486 --> 0.230828).  Saving model ...                                                   \n",
      "it: 148, train loss=0.2393140196800232, validation loss=0.23082779347896576, c=0.45376052298868297, ibs=0.24569464099868601, ibnll=0.7388951152139046\n",
      "Validation loss decreased (0.230828 --> 0.230174).  Saving model ...                                                   \n",
      "it: 149, train loss=0.23861853778362274, validation loss=0.2301741987466812, c=0.49061025625212, ibs=0.24528607455279455, ibnll=0.7371973161217749\n",
      "Validation loss decreased (0.230174 --> 0.229530).  Saving model ...                                                   \n",
      "it: 150, train loss=0.2379283905029297, validation loss=0.2295299470424652, c=0.5320546424496593, ibs=0.24487983709324293, ibnll=0.7355155473520475\n",
      "Validation loss decreased (0.229530 --> 0.228892).  Saving model ...                                                   \n",
      "it: 151, train loss=0.23724780976772308, validation loss=0.22889217734336853, c=0.5211693237534306, ibs=0.2444758961845924, ibnll=0.7338495023461377\n",
      "Validation loss decreased (0.228892 --> 0.228261).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 152, train loss=0.23657405376434326, validation loss=0.22826065123081207, c=0.4926763082426223, ibs=0.2440742211868528, ibnll=0.7321989246182227\n",
      "Validation loss decreased (0.228261 --> 0.227639).  Saving model ...                                                   \n",
      "it: 153, train loss=0.23590673506259918, validation loss=0.22763873636722565, c=0.48191433593388633, ibs=0.24367487713375466, ibnll=0.7305638836669488\n",
      "Validation loss decreased (0.227639 --> 0.227021).  Saving model ...                                                   \n",
      "it: 154, train loss=0.235249325633049, validation loss=0.22702129185199738, c=0.45598075796355114, ibs=0.2432778393279812, ibnll=0.7289442478958744\n",
      "Validation loss decreased (0.227021 --> 0.226414).  Saving model ...                                                   \n",
      "it: 155, train loss=0.23459652066230774, validation loss=0.22641350328922272, c=0.49831940547041226, ibs=0.2428830818695977, ibnll=0.7273397129348909\n",
      "Validation loss decreased (0.226414 --> 0.225811).  Saving model ...                                                   \n",
      "it: 156, train loss=0.23395361006259918, validation loss=0.22581082582473755, c=0.4775663757747695, ibs=0.24249064392768027, ibnll=0.7257503370896338\n",
      "Validation loss decreased (0.225811 --> 0.225215).  Saving model ...                                                   \n",
      "it: 157, train loss=0.23331613838672638, validation loss=0.22521521151065826, c=0.5026673656295291, ibs=0.2421005014871112, ibnll=0.7241759030645025\n",
      "Validation loss decreased (0.225215 --> 0.224627).  Saving model ...                                                   \n",
      "it: 158, train loss=0.232685849070549, validation loss=0.22462694346904755, c=0.49153535415831506, ibs=0.24171262783187078, ibnll=0.7226161895999619\n",
      "Validation loss decreased (0.224627 --> 0.224045).  Saving model ...                                                   \n",
      "it: 159, train loss=0.23206324875354767, validation loss=0.22404451668262482, c=0.48213019211199853, ibs=0.2413270357353985, ibnll=0.7210711685744211\n",
      "Validation loss decreased (0.224045 --> 0.223470).  Saving model ...                                                   \n",
      "it: 160, train loss=0.23144663870334625, validation loss=0.22347012162208557, c=0.45218785654815136, ibs=0.2409437481430278, ibnll=0.7195407329537474\n",
      "Validation loss decreased (0.223470 --> 0.222901).  Saving model ...                                                   \n",
      "it: 161, train loss=0.230838343501091, validation loss=0.22290116548538208, c=0.4696722069752382, ibs=0.24056273458835165, ibnll=0.7180247008475031\n",
      "Validation loss decreased (0.222901 --> 0.222339).  Saving model ...                                                   \n",
      "it: 162, train loss=0.23023569583892822, validation loss=0.22233937680721283, c=0.441364211045669, ibs=0.2401840308677878, ibnll=0.7165230250263025\n",
      "Validation loss decreased (0.222339 --> 0.221784).  Saving model ...                                                   \n",
      "it: 163, train loss=0.229640394449234, validation loss=0.22178371250629425, c=0.4845046100712325, ibs=0.23980756236322628, ibnll=0.715035379620578\n",
      "Validation loss decreased (0.221784 --> 0.221235).  Saving model ...                                                   \n",
      "it: 164, train loss=0.22905153036117554, validation loss=0.22123456001281738, c=0.48317863640568626, ibs=0.2394334105669674, ibnll=0.7135619453903989\n",
      "Validation loss decreased (0.221235 --> 0.220692).  Saving model ...                                                   \n",
      "it: 165, train loss=0.2284693419933319, validation loss=0.22069185972213745, c=0.4546547842980049, ibs=0.23906152459331195, ibnll=0.7121023834008421\n",
      "Validation loss decreased (0.220692 --> 0.220155).  Saving model ...                                                   \n",
      "it: 166, train loss=0.22789379954338074, validation loss=0.22015509009361267, c=0.4693638410065065, ibs=0.23869190066015858, ibnll=0.7106566130360527\n",
      "Validation loss decreased (0.220155 --> 0.219626).  Saving model ...                                                   \n",
      "it: 167, train loss=0.2273244559764862, validation loss=0.21962584555149078, c=0.45372968639180977, ibs=0.2383245510389315, ibnll=0.7092245483862362\n",
      "Validation loss decreased (0.219626 --> 0.219101).  Saving model ...                                                   \n",
      "it: 168, train loss=0.22676284611225128, validation loss=0.21910089254379272, c=0.46245644330691665, ibs=0.2379594340442895, ibnll=0.7078059694387062\n",
      "Validation loss decreased (0.219101 --> 0.218584).  Saving model ...                                                   \n",
      "it: 169, train loss=0.2262057363986969, validation loss=0.21858413517475128, c=0.4568133460791267, ibs=0.2375966075545903, ibnll=0.7064009276841139\n",
      "Validation loss decreased (0.218584 --> 0.218072).  Saving model ...                                                   \n",
      "it: 170, train loss=0.22565706074237823, validation loss=0.2180715799331665, c=0.4685929260846773, ibs=0.2372360279808273, ibnll=0.7050091676248269\n",
      "Validation loss decreased (0.218072 --> 0.217566).  Saving model ...                                                   \n",
      "it: 171, train loss=0.22511275112628937, validation loss=0.217565655708313, c=0.4566283264978877, ibs=0.23687767706987023, ibnll=0.7036305721221564\n",
      "Validation loss decreased (0.217566 --> 0.217066).  Saving model ...                                                   \n",
      "it: 172, train loss=0.22457537055015564, validation loss=0.217065691947937, c=0.4579234635665608, ibs=0.23652158109177587, ibnll=0.702265075885802\n",
      "Validation loss decreased (0.217066 --> 0.216571).  Saving model ...                                                   \n",
      "it: 173, train loss=0.22404414415359497, validation loss=0.21657083928585052, c=0.4637824169724629, ibs=0.2361677266245758, ibnll=0.7009125570929917\n",
      "Validation loss decreased (0.216571 --> 0.216082).  Saving model ...                                                   \n",
      "it: 174, train loss=0.22351816296577454, validation loss=0.21608246862888336, c=0.46405994634432146, ibs=0.23581612951563935, ibnll=0.6995729316938898\n",
      "Validation loss decreased (0.216082 --> 0.215600).  Saving model ...                                                   \n",
      "it: 175, train loss=0.22299890220165253, validation loss=0.21560026705265045, c=0.4852446883961886, ibs=0.23546674558050462, ibnll=0.6982460332710014\n",
      "Validation loss decreased (0.215600 --> 0.215123).  Saving model ...                                                   \n",
      "it: 176, train loss=0.2224859893321991, validation loss=0.21512266993522644, c=0.4591877640383607, ibs=0.235119591459416, ibnll=0.6969317485327787\n",
      "Validation loss decreased (0.215123 --> 0.214652).  Saving model ...                                                   \n",
      "it: 177, train loss=0.22197800874710083, validation loss=0.2146516889333725, c=0.4637824169724629, ibs=0.23477465650788287, ibnll=0.6956299495647489\n",
      "Validation loss decreased (0.214652 --> 0.214185).  Saving model ...                                                   \n",
      "it: 178, train loss=0.22147676348686218, validation loss=0.2141854614019394, c=0.44327608005180547, ibs=0.23443195239294967, ibnll=0.6943406314646074\n",
      "Validation loss decreased (0.214185 --> 0.213724).  Saving model ...                                                   \n",
      "it: 179, train loss=0.2209804803133011, validation loss=0.21372440457344055, c=0.46920965802214065, ibs=0.23409142915153663, ibnll=0.6930634848511971\n",
      "Validation loss decreased (0.213724 --> 0.213271).  Saving model ...                                                   \n",
      "it: 180, train loss=0.22048957645893097, validation loss=0.2132706344127655, c=0.4397607080082642, ibs=0.23375311625138673, ibnll=0.6917985384170021\n",
      "Validation loss decreased (0.213271 --> 0.212820).  Saving model ...                                                   \n",
      "it: 181, train loss=0.22000622749328613, validation loss=0.21281972527503967, c=0.45508649665422923, ibs=0.23341703229256136, ibnll=0.690545745354658\n",
      "Validation loss decreased (0.212820 --> 0.212377).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 182, train loss=0.21952581405639648, validation loss=0.212376669049263, c=0.4436461192142835, ibs=0.23308311867793527, ibnll=0.6893048660740435\n",
      "Validation loss decreased (0.212377 --> 0.211937).  Saving model ...                                                   \n",
      "it: 183, train loss=0.21905352175235748, validation loss=0.21193687617778778, c=0.4604520645101607, ibs=0.23275139359663857, ibnll=0.6880758209437104\n",
      "Validation loss decreased (0.211937 --> 0.211503).  Saving model ...                                                   \n",
      "it: 184, train loss=0.21858470141887665, validation loss=0.21150335669517517, c=0.45298960806685373, ibs=0.2324218461191329, ibnll=0.6868585177232979\n",
      "Validation loss decreased (0.211503 --> 0.211074).  Saving model ...                                                   \n",
      "it: 185, train loss=0.21812234818935394, validation loss=0.2110743522644043, c=0.4564433069166487, ibs=0.23209449723797262, ibnll=0.6856529232437679\n",
      "Validation loss decreased (0.211074 --> 0.210651).  Saving model ...                                                   \n",
      "it: 186, train loss=0.2176647037267685, validation loss=0.2106507271528244, c=0.4425360017268494, ibs=0.2317693064516905, ibnll=0.6844588372804837\n",
      "Validation loss decreased (0.210651 --> 0.210232).  Saving model ...                                                   \n",
      "it: 187, train loss=0.21721254289150238, validation loss=0.21023201942443848, c=0.45998951555706313, ibs=0.2314462858672706, ibnll=0.6832761874340617\n",
      "Validation loss decreased (0.210232 --> 0.209818).  Saving model ...                                                   \n",
      "it: 188, train loss=0.21676553785800934, validation loss=0.20981843769550323, c=0.45049184372012707, ibs=0.23112541464597136, ibnll=0.6821048766146737\n",
      "Validation loss decreased (0.209818 --> 0.209410).  Saving model ...                                                   \n",
      "it: 189, train loss=0.2163238674402237, validation loss=0.20940950512886047, c=0.47028893891270157, ibs=0.23080671427155697, ibnll=0.6809448205569162\n",
      "Validation loss decreased (0.209410 --> 0.209006).  Saving model ...                                                   \n",
      "it: 190, train loss=0.21588706970214844, validation loss=0.20900633931159973, c=0.47013475592833576, ibs=0.23049013651919173, ibnll=0.6797958418989637\n",
      "Validation loss decreased (0.209006 --> 0.208607).  Saving model ...                                                   \n",
      "it: 191, train loss=0.21545618772506714, validation loss=0.20860682427883148, c=0.4380955317771131, ibs=0.23017569887966427, ibnll=0.6786578935717661\n",
      "Validation loss decreased (0.208607 --> 0.208213).  Saving model ...                                                   \n",
      "it: 192, train loss=0.21502923965454102, validation loss=0.208212748169899, c=0.4548398038792439, ibs=0.22986341585603226, ibnll=0.6775309498310903\n",
      "Validation loss decreased (0.208213 --> 0.207824).  Saving model ...                                                   \n",
      "it: 193, train loss=0.21460777521133423, validation loss=0.20782360434532166, c=0.45160196120756113, ibs=0.22955325217362393, ibnll=0.6764148131131696\n",
      "Validation loss decreased (0.207824 --> 0.207438).  Saving model ...                                                   \n",
      "it: 194, train loss=0.2141914814710617, validation loss=0.20743820071220398, c=0.4449412562829566, ibs=0.22924520858295444, ibnll=0.6753094179433633\n",
      "Validation loss decreased (0.207438 --> 0.207059).  Saving model ...                                                   \n",
      "it: 195, train loss=0.213779017329216, validation loss=0.2070590704679489, c=0.43821887816460575, ibs=0.22893927975518083, ibnll=0.6742146758474324\n",
      "Validation loss decreased (0.207059 --> 0.206683).  Saving model ...                                                   \n",
      "it: 196, train loss=0.21337316930294037, validation loss=0.20668263733386993, c=0.44111751827068363, ibs=0.22863544018319945, ibnll=0.6731304272196278\n",
      "Validation loss decreased (0.206683 --> 0.206312).  Saving model ...                                                   \n",
      "it: 197, train loss=0.21297016739845276, validation loss=0.206311896443367, c=0.4580159733571803, ibs=0.22833371720623324, ibnll=0.6720566705431326\n",
      "Validation loss decreased (0.206312 --> 0.205945).  Saving model ...                                                   \n",
      "it: 198, train loss=0.21257299184799194, validation loss=0.20594529807567596, c=0.4635973973912239, ibs=0.22803408792430818, ibnll=0.6709932838874448\n",
      "Validation loss decreased (0.205945 --> 0.205583).  Saving model ...                                                   \n",
      "it: 199, train loss=0.21218007802963257, validation loss=0.20558321475982666, c=0.46504671744426285, ibs=0.22773652523606, ibnll=0.6699401218994613\n",
      "                                                                                                                       \n",
      "Results: 0.4148475968343201, 0.2283435769179817, 0.6680456412712777\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 578, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 4, 'num_latent': 118, 'num_odefunc_layers1': 4, 'odefunc_neurons1': 623, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.0001}\n",
      "  5%|                                     | 5/100 [7:57:02<158:15:19, 5997.04s/trial, best loss: 0.16803939640522003]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca63f8dfd08d49a8a4e8077407359385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.377158).  Saving model ...                                                        \n",
      "it: 0, train loss=0.44525110721588135, validation loss=0.37715792655944824, c=0.5650535987748851, ibs=0.26822108439146763, ibnll=0.9397420776279374\n",
      "Validation loss decreased (0.377158 --> 0.372290).  Saving model ...                                                   \n",
      "it: 1, train loss=0.437107115983963, validation loss=0.37229031324386597, c=0.5400306278713629, ibs=0.2670418367939295, ibnll=0.9306955431601874\n",
      "Validation loss decreased (0.372290 --> 0.363857).  Saving model ...                                                   \n",
      "it: 2, train loss=0.43149322271347046, validation loss=0.36385729908943176, c=0.43972434915773356, ibs=0.2649645206085912, ibnll=0.9156029673323108\n",
      "Validation loss decreased (0.363857 --> 0.354520).  Saving model ...                                                   \n",
      "it: 3, train loss=0.421893835067749, validation loss=0.35452038049697876, c=0.5194180704441042, ibs=0.26256984596834854, ibnll=0.8988426273408637\n",
      "Validation loss decreased (0.354520 --> 0.344740).  Saving model ...                                                   \n",
      "it: 4, train loss=0.41116926074028015, validation loss=0.3447395861148834, c=0.5916385911179173, ibs=0.25998402181087465, ibnll=0.8810652428680444\n",
      "Validation loss decreased (0.344740 --> 0.334999).  Saving model ...                                                   \n",
      "it: 5, train loss=0.3999766409397125, validation loss=0.3349989354610443, c=0.4751607963246554, ibs=0.2572983083532367, ibnll=0.8634623609258012\n",
      "Validation loss decreased (0.334999 --> 0.325044).  Saving model ...                                                   \n",
      "it: 6, train loss=0.38888677954673767, validation loss=0.3250439167022705, c=0.5478713629402756, ibs=0.25446414466049877, ibnll=0.8453957105474597\n",
      "Validation loss decreased (0.325044 --> 0.316092).  Saving model ...                                                   \n",
      "it: 7, train loss=0.3775060176849365, validation loss=0.3160923719406128, c=0.5642266462480857, ibs=0.2518185328807559, ibnll=0.8290036264170031\n",
      "Validation loss decreased (0.316092 --> 0.307070).  Saving model ...                                                   \n",
      "it: 8, train loss=0.36730244755744934, validation loss=0.3070700764656067, c=0.5190505359877489, ibs=0.24902314270615547, ibnll=0.81229627867447\n",
      "Validation loss decreased (0.307070 --> 0.298498).  Saving model ...                                                   \n",
      "it: 9, train loss=0.3569037914276123, validation loss=0.2984979450702667, c=0.4613476263399694, ibs=0.24627445175295923, ibnll=0.7963337222755975\n",
      "Validation loss decreased (0.298498 --> 0.290258).  Saving model ...                                                   \n",
      "it: 10, train loss=0.3470393121242523, validation loss=0.29025840759277344, c=0.5111485451761103, ibs=0.2435224432833597, ibnll=0.7808818143613719\n",
      "Validation loss decreased (0.290258 --> 0.282380).  Saving model ...                                                   \n",
      "it: 11, train loss=0.3375402092933655, validation loss=0.282380074262619, c=0.5045022970903522, ibs=0.2407864337086746, ibnll=0.7659968606605442\n",
      "Validation loss decreased (0.282380 --> 0.274809).  Saving model ...                                                   \n",
      "it: 12, train loss=0.32840707898139954, validation loss=0.2748090326786041, c=0.4641653905053599, ibs=0.2380478685184777, ibnll=0.751546611926869\n",
      "Validation loss decreased (0.274809 --> 0.267509).  Saving model ...                                                   \n",
      "it: 13, train loss=0.31960591673851013, validation loss=0.2675092816352844, c=0.5437059724349158, ibs=0.2353099096126763, ibnll=0.737542668943359\n",
      "Validation loss decreased (0.267509 --> 0.260482).  Saving model ...                                                   \n",
      "it: 14, train loss=0.311100035905838, validation loss=0.2604823708534241, c=0.5220826952526799, ibs=0.23256632609741593, ibnll=0.7239467822330491\n",
      "Validation loss decreased (0.260482 --> 0.253742).  Saving model ...                                                   \n",
      "it: 15, train loss=0.3028891384601593, validation loss=0.2537415623664856, c=0.5509035222052068, ibs=0.22983275847008885, ibnll=0.7108125822693536\n",
      "Validation loss decreased (0.253742 --> 0.247297).  Saving model ...                                                   \n",
      "it: 16, train loss=0.2949883043766022, validation loss=0.24729669094085693, c=0.5135068912710566, ibs=0.22711762973191388, ibnll=0.6981551910411197\n",
      "Validation loss decreased (0.247297 --> 0.241156).  Saving model ...                                                   \n",
      "it: 17, train loss=0.2874082326889038, validation loss=0.24115583300590515, c=0.4811638591117917, ibs=0.22442572170390238, ibnll=0.685976698876971\n",
      "Validation loss decreased (0.241156 --> 0.235296).  Saving model ...                                                   \n",
      "it: 18, train loss=0.2801581919193268, validation loss=0.23529644310474396, c=0.40499234303215925, ibs=0.2217617981352121, ibnll=0.674277427354308\n",
      "Validation loss decreased (0.235296 --> 0.229735).  Saving model ...                                                   \n",
      "it: 19, train loss=0.2732161283493042, validation loss=0.22973543405532837, c=0.4281470137825421, ibs=0.2191326334539882, ibnll=0.6630641617477586\n",
      "Validation loss decreased (0.229735 --> 0.224461).  Saving model ...                                                   \n",
      "it: 20, train loss=0.2665993273258209, validation loss=0.22446075081825256, c=0.37543644716692187, ibs=0.21654352542081143, ibnll=0.6523356716771355\n",
      "Validation loss decreased (0.224461 --> 0.219468).  Saving model ...                                                   \n",
      "it: 21, train loss=0.260296106338501, validation loss=0.2194679081439972, c=0.293323124042879, ibs=0.2139998905453149, ibnll=0.6420907985667338\n",
      "Validation loss decreased (0.219468 --> 0.214758).  Saving model ...                                                   \n",
      "it: 22, train loss=0.2543025612831116, validation loss=0.21475788950920105, c=0.42643185298621744, ibs=0.21150699364204587, ibnll=0.6323268762408227\n",
      "Validation loss decreased (0.214758 --> 0.210320).  Saving model ...                                                   \n",
      "it: 23, train loss=0.24861972033977509, validation loss=0.2103201299905777, c=0.4352220520673813, ibs=0.20906962001573431, ibnll=0.6230389320300359\n",
      "Validation loss decreased (0.210320 --> 0.206144).  Saving model ...                                                   \n",
      "it: 24, train loss=0.2432372272014618, validation loss=0.20614396035671234, c=0.38664624808575804, ibs=0.20669191273154253, ibnll=0.6142194337845622\n",
      "Validation loss decreased (0.206144 --> 0.202236).  Saving model ...                                                   \n",
      "it: 25, train loss=0.238144189119339, validation loss=0.20223557949066162, c=0.2675957120980092, ibs=0.20437805226883213, ibnll=0.6058604934440737\n",
      "Validation loss decreased (0.202236 --> 0.198570).  Saving model ...                                                   \n",
      "it: 26, train loss=0.23334696888923645, validation loss=0.1985701471567154, c=0.22618683001531392, ibs=0.2021316355258789, ibnll=0.5979526535128775\n",
      "Validation loss decreased (0.198570 --> 0.195150).  Saving model ...                                                   \n",
      "it: 27, train loss=0.22882063686847687, validation loss=0.1951497495174408, c=0.3311179173047473, ibs=0.1999559916775169, ibnll=0.5904858483648137\n",
      "Validation loss decreased (0.195150 --> 0.191966).  Saving model ...                                                   \n",
      "it: 28, train loss=0.22456718981266022, validation loss=0.19196562469005585, c=0.3965084226646248, ibs=0.19785414139180713, ibnll=0.5834491412331854\n",
      "Validation loss decreased (0.191966 --> 0.189003).  Saving model ...                                                   \n",
      "it: 29, train loss=0.22057746350765228, validation loss=0.18900305032730103, c=0.4137519142419602, ibs=0.19582839287288578, ibnll=0.5768299857775743\n",
      "Validation loss decreased (0.189003 --> 0.186259).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.21683675050735474, validation loss=0.18625947833061218, c=0.4407350689127106, ibs=0.19388077837738368, ibnll=0.5706155661227817\n",
      "Validation loss decreased (0.186259 --> 0.183718).  Saving model ...                                                   \n",
      "it: 31, train loss=0.21334217488765717, validation loss=0.18371783196926117, c=0.45479326186830016, ibs=0.1920127609506281, ibnll=0.5647922597853136\n",
      "Validation loss decreased (0.183718 --> 0.181381).  Saving model ...                                                   \n",
      "it: 32, train loss=0.21007610857486725, validation loss=0.1813809871673584, c=0.4356508422664625, ibs=0.19022540601577595, ibnll=0.5593457618637397\n",
      "Validation loss decreased (0.181381 --> 0.179222).  Saving model ...                                                   \n",
      "it: 33, train loss=0.20704114437103271, validation loss=0.17922185361385345, c=0.46624808575803983, ibs=0.188519340596495, ibnll=0.5542613673757983\n",
      "Validation loss decreased (0.179222 --> 0.177248).  Saving model ...                                                   \n",
      "it: 34, train loss=0.2042098343372345, validation loss=0.17724844813346863, c=0.42238897396630937, ibs=0.18689479064717573, ibnll=0.5495241015471334\n",
      "Validation loss decreased (0.177248 --> 0.175435).  Saving model ...                                                   \n",
      "it: 35, train loss=0.20158959925174713, validation loss=0.17543530464172363, c=0.476906584992343, ibs=0.18535144339786355, ibnll=0.5451184570170932\n",
      "Validation loss decreased (0.175435 --> 0.173786).  Saving model ...                                                   \n",
      "it: 36, train loss=0.19915460050106049, validation loss=0.17378607392311096, c=0.5322511485451761, ibs=0.1838886816691157, ibnll=0.5410290747581997\n",
      "Validation loss decreased (0.173786 --> 0.172282).  Saving model ...                                                   \n",
      "it: 37, train loss=0.19690795242786407, validation loss=0.17228199541568756, c=0.5346094946401225, ibs=0.18250554476784053, ibnll=0.5372407241331806\n",
      "Validation loss decreased (0.172282 --> 0.170920).  Saving model ...                                                   \n",
      "it: 38, train loss=0.1948302835226059, validation loss=0.170920267701149, c=0.5306891271056662, ibs=0.18120064831136204, ibnll=0.5337378273100772\n",
      "Validation loss decreased (0.170920 --> 0.169686).  Saving model ...                                                   \n",
      "it: 39, train loss=0.19291828572750092, validation loss=0.16968600451946259, c=0.5183460949464013, ibs=0.17997220646661335, ibnll=0.5305046603387368\n",
      "Validation loss decreased (0.169686 --> 0.168578).  Saving model ...                                                   \n",
      "it: 40, train loss=0.1911565214395523, validation loss=0.16857826709747314, c=0.5362633996937213, ibs=0.17881824777511976, ibnll=0.5275258366917789\n",
      "Validation loss decreased (0.168578 --> 0.167577).  Saving model ...                                                   \n",
      "it: 41, train loss=0.18954378366470337, validation loss=0.1675768941640854, c=0.5605819295558959, ibs=0.1777369960864082, ibnll=0.524787544746163\n",
      "Validation loss decreased (0.167577 --> 0.166692).  Saving model ...                                                   \n",
      "it: 42, train loss=0.1880590170621872, validation loss=0.16669179499149323, c=0.53791730474732, ibs=0.17672569829740034, ibnll=0.5222749580289247\n",
      "Validation loss decreased (0.166692 --> 0.165892).  Saving model ...                                                   \n",
      "it: 43, train loss=0.18671098351478577, validation loss=0.16589245200157166, c=0.5396630934150076, ibs=0.17578230143456694, ibnll=0.5199740203903587\n",
      "Validation loss decreased (0.165892 --> 0.165194).  Saving model ...                                                   \n",
      "it: 44, train loss=0.18546941876411438, validation loss=0.1651940494775772, c=0.5477794793261869, ibs=0.17490404382137978, ibnll=0.517870897378853\n",
      "Validation loss decreased (0.165194 --> 0.164570).  Saving model ...                                                   \n",
      "it: 45, train loss=0.18434889614582062, validation loss=0.164570152759552, c=0.5444104134762634, ibs=0.17408762124393876, ibnll=0.515952766043258\n",
      "Validation loss decreased (0.164570 --> 0.164025).  Saving model ...                                                   \n",
      "it: 46, train loss=0.18332156538963318, validation loss=0.1640254557132721, c=0.544104134762634, ibs=0.1733303503333126, ibnll=0.5142058045827589\n",
      "Validation loss decreased (0.164025 --> 0.163551).  Saving model ...                                                   \n",
      "it: 47, train loss=0.1823921501636505, validation loss=0.16355116665363312, c=0.5376416539050536, ibs=0.17262978132450724, ibnll=0.512619352116113\n",
      "Validation loss decreased (0.163551 --> 0.163136).  Saving model ...                                                   \n",
      "it: 48, train loss=0.18155121803283691, validation loss=0.1631356179714203, c=0.5402756508422665, ibs=0.1719820511152095, ibnll=0.5111801357188497\n",
      "Validation loss decreased (0.163136 --> 0.162785).  Saving model ...                                                   \n",
      "it: 49, train loss=0.18078695237636566, validation loss=0.16278532147407532, c=0.5502297090352221, ibs=0.17138532880326715, ibnll=0.5098782627608198\n",
      "Validation loss decreased (0.162785 --> 0.162480).  Saving model ...                                                   \n",
      "it: 50, train loss=0.18010476231575012, validation loss=0.16247950494289398, c=0.5409800918836141, ibs=0.17083653380985583, ibnll=0.5087036134895807\n",
      "Validation loss decreased (0.162480 --> 0.162228).  Saving model ...                                                   \n",
      "it: 51, train loss=0.1794823557138443, validation loss=0.16222833096981049, c=0.547166921898928, ibs=0.17033430998147314, ibnll=0.5076450023956421\n",
      "Validation loss decreased (0.162228 --> 0.162014).  Saving model ...                                                   \n",
      "it: 52, train loss=0.17893381416797638, validation loss=0.16201357543468475, c=0.5483307810107197, ibs=0.1698682660205134, ibnll=0.506690188850314\n",
      "Validation loss decreased (0.162014 --> 0.161837).  Saving model ...                                                   \n",
      "it: 53, train loss=0.17843195796012878, validation loss=0.16183727979660034, c=0.5415926493108729, ibs=0.16944535935476496, ibnll=0.5058359398988398\n",
      "Validation loss decreased (0.161837 --> 0.161699).  Saving model ...                                                   \n",
      "it: 54, train loss=0.17798542976379395, validation loss=0.16169866919517517, c=0.5412557427258805, ibs=0.16906520467937688, ibnll=0.5050751922636144\n",
      "Validation loss decreased (0.161699 --> 0.161586).  Saving model ...                                                   \n",
      "it: 55, train loss=0.1775943636894226, validation loss=0.1615864783525467, c=0.5502909647779479, ibs=0.16870743343741046, ibnll=0.5043893214559596\n",
      "Validation loss decreased (0.161586 --> 0.161509).  Saving model ...                                                   \n",
      "it: 56, train loss=0.17723752558231354, validation loss=0.16150878369808197, c=0.5400612557427259, ibs=0.16838678138696833, ibnll=0.5037898839426925\n",
      "Validation loss decreased (0.161509 --> 0.161439).  Saving model ...                                                   \n",
      "it: 57, train loss=0.17692886292934418, validation loss=0.16143858432769775, c=0.5528024502297091, ibs=0.1680927872547778, ibnll=0.5032355506631924\n",
      "Validation loss decreased (0.161439 --> 0.161393).  Saving model ...                                                   \n",
      "it: 58, train loss=0.17664214968681335, validation loss=0.16139288246631622, c=0.5341500765696784, ibs=0.16783718059341143, ibnll=0.5027599181779919\n",
      "Validation loss decreased (0.161393 --> 0.161367).  Saving model ...                                                   \n",
      "it: 59, train loss=0.17639879882335663, validation loss=0.16136710345745087, c=0.5501378254211332, ibs=0.16758687844096765, ibnll=0.5023189939709098\n",
      "Validation loss decreased (0.161367 --> 0.161363).  Saving model ...                                                   \n",
      "it: 60, train loss=0.17617732286453247, validation loss=0.16136294603347778, c=0.567687595712098, ibs=0.1673716570514443, ibnll=0.5019608953800208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 61, train loss=0.17598943412303925, validation loss=0.16137553751468658, c=0.5449923430321593, ibs=0.16718186044401182, ibnll=0.501637752649523\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 62, train loss=0.17583151161670685, validation loss=0.16138124465942383, c=0.554854517611026, ibs=0.16700342536489124, ibnll=0.5013401120524973\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 63, train loss=0.17567786574363708, validation loss=0.16139459609985352, c=0.5624196018376723, ibs=0.16683668857708103, ibnll=0.5010707018715873\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 64, train loss=0.1755441427230835, validation loss=0.16142193973064423, c=0.5507810107197549, ibs=0.1666991220182837, ibnll=0.5008558608760241\n",
      "EarlyStopping counter: 5 out of 20                                                                                     \n",
      "it: 65, train loss=0.17543740570545197, validation loss=0.1614418625831604, c=0.5460336906584993, ibs=0.16657021447285236, ibnll=0.5006382637599126\n",
      "EarlyStopping counter: 6 out of 20                                                                                     \n",
      "it: 66, train loss=0.1753213107585907, validation loss=0.1614401787519455, c=0.5494946401225115, ibs=0.16655602864785835, ibnll=0.500615820551815\n",
      "EarlyStopping counter: 7 out of 20                                                                                     \n",
      "it: 67, train loss=0.17530900239944458, validation loss=0.1614341288805008, c=0.5560796324655436, ibs=0.166541613969901, ibnll=0.5005922783465654\n",
      "EarlyStopping counter: 8 out of 20                                                                                     \n",
      "it: 68, train loss=0.1752920150756836, validation loss=0.16142866015434265, c=0.5612863705972435, ibs=0.16652826946205554, ibnll=0.5005698498781397\n",
      "EarlyStopping counter: 9 out of 20                                                                                     \n",
      "it: 69, train loss=0.1752769500017166, validation loss=0.1614277958869934, c=0.5700153139356815, ibs=0.16651690771053398, ibnll=0.5005504196584283\n",
      "EarlyStopping counter: 10 out of 20                                                                                    \n",
      "it: 70, train loss=0.17526817321777344, validation loss=0.16142867505550385, c=0.5726186830015314, ibs=0.1665069976930635, ibnll=0.5005343768236568\n",
      "EarlyStopping counter: 11 out of 20                                                                                    \n",
      "it: 71, train loss=0.1752602905035019, validation loss=0.1614307016134262, c=0.5741500765696784, ibs=0.16649746272808935, ibnll=0.5005193471745119\n",
      "EarlyStopping counter: 12 out of 20                                                                                    \n",
      "it: 72, train loss=0.17525261640548706, validation loss=0.161430224776268, c=0.5734456355283308, ibs=0.16649644239366773, ibnll=0.5005173685494565\n",
      "EarlyStopping counter: 13 out of 20                                                                                    \n",
      "it: 73, train loss=0.17525143921375275, validation loss=0.16142906248569489, c=0.5735681470137826, ibs=0.16649539735649327, ibnll=0.5005150076754861\n",
      "EarlyStopping counter: 14 out of 20                                                                                    \n",
      "it: 74, train loss=0.17524966597557068, validation loss=0.16142727434635162, c=0.5739969372128637, ibs=0.1664943538387093, ibnll=0.5005123609606487\n",
      "EarlyStopping counter: 15 out of 20                                                                                    \n",
      "it: 75, train loss=0.17524750530719757, validation loss=0.16142494976520538, c=0.5743950995405819, ibs=0.16649328039379335, ibnll=0.5005093980131026\n",
      "EarlyStopping counter: 16 out of 20                                                                                    \n",
      "it: 76, train loss=0.17524497210979462, validation loss=0.1614222526550293, c=0.5737519142419601, ibs=0.1664921931193316, ibnll=0.5005061915907291\n",
      "EarlyStopping counter: 17 out of 20                                                                                    \n",
      "it: 77, train loss=0.17524223029613495, validation loss=0.16141928732395172, c=0.573813169984686, ibs=0.16649110318415025, ibnll=0.5005028195443133\n",
      "EarlyStopping counter: 18 out of 20                                                                                    \n",
      "it: 78, train loss=0.1752394288778305, validation loss=0.16141903400421143, c=0.5742725880551302, ibs=0.16649100810333836, ibnll=0.5005024883065917\n",
      "EarlyStopping counter: 19 out of 20                                                                                    \n",
      "it: 79, train loss=0.17523913085460663, validation loss=0.16141867637634277, c=0.5737212863705973, ibs=0.1664909021748737, ibnll=0.5005021385231124\n",
      "EarlyStopping counter: 20 out of 20                                                                                    \n",
      "it: 80, train loss=0.17523881793022156, validation loss=0.1614183634519577, c=0.574364471669219, ibs=0.16649081339245508, ibnll=0.5005017956624707\n",
      "Early stopping                                                                                                         \n",
      "                                                                                                                       \n",
      "Results: 0.5907740765105756, 0.18551668258204837, 0.5467454852111856\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 327, 'lr': 0.0001, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 3, 'num_latent': 103, 'num_odefunc_layers1': 2, 'odefunc_neurons1': 1474, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 1.0, 'weight_decay': 0.001}\n",
      "  6%|                                     | 6/100 [8:41:34<127:04:24, 4866.64s/trial, best loss: 0.1614183634519577]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c46b584cf64063a77b8de45a881c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.269541).  Saving model ...                                                        \n",
      "it: 0, train loss=0.2823184132575989, validation loss=0.2695411145687103, c=0.5288565681572025, ibs=0.2442519818426777, ibnll=0.6764783039726875\n",
      "Validation loss decreased (0.269541 --> 0.256272).  Saving model ...                                                   \n",
      "it: 1, train loss=0.26869118213653564, validation loss=0.2562720477581024, c=0.5233792356490794, ibs=0.2326021301166712, ibnll=0.6508122522173532\n",
      "Validation loss decreased (0.256272 --> 0.243734).  Saving model ...                                                   \n",
      "it: 2, train loss=0.2557070553302765, validation loss=0.24373376369476318, c=0.5178709577595544, ibs=0.22185780287409357, ibnll=0.6275154769918667\n",
      "Validation loss decreased (0.243734 --> 0.232167).  Saving model ...                                                   \n",
      "it: 3, train loss=0.24344375729560852, validation loss=0.23216743767261505, c=0.5215534581463717, ibs=0.2120970006253014, ibnll=0.6065612852095482\n",
      "Validation loss decreased (0.232167 --> 0.221608).  Saving model ...                                                   \n",
      "it: 4, train loss=0.23213373124599457, validation loss=0.22160765528678894, c=0.5260714838310382, ibs=0.20347369733045123, ibnll=0.5881185739027476\n",
      "Validation loss decreased (0.221608 --> 0.212062).  Saving model ...                                                   \n",
      "it: 5, train loss=0.22180721163749695, validation loss=0.21206241846084595, c=0.5236267987002939, ibs=0.19602725735131937, ibnll=0.5721483020058242\n",
      "Validation loss decreased (0.212062 --> 0.203584).  Saving model ...                                                   \n",
      "it: 6, train loss=0.21249787509441376, validation loss=0.2035841941833496, c=0.5217700758161844, ibs=0.1897668560611636, ibnll=0.5585933489349089\n",
      "Validation loss decreased (0.203584 --> 0.196206).  Saving model ...                                                   \n",
      "it: 7, train loss=0.20425881445407867, validation loss=0.196205735206604, c=0.5280829336221569, ibs=0.18467636014095173, ibnll=0.5473880426399806\n",
      "Validation loss decreased (0.196206 --> 0.189882).  Saving model ...                                                   \n",
      "it: 8, train loss=0.19711029529571533, validation loss=0.18988218903541565, c=0.5324462323998144, ibs=0.18069991620438916, ibnll=0.5384150666742024\n",
      "Validation loss decreased (0.189882 --> 0.184547).  Saving model ...                                                   \n",
      "it: 9, train loss=0.1910138577222824, validation loss=0.18454694747924805, c=0.5448553303419464, ibs=0.17772802751376587, ibnll=0.5314643687728535\n",
      "Validation loss decreased (0.184547 --> 0.180149).  Saving model ...                                                   \n",
      "it: 10, train loss=0.1859123557806015, validation loss=0.18014946579933167, c=0.5599876218474392, ibs=0.17564418491822734, ibnll=0.5263317785964337\n",
      "Validation loss decreased (0.180149 --> 0.176644).  Saving model ...                                                   \n",
      "it: 11, train loss=0.18175247311592102, validation loss=0.17664407193660736, c=0.5740058796224663, ibs=0.1743388608486806, ibnll=0.5228388146250764\n",
      "Validation loss decreased (0.176644 --> 0.173926).  Saving model ...                                                   \n",
      "it: 12, train loss=0.17847731709480286, validation loss=0.1739264279603958, c=0.5820826241683429, ibs=0.17367472396557251, ibnll=0.5207353052547719\n",
      "Validation loss decreased (0.173926 --> 0.171857).  Saving model ...                                                   \n",
      "it: 13, train loss=0.17598018050193787, validation loss=0.17185744643211365, c=0.5874361751508588, ibs=0.17349216796873773, ibnll=0.5197087556966797\n",
      "Validation loss decreased (0.171857 --> 0.170307).  Saving model ...                                                   \n",
      "it: 14, train loss=0.17414355278015137, validation loss=0.17030666768550873, c=0.5969673526226211, ibs=0.17359417808902058, ibnll=0.5193618364090419\n",
      "Validation loss decreased (0.170307 --> 0.169194).  Saving model ...                                                   \n",
      "it: 15, train loss=0.17282918095588684, validation loss=0.1691942811012268, c=0.6037753365310228, ibs=0.17381926241381843, ibnll=0.5193852258879722\n",
      "Validation loss decreased (0.169194 --> 0.168412).  Saving model ...                                                   \n",
      "it: 16, train loss=0.17192219197750092, validation loss=0.16841156780719757, c=0.6135231316725979, ibs=0.17410123872285663, ibnll=0.5196441584604655\n",
      "Validation loss decreased (0.168412 --> 0.167821).  Saving model ...                                                   \n",
      "it: 17, train loss=0.17130599915981293, validation loss=0.16782091557979584, c=0.6171437412966114, ibs=0.17444395349042013, ibnll=0.5201035489975971\n",
      "Validation loss decreased (0.167821 --> 0.167338).  Saving model ...                                                   \n",
      "it: 18, train loss=0.1708822250366211, validation loss=0.16733762621879578, c=0.6187838465109082, ibs=0.17482757698407125, ibnll=0.5206561876176345\n",
      "Validation loss decreased (0.167338 --> 0.167042).  Saving model ...                                                   \n",
      "it: 19, train loss=0.1705501824617386, validation loss=0.16704198718070984, c=0.6186910103667028, ibs=0.17514956211172072, ibnll=0.5211326551162855\n",
      "Validation loss decreased (0.167042 --> 0.166749).  Saving model ...                                                   \n",
      "it: 20, train loss=0.1703432947397232, validation loss=0.16674861311912537, c=0.6174841404920316, ibs=0.17533026182317346, ibnll=0.521352879102765\n",
      "Validation loss decreased (0.166749 --> 0.166572).  Saving model ...                                                   \n",
      "it: 21, train loss=0.17007873952388763, validation loss=0.1665719896554947, c=0.6190314095621229, ibs=0.1753790707866408, ibnll=0.5214163287361134\n",
      "Validation loss decreased (0.166572 --> 0.166285).  Saving model ...                                                   \n",
      "it: 22, train loss=0.16987179219722748, validation loss=0.16628523170948029, c=0.6205477332508124, ibs=0.17541309758135185, ibnll=0.5213307462957417\n",
      "Validation loss decreased (0.166285 --> 0.165957).  Saving model ...                                                   \n",
      "it: 23, train loss=0.16963911056518555, validation loss=0.16595710813999176, c=0.6215070400742689, ibs=0.17537287354009098, ibnll=0.5210590085536694\n",
      "Validation loss decreased (0.165957 --> 0.165764).  Saving model ...                                                   \n",
      "it: 24, train loss=0.16944082081317902, validation loss=0.16576437652111053, c=0.6219093300324926, ibs=0.1752149499344134, ibnll=0.5206388001586431\n",
      "Validation loss decreased (0.165764 --> 0.165615).  Saving model ...                                                   \n",
      "it: 25, train loss=0.16927823424339294, validation loss=0.16561457514762878, c=0.6220640569395017, ibs=0.1749789490270205, ibnll=0.5201024841540732\n",
      "Validation loss decreased (0.165615 --> 0.165286).  Saving model ...                                                   \n",
      "it: 26, train loss=0.16908085346221924, validation loss=0.1652863770723343, c=0.6215689308370725, ibs=0.17471163860386418, ibnll=0.5194069635118004\n",
      "Validation loss decreased (0.165286 --> 0.164873).  Saving model ...                                                   \n",
      "it: 27, train loss=0.16872484982013702, validation loss=0.16487348079681396, c=0.6211047501160452, ibs=0.1744857777022588, ibnll=0.518715515875313\n",
      "Validation loss decreased (0.164873 --> 0.164521).  Saving model ...                                                   \n",
      "it: 28, train loss=0.16832409799098969, validation loss=0.16452103853225708, c=0.6189076280365156, ibs=0.17418850653171716, ibnll=0.5179084964763832\n",
      "Validation loss decreased (0.164521 --> 0.164188).  Saving model ...                                                   \n",
      "it: 29, train loss=0.16797375679016113, validation loss=0.16418807208538055, c=0.6194336995203465, ibs=0.17371771666884253, ibnll=0.5167986007005629\n",
      "Validation loss decreased (0.164188 --> 0.163791).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.16762438416481018, validation loss=0.16379088163375854, c=0.6194336995203465, ibs=0.17325243649160807, ibnll=0.515641133797427\n",
      "Validation loss decreased (0.163791 --> 0.163286).  Saving model ...                                                   \n",
      "it: 31, train loss=0.16725394129753113, validation loss=0.16328638792037964, c=0.6192789726133374, ibs=0.17274807123872357, ibnll=0.514349290936896\n",
      "Validation loss decreased (0.163286 --> 0.162899).  Saving model ...                                                   \n",
      "it: 32, train loss=0.1667647659778595, validation loss=0.16289937496185303, c=0.6190004641807211, ibs=0.17216210878349278, ibnll=0.5129999417394722\n",
      "Validation loss decreased (0.162899 --> 0.162527).  Saving model ...                                                   \n",
      "it: 33, train loss=0.1662665456533432, validation loss=0.16252654790878296, c=0.6185053380782918, ibs=0.1714846491442083, ibnll=0.5114779602392434\n",
      "Validation loss decreased (0.162527 --> 0.161936).  Saving model ...                                                   \n",
      "it: 34, train loss=0.16580140590667725, validation loss=0.16193576157093048, c=0.6185053380782918, ibs=0.17089222803918064, ibnll=0.5099546366231051\n",
      "Validation loss decreased (0.161936 --> 0.161532).  Saving model ...                                                   \n",
      "it: 35, train loss=0.16533347964286804, validation loss=0.16153192520141602, c=0.6188766826551138, ibs=0.17027057194592257, ibnll=0.5085371159370045\n",
      "Validation loss decreased (0.161532 --> 0.161290).  Saving model ...                                                   \n",
      "it: 36, train loss=0.16498462855815887, validation loss=0.16129036247730255, c=0.61847439269689, ibs=0.16972207948004836, ibnll=0.5073643756788776\n",
      "Validation loss decreased (0.161290 --> 0.160907).  Saving model ...                                                   \n",
      "it: 37, train loss=0.16465897858142853, validation loss=0.16090746223926544, c=0.6181030481200681, ibs=0.169276099580351, ibnll=0.5062764647109503\n",
      "Validation loss decreased (0.160907 --> 0.160565).  Saving model ...                                                   \n",
      "it: 38, train loss=0.16425037384033203, validation loss=0.16056500375270844, c=0.6170199597710042, ibs=0.16884852955712076, ibnll=0.5052343514864043\n",
      "Validation loss decreased (0.160565 --> 0.160306).  Saving model ...                                                   \n",
      "it: 39, train loss=0.16389481723308563, validation loss=0.16030599176883698, c=0.6171746866780133, ibs=0.16827546704684224, ibnll=0.5039964667085753\n",
      "Validation loss decreased (0.160306 --> 0.159883).  Saving model ...                                                   \n",
      "it: 40, train loss=0.1635642945766449, validation loss=0.15988264977931976, c=0.617576976636237, ibs=0.16777808312803813, ibnll=0.5027926153346355\n",
      "Validation loss decreased (0.159883 --> 0.159535).  Saving model ...                                                   \n",
      "it: 41, train loss=0.16320624947547913, validation loss=0.15953542292118073, c=0.6188766826551138, ibs=0.16727640702350327, ibnll=0.5016484095382675\n",
      "Validation loss decreased (0.159535 --> 0.159324).  Saving model ...                                                   \n",
      "it: 42, train loss=0.1628722995519638, validation loss=0.15932364761829376, c=0.6196193718087576, ibs=0.16679136587948476, ibnll=0.5006247579734625\n",
      "Validation loss decreased (0.159324 --> 0.159014).  Saving model ...                                                   \n",
      "it: 43, train loss=0.16257044672966003, validation loss=0.15901407599449158, c=0.6191551910877302, ibs=0.16646791653108536, ibnll=0.49980173238454517\n",
      "Validation loss decreased (0.159014 --> 0.158855).  Saving model ...                                                   \n",
      "it: 44, train loss=0.16225571930408478, validation loss=0.15885524451732635, c=0.6186600649853009, ibs=0.16621274256519644, ibnll=0.4991984998822826\n",
      "Validation loss decreased (0.158855 --> 0.158700).  Saving model ...                                                   \n",
      "it: 45, train loss=0.16200973093509674, validation loss=0.1587003916501999, c=0.6189076280365156, ibs=0.16597277835976892, ibnll=0.49863548240686606\n",
      "Validation loss decreased (0.158700 --> 0.158298).  Saving model ...                                                   \n",
      "it: 46, train loss=0.161773219704628, validation loss=0.1582982987165451, c=0.6194955902831503, ibs=0.16577578409954907, ibnll=0.4979873075950174\n",
      "Validation loss decreased (0.158298 --> 0.158194).  Saving model ...                                                   \n",
      "it: 47, train loss=0.16147644817829132, validation loss=0.1581941694021225, c=0.6208571870648306, ibs=0.16538044921309916, ibnll=0.4971994755915894\n",
      "Validation loss decreased (0.158194 --> 0.157849).  Saving model ...                                                   \n",
      "it: 48, train loss=0.16118165850639343, validation loss=0.1578485369682312, c=0.6211047501160452, ibs=0.16514660988787683, ibnll=0.4965187705893095\n",
      "Validation loss decreased (0.157849 --> 0.157600).  Saving model ...                                                   \n",
      "it: 49, train loss=0.160908043384552, validation loss=0.15760023891925812, c=0.6217546031254835, ibs=0.16488396210573059, ibnll=0.49584910613034644\n",
      "Validation loss decreased (0.157600 --> 0.157384).  Saving model ...                                                   \n",
      "it: 50, train loss=0.16062861680984497, validation loss=0.1573839783668518, c=0.6220950023209036, ibs=0.1645966858817602, ibnll=0.4951750771986156\n",
      "Validation loss decreased (0.157384 --> 0.156973).  Saving model ...                                                   \n",
      "it: 51, train loss=0.16032667458057404, validation loss=0.15697337687015533, c=0.62237351075352, ibs=0.16443372077807955, ibnll=0.4946132986532872\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 52, train loss=0.1600331813097, validation loss=0.1569739729166031, c=0.6231780906699675, ibs=0.16407379635750485, ibnll=0.4939865749495708\n",
      "Validation loss decreased (0.156973 --> 0.156280).  Saving model ...                                                   \n",
      "it: 53, train loss=0.15975888073444366, validation loss=0.1562802642583847, c=0.6226520191861364, ibs=0.16408925430061963, ibnll=0.4935545343831672\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 54, train loss=0.15948796272277832, validation loss=0.15682125091552734, c=0.625406158130899, ibs=0.16357559274256275, ibnll=0.4929705833305499\n",
      "Validation loss decreased (0.156280 --> 0.155633).  Saving model ...                                                   \n",
      "it: 55, train loss=0.1592717468738556, validation loss=0.15563277900218964, c=0.6223425653721182, ibs=0.16409082921296983, ibnll=0.49308618223682665\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 56, train loss=0.15912754833698273, validation loss=0.15650266408920288, c=0.6257156119449172, ibs=0.16324274994302032, ibnll=0.492092433268282\n",
      "Validation loss decreased (0.155633 --> 0.155353).  Saving model ...                                                   \n",
      "it: 57, train loss=0.15879696607589722, validation loss=0.15535332262516022, c=0.6234256537211821, ibs=0.16344490585969784, ibnll=0.4916892745825947\n",
      "Validation loss decreased (0.155353 --> 0.155259).  Saving model ...                                                   \n",
      "it: 58, train loss=0.15838924050331116, validation loss=0.15525920689105988, c=0.6238588890608077, ibs=0.16311893037460481, ibnll=0.49100554318130013\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 59, train loss=0.15808233618736267, validation loss=0.1556960791349411, c=0.6265201918613647, ibs=0.16262481349372976, ibnll=0.49034744312533174\n",
      "Validation loss decreased (0.155259 --> 0.154488).  Saving model ...                                                   \n",
      "it: 60, train loss=0.15790331363677979, validation loss=0.15448813140392303, c=0.6239207798236113, ibs=0.16307686397408877, ibnll=0.4903934535605853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 61, train loss=0.157700315117836, validation loss=0.15509314835071564, c=0.6276342255918305, ibs=0.1623064416310488, ibnll=0.48934941498639256\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 62, train loss=0.15731199085712433, validation loss=0.15461310744285583, c=0.6276032802104285, ibs=0.16225899648688658, ibnll=0.4889419849984559\n",
      "Validation loss decreased (0.154488 --> 0.153938).  Saving model ...                                                   \n",
      "it: 63, train loss=0.1569836586713791, validation loss=0.15393781661987305, c=0.6261797926659446, ibs=0.16251857027713618, ibnll=0.48896398554458625\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 64, train loss=0.15678803622722626, validation loss=0.15475299954414368, c=0.6301098561039765, ibs=0.16182995651436496, ibnll=0.4881432401896437\n",
      "Validation loss decreased (0.153938 --> 0.153501).  Saving model ...                                                   \n",
      "it: 65, train loss=0.15656305849552155, validation loss=0.15350118279457092, c=0.627015317963794, ibs=0.16227698998644996, ibnll=0.48817296308677816\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 66, train loss=0.1562635749578476, validation loss=0.15390954911708832, c=0.6301098561039765, ibs=0.16159282430203492, ibnll=0.4871229768880887\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 67, train loss=0.15590135753154755, validation loss=0.15357203781604767, c=0.6306978183506111, ibs=0.1614701687845066, ibnll=0.48667172360453825\n",
      "Validation loss decreased (0.153501 --> 0.152932).  Saving model ...                                                   \n",
      "it: 68, train loss=0.15560920536518097, validation loss=0.15293154120445251, c=0.6299860745783692, ibs=0.16166374555870056, ibnll=0.4866323495980314\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 69, train loss=0.1553952991962433, validation loss=0.15376292169094086, c=0.6347826086956522, ibs=0.1609547405377488, ibnll=0.4857576307185523\n",
      "Validation loss decreased (0.152932 --> 0.152333).  Saving model ...                                                   \n",
      "it: 70, train loss=0.15520724654197693, validation loss=0.15233342349529266, c=0.6299551291969674, ibs=0.16167562058814566, ibnll=0.4862506089317156\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 71, train loss=0.15503722429275513, validation loss=0.15346042811870575, c=0.635525297849296, ibs=0.16056573319490647, ibnll=0.48476250302306456\n",
      "Validation loss decreased (0.152333 --> 0.152069).  Saving model ...                                                   \n",
      "it: 72, train loss=0.1546931266784668, validation loss=0.15206941962242126, c=0.6325545412347208, ibs=0.16101210507780386, ibnll=0.48478403773435547\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 73, train loss=0.15431813895702362, validation loss=0.1523776650428772, c=0.6343493733560266, ibs=0.1603976616799014, ibnll=0.48375689121678955\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 74, train loss=0.1539464145898819, validation loss=0.15215888619422913, c=0.6348754448398577, ibs=0.1602158363159897, ibnll=0.48326152980791587\n",
      "Validation loss decreased (0.152069 --> 0.151402).  Saving model ...                                                   \n",
      "it: 75, train loss=0.15366947650909424, validation loss=0.1514015793800354, c=0.6332662850069627, ibs=0.16051495271371316, ibnll=0.48338858192051437\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 76, train loss=0.15348602831363678, validation loss=0.15256105363368988, c=0.6376914745474238, ibs=0.1596069278981243, ibnll=0.48226602839798716\n",
      "Validation loss decreased (0.151402 --> 0.150753).  Saving model ...                                                   \n",
      "it: 77, train loss=0.15339809656143188, validation loss=0.15075275301933289, c=0.6319975243694879, ibs=0.1610500307191515, ibnll=0.4840328693792872\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 78, train loss=0.1535377949476242, validation loss=0.15275870263576508, c=0.6389602351848986, ibs=0.15922538591633456, ibnll=0.4815739773461773\n",
      "Validation loss decreased (0.150753 --> 0.150479).  Saving model ...                                                   \n",
      "it: 79, train loss=0.15312916040420532, validation loss=0.15047858655452728, c=0.6340089741606065, ibs=0.1600941021057192, ibnll=0.48202895371749976\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 80, train loss=0.15255801379680634, validation loss=0.1507740318775177, c=0.6362679870029398, ibs=0.1592891380265663, ibnll=0.48060234793072326\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 81, train loss=0.15204061567783356, validation loss=0.1516226828098297, c=0.6393625251431224, ibs=0.15870659739323645, ibnll=0.47987803965891407\n",
      "Validation loss decreased (0.150479 --> 0.149713).  Saving model ...                                                   \n",
      "it: 82, train loss=0.15205785632133484, validation loss=0.14971284568309784, c=0.6336376295837847, ibs=0.16015417116835998, ibnll=0.48167764833180793\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 83, train loss=0.15219493210315704, validation loss=0.15112705528736115, c=0.6394863066687297, ibs=0.15839710408892005, ibnll=0.4789511897559385\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 84, train loss=0.15147283673286438, validation loss=0.1502247005701065, c=0.6380318737428439, ibs=0.15854313963693123, ibnll=0.4787506681274315\n",
      "Validation loss decreased (0.149713 --> 0.149353).  Saving model ...                                                   \n",
      "it: 85, train loss=0.1510002762079239, validation loss=0.14935314655303955, c=0.6356800247563051, ibs=0.15921168679075656, ibnll=0.47960428301421987\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 86, train loss=0.15100741386413574, validation loss=0.15099523961544037, c=0.6423642271390995, ibs=0.15794786123596358, ibnll=0.47792556574556977\n",
      "Validation loss decreased (0.149353 --> 0.148955).  Saving model ...                                                   \n",
      "it: 87, train loss=0.15087158977985382, validation loss=0.14895541965961456, c=0.6369178400123782, ibs=0.1589071592206626, ibnll=0.4787617103041576\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 88, train loss=0.15049351751804352, validation loss=0.14951612055301666, c=0.6402908865851772, ibs=0.15789301509288595, ibnll=0.4770043460957375\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 89, train loss=0.1499783843755722, validation loss=0.14972007274627686, c=0.642085718706483, ibs=0.15757893690668945, ibnll=0.4764500025038401\n",
      "Validation loss decreased (0.148955 --> 0.148322).  Saving model ...                                                   \n",
      "it: 90, train loss=0.14981277287006378, validation loss=0.148321732878685, c=0.6377843106916292, ibs=0.15865227583010655, ibnll=0.4778810310801741\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.14985036849975586, validation loss=0.1499488651752472, c=0.6439733869719945, ibs=0.15714029989195888, ibnll=0.4756476491845156\n",
      "Validation loss decreased (0.148322 --> 0.148081).  Saving model ...                                                   \n",
      "it: 92, train loss=0.14955003559589386, validation loss=0.14808060228824615, c=0.6399814327711589, ibs=0.1580054395452832, ibnll=0.47644220710013513\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 93, train loss=0.14913277328014374, validation loss=0.14855699241161346, c=0.6421785548506885, ibs=0.1572116713724054, ibnll=0.4750642920431713\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 94, train loss=0.1487048864364624, validation loss=0.14874133467674255, c=0.6430759709113415, ibs=0.1569312479812738, ibnll=0.4745641540816151\n",
      "Validation loss decreased (0.148081 --> 0.147495).  Saving model ...                                                   \n",
      "it: 95, train loss=0.14852206408977509, validation loss=0.14749494194984436, c=0.6401361596781681, ibs=0.15787233887676877, ibnll=0.47584262426382096\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 96, train loss=0.14850398898124695, validation loss=0.14918115735054016, c=0.6451493114652638, ibs=0.15646140546303045, ibnll=0.47379202641102625\n",
      "Validation loss decreased (0.147495 --> 0.147073).  Saving model ...                                                   \n",
      "it: 97, train loss=0.14835159480571747, validation loss=0.14707329869270325, c=0.640662231161999, ibs=0.15780882085271175, ibnll=0.4754932073845179\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 98, train loss=0.14814262092113495, validation loss=0.14833173155784607, c=0.6444685130744237, ibs=0.15635367780746007, ibnll=0.473122761770514\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 99, train loss=0.14762213826179504, validation loss=0.1473093181848526, c=0.6436639331579762, ibs=0.15671420963235197, ibnll=0.473348902631341\n",
      "Validation loss decreased (0.147073 --> 0.146970).  Saving model ...                                                   \n",
      "it: 100, train loss=0.14722603559494019, validation loss=0.1469704955816269, c=0.643354479343958, ibs=0.15673165618708831, ibnll=0.4732090895325924\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 101, train loss=0.14701524376869202, validation loss=0.14788727462291718, c=0.6457991644747022, ibs=0.1559136447732268, ibnll=0.4719674519598472\n",
      "Validation loss decreased (0.146970 --> 0.146168).  Saving model ...                                                   \n",
      "it: 102, train loss=0.14693918824195862, validation loss=0.14616797864437103, c=0.6422404456134921, ibs=0.15731653478497215, ibnll=0.47400845790791163\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 103, train loss=0.14699751138687134, validation loss=0.14828196167945862, c=0.6474392696889989, ibs=0.1555327928834214, ibnll=0.4713401811587728\n",
      "Validation loss decreased (0.146168 --> 0.145821).  Saving model ...                                                   \n",
      "it: 104, train loss=0.14684128761291504, validation loss=0.14582127332687378, c=0.642797462478725, ibs=0.15732312091316056, ibnll=0.47385061961282404\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 105, train loss=0.1466779112815857, validation loss=0.14735954999923706, c=0.6468203620609624, ibs=0.15539885309597126, ibnll=0.4705878645695748\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 106, train loss=0.14605070650577545, validation loss=0.1459912210702896, c=0.6452421476094693, ibs=0.15590342039378607, ibnll=0.470982758659522\n",
      "Validation loss decreased (0.145821 --> 0.145795).  Saving model ...                                                   \n",
      "it: 107, train loss=0.14558537304401398, validation loss=0.14579471945762634, c=0.6453968745164784, ibs=0.15584015376455715, ibnll=0.4707570403269637\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 108, train loss=0.1453663408756256, validation loss=0.14683416485786438, c=0.6474392696889989, ibs=0.15507617543527835, ibnll=0.46964977069418884\n",
      "Validation loss decreased (0.145795 --> 0.145037).  Saving model ...                                                   \n",
      "it: 109, train loss=0.1453450322151184, validation loss=0.14503678679466248, c=0.6438186600649853, ibs=0.15672382467471357, ibnll=0.47223880991613315\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 110, train loss=0.14548173546791077, validation loss=0.1473291665315628, c=0.6480891226984372, ibs=0.15474087279589693, ibnll=0.4691470757963808\n",
      "Validation loss decreased (0.145037 --> 0.144707).  Saving model ...                                                   \n",
      "it: 111, train loss=0.14532823860645294, validation loss=0.14470717310905457, c=0.6440043323533963, ibs=0.15659728192062677, ibnll=0.47181378159662496\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 112, train loss=0.145091712474823, validation loss=0.14611215889453888, c=0.6467584712981588, ibs=0.15468690252741782, ibnll=0.4684493364957447\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 113, train loss=0.144417405128479, validation loss=0.1450595259666443, c=0.6459538913817113, ibs=0.15508096014307363, ibnll=0.4687889307674494\n",
      "Validation loss decreased (0.144707 --> 0.144564).  Saving model ...                                                   \n",
      "it: 114, train loss=0.14399060606956482, validation loss=0.1445639580488205, c=0.645118366083862, ibs=0.15539414996801756, ibnll=0.4692274518499057\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 115, train loss=0.1438785046339035, validation loss=0.14607089757919312, c=0.6477487235030172, ibs=0.15429139131307065, ibnll=0.4675852012493303\n",
      "Validation loss decreased (0.144564 --> 0.143938).  Saving model ...                                                   \n",
      "it: 116, train loss=0.14395250380039215, validation loss=0.14393800497055054, c=0.6438186600649853, ibs=0.1563576690161106, ibnll=0.47098714390373514\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 117, train loss=0.1441887617111206, validation loss=0.1463373899459839, c=0.6489246479962866, ibs=0.15400802301585662, ibnll=0.467104444319091\n",
      "Validation loss decreased (0.143938 --> 0.143656).  Saving model ...                                                   \n",
      "it: 118, train loss=0.14389240741729736, validation loss=0.14365644752979279, c=0.644932693795451, ibs=0.1557627103322882, ibnll=0.4696140002989086\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 119, train loss=0.14346911013126373, validation loss=0.14475177228450775, c=0.6479034504100263, ibs=0.1540801087898386, ibnll=0.46654267421232476\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 120, train loss=0.142840176820755, validation loss=0.14418542385101318, c=0.64796534117283, ibs=0.1541763057578274, ibnll=0.46649519161352293\n",
      "Validation loss decreased (0.143656 --> 0.143307).  Saving model ...                                                   \n",
      "it: 121, train loss=0.14253753423690796, validation loss=0.143306702375412, c=0.645830109856104, ibs=0.15490462061916946, ibnll=0.46766489685438645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 122, train loss=0.14254355430603027, validation loss=0.14518830180168152, c=0.6499767909639487, ibs=0.1534874290787786, ibnll=0.4654798987813106\n",
      "Validation loss decreased (0.143307 --> 0.142779).  Saving model ...                                                   \n",
      "it: 123, train loss=0.142656609416008, validation loss=0.14277924597263336, c=0.645489710660684, ibs=0.15577090068245777, ibnll=0.4693005502708991\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 124, train loss=0.14285698533058167, validation loss=0.14505945146083832, c=0.6509360977874052, ibs=0.15320819059565832, ibnll=0.4648327544135913\n",
      "Validation loss decreased (0.142779 --> 0.142516).  Saving model ...                                                   \n",
      "it: 125, train loss=0.14241454005241394, validation loss=0.14251594245433807, c=0.6466965805353551, ibs=0.1546669967352588, ibnll=0.46685128061841485\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 126, train loss=0.14186444878578186, validation loss=0.14320890605449677, c=0.6497601732941358, ibs=0.15341844973985894, ibnll=0.4644923853954248\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 127, train loss=0.14135152101516724, validation loss=0.1432693749666214, c=0.6505956985919852, ibs=0.15319695516382864, ibnll=0.4640508009042994\n",
      "Validation loss decreased (0.142516 --> 0.141980).  Saving model ...                                                   \n",
      "it: 128, train loss=0.14121009409427643, validation loss=0.14198008179664612, c=0.6481510134612409, ibs=0.1544753671903514, ibnll=0.4662505328041253\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 129, train loss=0.14134778082370758, validation loss=0.14430300891399384, c=0.6521120222806747, ibs=0.1526887429077018, ibnll=0.46341075139407445\n",
      "Validation loss decreased (0.141980 --> 0.141635).  Saving model ...                                                   \n",
      "it: 130, train loss=0.1414729356765747, validation loss=0.1416352242231369, c=0.647253597400588, ibs=0.15522396436551233, ibnll=0.46773174655439775\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 131, train loss=0.1415887027978897, validation loss=0.14373673498630524, c=0.6523595853318892, ibs=0.15254639859432786, ibnll=0.46286985463281266\n",
      "Validation loss decreased (0.141635 --> 0.141503).  Saving model ...                                                   \n",
      "it: 132, train loss=0.1409626305103302, validation loss=0.14150336384773254, c=0.6502243540151632, ibs=0.15373483927240886, ibnll=0.4645030665467548\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 133, train loss=0.14035460352897644, validation loss=0.14182868599891663, c=0.6512146062200217, ibs=0.152969127204611, ibnll=0.46300227956237877\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 134, train loss=0.13999296724796295, validation loss=0.14241820573806763, c=0.6517406777038527, ibs=0.15246176769114347, ibnll=0.462159548097978\n",
      "Validation loss decreased (0.141503 --> 0.140910).  Saving model ...                                                   \n",
      "it: 135, train loss=0.1399707794189453, validation loss=0.14090973138809204, c=0.6496054463871267, ibs=0.1541619607641205, ibnll=0.4652079004436116\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 136, train loss=0.14018571376800537, validation loss=0.14343135058879852, c=0.6529475475785239, ibs=0.15208139307046528, ibnll=0.46174393103324135\n",
      "Validation loss decreased (0.140910 --> 0.140660).  Saving model ...                                                   \n",
      "it: 137, train loss=0.14030329883098602, validation loss=0.14065977931022644, c=0.6501934086337614, ibs=0.15478738808092454, ibnll=0.4664644965041393\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 138, train loss=0.14036013185977936, validation loss=0.14266444742679596, c=0.6534117282995513, ibs=0.15194415047520485, ibnll=0.4611090398650757\n",
      "Validation loss decreased (0.140660 --> 0.140558).  Saving model ...                                                   \n",
      "it: 139, train loss=0.13966475427150726, validation loss=0.1405583620071411, c=0.6519882407550673, ibs=0.1530540860308509, ibnll=0.46268745310938386\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 140, train loss=0.1390344351530075, validation loss=0.1406935751438141, c=0.6521429676620765, ibs=0.15252985760114815, ibnll=0.4616290205222822\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 141, train loss=0.13874442875385284, validation loss=0.14160920679569244, c=0.6537521274949714, ibs=0.15180861737758777, ibnll=0.46044271171944584\n",
      "Validation loss decreased (0.140558 --> 0.139872).  Saving model ...                                                   \n",
      "it: 142, train loss=0.1388149857521057, validation loss=0.1398722529411316, c=0.6517097323224509, ibs=0.15383444781055933, ibnll=0.46417054077239517\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 143, train loss=0.13913017511367798, validation loss=0.14262157678604126, c=0.6548352158440353, ibs=0.15145190197722247, ibnll=0.46004814008637396\n",
      "Validation loss decreased (0.139872 --> 0.139645).  Saving model ...                                                   \n",
      "it: 144, train loss=0.1392274796962738, validation loss=0.13964535295963287, c=0.6522048584248801, ibs=0.1542660131097586, ibnll=0.46502274802681876\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 145, train loss=0.13916561007499695, validation loss=0.14143092930316925, c=0.6550208881324462, ibs=0.15141064352120112, ibnll=0.4594746825137087\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 146, train loss=0.1383657157421112, validation loss=0.13965678215026855, c=0.6537521274949714, ibs=0.15227556060381361, ibnll=0.46068448378595267\n",
      "Validation loss decreased (0.139645 --> 0.139472).  Saving model ...                                                   \n",
      "it: 147, train loss=0.1377699375152588, validation loss=0.13947199285030365, c=0.6535045644437568, ibs=0.1522133639018221, ibnll=0.4604882334482927\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 148, train loss=0.137625053524971, validation loss=0.14095619320869446, c=0.6554541234720718, ibs=0.15111560394625675, ibnll=0.45865569302407916\n",
      "Validation loss decreased (0.139472 --> 0.138904).  Saving model ...                                                   \n",
      "it: 149, train loss=0.1378321796655655, validation loss=0.1389036476612091, c=0.6537521274949714, ibs=0.15366065261564169, ibnll=0.46345797699424734\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 150, train loss=0.13824515044689178, validation loss=0.141764298081398, c=0.6558254680488937, ibs=0.15084196649874365, ibnll=0.4583539561169867\n",
      "Validation loss decreased (0.138904 --> 0.138652).  Saving model ...                                                   \n",
      "it: 151, train loss=0.13820503652095795, validation loss=0.1386520117521286, c=0.653659291350766, ibs=0.15356063429612885, ibnll=0.4631578882445553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 152, train loss=0.13790154457092285, validation loss=0.14006514847278595, c=0.6563205941513229, ibs=0.1509263277601421, ibnll=0.45789844795612444\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 153, train loss=0.137063130736351, validation loss=0.1389235556125641, c=0.6557016865232864, ibs=0.15140503519633627, ibnll=0.458519939821558\n",
      "Validation loss decreased (0.138652 --> 0.138330).  Saving model ...                                                   \n",
      "it: 154, train loss=0.13660873472690582, validation loss=0.13832981884479523, c=0.6548042704626335, ibs=0.15202669808360472, ibnll=0.4596694397718606\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 155, train loss=0.1366717368364334, validation loss=0.14044426381587982, c=0.6563205941513229, ibs=0.15044681949364058, ibnll=0.45697139369148193\n",
      "Validation loss decreased (0.138330 --> 0.138021).  Saving model ...                                                   \n",
      "it: 156, train loss=0.13701005280017853, validation loss=0.13802103698253632, c=0.6551137242766517, ibs=0.15353848663750128, ibnll=0.4629038701002779\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 157, train loss=0.13748276233673096, validation loss=0.14076992869377136, c=0.6561968126257156, ibs=0.15024783532486033, ibnll=0.4566721952997649\n",
      "Validation loss decreased (0.138021 --> 0.137646).  Saving model ...                                                   \n",
      "it: 158, train loss=0.13716569542884827, validation loss=0.1376456469297409, c=0.6556088503790809, ibs=0.1526189776309426, ibnll=0.4607699342393388\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 159, train loss=0.1365530639886856, validation loss=0.13854488730430603, c=0.6570323379235649, ibs=0.15053752083447772, ibnll=0.4565090631310221\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 160, train loss=0.1358126848936081, validation loss=0.13834069669246674, c=0.657187064830574, ibs=0.1504477902270648, ibnll=0.4562209881000072\n",
      "Validation loss decreased (0.137646 --> 0.137244).  Saving model ...                                                   \n",
      "it: 161, train loss=0.13562767207622528, validation loss=0.13724425435066223, c=0.6565681572025375, ibs=0.15194314830706354, ibnll=0.45913234751276044\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 162, train loss=0.13594591617584229, validation loss=0.1399962455034256, c=0.6571561194491722, ibs=0.14978667667630452, ibnll=0.45533601714869687\n",
      "Validation loss decreased (0.137244 --> 0.137160).  Saving model ...                                                   \n",
      "it: 163, train loss=0.13634340465068817, validation loss=0.1371600329875946, c=0.6562587033885192, ibs=0.153281228317997, ibnll=0.4620806957795876\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 164, train loss=0.13670134544372559, validation loss=0.13949346542358398, c=0.6572489555933777, ibs=0.1497062395968275, ibnll=0.4550401633435019\n",
      "Validation loss decreased (0.137160 --> 0.136763).  Saving model ...                                                   \n",
      "it: 165, train loss=0.1359928548336029, validation loss=0.13676293194293976, c=0.6582701531796379, ibs=0.15144883906827947, ibnll=0.45792469794208684\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 166, train loss=0.13518543541431427, validation loss=0.13707365095615387, c=0.6586414977564599, ibs=0.15038409849696643, ibnll=0.4556728064868508\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 167, train loss=0.13474421203136444, validation loss=0.13807140290737152, c=0.6578369178400124, ibs=0.14959102725581933, ibnll=0.45422522107757324\n",
      "Validation loss decreased (0.136763 --> 0.136382).  Saving model ...                                                   \n",
      "it: 168, train loss=0.13491284847259521, validation loss=0.1363823562860489, c=0.6582392077982361, ibs=0.152093045646524, ibnll=0.4592046776785987\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 169, train loss=0.1354430913925171, validation loss=0.13942788541316986, c=0.6570632833049668, ibs=0.14929259838820574, ibnll=0.4540292128159733\n",
      "Validation loss decreased (0.136382 --> 0.136264).  Saving model ...                                                   \n",
      "it: 170, train loss=0.1356305629014969, validation loss=0.13626408576965332, c=0.6585796069936561, ibs=0.15263801131328292, ibnll=0.460390599754592\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 171, train loss=0.13552069664001465, validation loss=0.137844517827034, c=0.6583010985610398, ibs=0.14938077969784416, ibnll=0.4537754331198657\n",
      "Validation loss decreased (0.136264 --> 0.136122).  Saving model ...                                                   \n",
      "it: 172, train loss=0.13461291790008545, validation loss=0.13612164556980133, c=0.6585796069936561, ibs=0.15029422860552702, ibnll=0.45517425704274866\n",
      "Validation loss decreased (0.136122 --> 0.135873).  Saving model ...                                                   \n",
      "it: 173, train loss=0.13398928940296173, validation loss=0.13587282598018646, c=0.6595389138171128, ibs=0.15039939330788935, ibnll=0.45529521330879835\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 174, train loss=0.13393498957157135, validation loss=0.1377766728401184, c=0.6576821909330033, ibs=0.14897817722392495, ibnll=0.4527223942846747\n",
      "Validation loss decreased (0.135873 --> 0.135647).  Saving model ...                                                   \n",
      "it: 175, train loss=0.13429903984069824, validation loss=0.13564656674861908, c=0.6599102583939347, ibs=0.15218998990981636, ibnll=0.4592173909741198\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 176, train loss=0.13487668335437775, validation loss=0.13871699571609497, c=0.6576821909330033, ibs=0.1488805784852687, ibnll=0.45289709872153744\n",
      "Validation loss decreased (0.135647 --> 0.135404).  Saving model ...                                                   \n",
      "it: 177, train loss=0.13487564027309418, validation loss=0.13540352880954742, c=0.6599102583939347, ibs=0.15219026896062254, ibnll=0.45917272240694\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 178, train loss=0.1345708817243576, validation loss=0.13671809434890747, c=0.6588581154262726, ibs=0.14894646335740147, ibnll=0.4524580322501502\n",
      "Validation loss decreased (0.135404 --> 0.135305).  Saving model ...                                                   \n",
      "it: 179, train loss=0.13361208140850067, validation loss=0.13530497252941132, c=0.6600030945381402, ibs=0.14959882151996348, ibnll=0.45340009599594056\n",
      "Validation loss decreased (0.135305 --> 0.134890).  Saving model ...                                                   \n",
      "it: 180, train loss=0.13307388126850128, validation loss=0.13488991558551788, c=0.6604982206405694, ibs=0.15008899022454783, ibnll=0.45433264475672336\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 181, train loss=0.13314294815063477, validation loss=0.1370794177055359, c=0.6583939347052452, ibs=0.14846671190896335, ibnll=0.4513832455375507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.134890 --> 0.134850).  Saving model ...                                                   \n",
      "it: 182, train loss=0.13358767330646515, validation loss=0.13485030829906464, c=0.6607767290731859, ibs=0.15209873729812798, ibnll=0.45883972326306677\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 183, train loss=0.1342737078666687, validation loss=0.1379466950893402, c=0.6587033885192635, ibs=0.14839681365121407, ibnll=0.4516146485459465\n",
      "Validation loss decreased (0.134850 --> 0.134505).  Saving model ...                                                   \n",
      "it: 184, train loss=0.13420478999614716, validation loss=0.13450495898723602, c=0.6613028005570168, ibs=0.15172987159205556, ibnll=0.45793919044936476\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 185, train loss=0.13374052941799164, validation loss=0.13555437326431274, c=0.6599412037753365, ibs=0.14849828812950058, ibnll=0.45112199581337675\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 186, train loss=0.13267366588115692, validation loss=0.1346370130777359, c=0.6605910567847749, ibs=0.14881119033399454, ibnll=0.4514723656865932\n",
      "Validation loss decreased (0.134505 --> 0.133960).  Saving model ...                                                   \n",
      "it: 187, train loss=0.13226449489593506, validation loss=0.13395993411540985, c=0.660745783691784, ibs=0.15006234020025408, ibnll=0.4540154688491417\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 188, train loss=0.13258643448352814, validation loss=0.1368599385023117, c=0.6583629893238434, ibs=0.14798199939423667, ibnll=0.45019348219456123\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 189, train loss=0.1331675946712494, validation loss=0.13421714305877686, c=0.6617360358966424, ibs=0.15206248973981504, ibnll=0.45858949620064987\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 190, train loss=0.1337496042251587, validation loss=0.13667987287044525, c=0.6589818969518799, ibs=0.14803853231476863, ibnll=0.4504246114186834\n",
      "Validation loss decreased (0.133960 --> 0.133577).  Saving model ...                                                   \n",
      "it: 191, train loss=0.1330959051847458, validation loss=0.13357660174369812, c=0.6615813089896333, ibs=0.1502544679243295, ibnll=0.45437464663350036\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 192, train loss=0.13220040500164032, validation loss=0.13389746844768524, c=0.6612409097942132, ibs=0.1485659215904339, ibnll=0.45073258931774907\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 193, train loss=0.13159428536891937, validation loss=0.13460741937160492, c=0.6597245861055238, ibs=0.14783103740275846, ibnll=0.4492667065760899\n",
      "Validation loss decreased (0.133577 --> 0.133211).  Saving model ...                                                   \n",
      "it: 194, train loss=0.13175398111343384, validation loss=0.13321074843406677, c=0.6624168342874825, ibs=0.15038230109704695, ibnll=0.4545341896154999\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 195, train loss=0.13237391412258148, validation loss=0.13647884130477905, c=0.6603434937335603, ibs=0.14753494783020787, ibnll=0.4490721509106672\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 196, train loss=0.1327393501996994, validation loss=0.13343586027622223, c=0.6627572334829027, ibs=0.15162686580596912, ibnll=0.4574134429316845\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 197, train loss=0.13285069167613983, validation loss=0.13513237237930298, c=0.660034039919542, ibs=0.14775510867170702, ibnll=0.4493703420727967\n",
      "Validation loss decreased (0.133211 --> 0.132885).  Saving model ...                                                   \n",
      "it: 198, train loss=0.13188974559307098, validation loss=0.1328851580619812, c=0.6624787250502863, ibs=0.1490813035173798, ibnll=0.45159175838627513\n",
      "Validation loss decreased (0.132885 --> 0.132811).  Saving model ...                                                   \n",
      "it: 199, train loss=0.13108038902282715, validation loss=0.13281087577342987, c=0.6625715611944917, ibs=0.14856740389021347, ibnll=0.45039852416108844\n",
      "                                                                                                                       \n",
      "Results: 0.6604070729422084, 0.16514659932444886, 0.49324038154584066\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 646, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 3, 'num_latent': 134, 'num_odefunc_layers1': 4, 'odefunc_neurons1': 697, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.001}\n",
      "  7%|                                   | 7/100 [10:19:33<134:16:08, 5197.51s/trial, best loss: 0.13281087577342987]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f44817c7854b51b51523ce2da7c1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.409047).  Saving model ...                                                        \n",
      "it: 0, train loss=0.43692028522491455, validation loss=0.40904706716537476, c=0.5964375173949346, ibs=0.2770400978379276, ibnll=0.9668574094433864\n",
      "Validation loss decreased (0.409047 --> 0.401583).  Saving model ...                                                   \n",
      "it: 1, train loss=0.42815762758255005, validation loss=0.401582807302475, c=0.5909639113090268, ibs=0.2752466839926786, ibnll=0.9539170047498091\n",
      "Validation loss decreased (0.401583 --> 0.393503).  Saving model ...                                                   \n",
      "it: 2, train loss=0.42032039165496826, validation loss=0.39350301027297974, c=0.5491542196245787, ibs=0.27324692941505413, ibnll=0.9401194319860201\n",
      "Validation loss decreased (0.393503 --> 0.383875).  Saving model ...                                                   \n",
      "it: 3, train loss=0.41212019324302673, validation loss=0.3838752508163452, c=0.475368772613415, ibs=0.2707808256842514, ibnll=0.9232622882608792\n",
      "Validation loss decreased (0.383875 --> 0.375103).  Saving model ...                                                   \n",
      "it: 4, train loss=0.4018801748752594, validation loss=0.3751032054424286, c=0.5987259176794384, ibs=0.2685551287120336, ibnll=0.9076910766856807\n",
      "Validation loss decreased (0.375103 --> 0.363023).  Saving model ...                                                   \n",
      "it: 5, train loss=0.3926382064819336, validation loss=0.36302345991134644, c=0.5607199183597736, ibs=0.2652624239406811, ibnll=0.8865192813726727\n",
      "Validation loss decreased (0.363023 --> 0.352162).  Saving model ...                                                   \n",
      "it: 6, train loss=0.38003459572792053, validation loss=0.3521619141101837, c=0.6176516065188484, ibs=0.26220697549167765, ibnll=0.8672461362693711\n",
      "Validation loss decreased (0.352162 --> 0.341548).  Saving model ...                                                   \n",
      "it: 7, train loss=0.3687778413295746, validation loss=0.3415481746196747, c=0.49315026131057305, ibs=0.25907984113241217, ibnll=0.8483509981506946\n",
      "Validation loss decreased (0.341548 --> 0.331147).  Saving model ...                                                   \n",
      "it: 8, train loss=0.35761553049087524, validation loss=0.3311466574668884, c=0.5030151219964746, ibs=0.2558962620639208, ibnll=0.8295266066256342\n",
      "Validation loss decreased (0.331147 --> 0.320887).  Saving model ...                                                   \n",
      "it: 9, train loss=0.3467809557914734, validation loss=0.32088732719421387, c=0.5679871354794817, ibs=0.252632605121166, ibnll=0.8110132615286506\n",
      "Validation loss decreased (0.320887 --> 0.310946).  Saving model ...                                                   \n",
      "it: 10, train loss=0.3361298739910126, validation loss=0.31094563007354736, c=0.6020038964653492, ibs=0.2493403074773234, ibnll=0.7929804560765683\n",
      "Validation loss decreased (0.310946 --> 0.301461).  Saving model ...                                                   \n",
      "it: 11, train loss=0.3257638216018677, validation loss=0.30146074295043945, c=0.5272288709527786, ibs=0.2460550540106652, ibnll=0.7757056903220334\n",
      "Validation loss decreased (0.301461 --> 0.292389).  Saving model ...                                                   \n",
      "it: 12, train loss=0.31589362025260925, validation loss=0.2923892140388489, c=0.6156724495160343, ibs=0.24277585085141765, ibnll=0.759005975000919\n",
      "Validation loss decreased (0.292389 --> 0.283715).  Saving model ...                                                   \n",
      "it: 13, train loss=0.30642837285995483, validation loss=0.28371506929397583, c=0.6062096050963293, ibs=0.23948308097343451, ibnll=0.7428904637437196\n",
      "Validation loss decreased (0.283715 --> 0.275487).  Saving model ...                                                   \n",
      "it: 14, train loss=0.2973337471485138, validation loss=0.2754870355129242, c=0.4828215357021369, ibs=0.23622922611900424, ibnll=0.727490380341826\n",
      "Validation loss decreased (0.275487 --> 0.267642).  Saving model ...                                                   \n",
      "it: 15, train loss=0.2886962890625, validation loss=0.26764193177223206, c=0.460648792404985, ibs=0.23299301597262218, ibnll=0.712684744201737\n",
      "Validation loss decreased (0.267642 --> 0.260152).  Saving model ...                                                   \n",
      "it: 16, train loss=0.28045785427093506, validation loss=0.2601517140865326, c=0.5366607910443145, ibs=0.2297727541103709, ibnll=0.6984374105653404\n",
      "Validation loss decreased (0.260152 --> 0.253031).  Saving model ...                                                   \n",
      "it: 17, train loss=0.27258679270744324, validation loss=0.2530311346054077, c=0.5441754027893744, ibs=0.22657737527825617, ibnll=0.684758888606276\n",
      "Validation loss decreased (0.253031 --> 0.246277).  Saving model ...                                                   \n",
      "it: 18, train loss=0.26509198546409607, validation loss=0.24627652764320374, c=0.5354238179175558, ibs=0.22341697436376118, ibnll=0.6716632842563024\n",
      "Validation loss decreased (0.246277 --> 0.239909).  Saving model ...                                                   \n",
      "it: 19, train loss=0.2579633891582489, validation loss=0.23990917205810547, c=0.5694405789034234, ibs=0.22030753438623893, ibnll=0.659180117591937\n",
      "Validation loss decreased (0.239909 --> 0.233888).  Saving model ...                                                   \n",
      "it: 20, train loss=0.25122982263565063, validation loss=0.23388835787773132, c=0.5882425704301574, ibs=0.2172515174036042, ibnll=0.6473027563585375\n",
      "Validation loss decreased (0.233888 --> 0.228259).  Saving model ...                                                   \n",
      "it: 21, train loss=0.24485540390014648, validation loss=0.2282593846321106, c=0.6328973003061509, ibs=0.2142654447100214, ibnll=0.6360510915786408\n",
      "Validation loss decreased (0.228259 --> 0.222960).  Saving model ...                                                   \n",
      "it: 22, train loss=0.2388838529586792, validation loss=0.22295980155467987, c=0.6233107585737699, ibs=0.21135267619294665, ibnll=0.6254089233673\n",
      "Validation loss decreased (0.222960 --> 0.218032).  Saving model ...                                                   \n",
      "it: 23, train loss=0.2332535833120346, validation loss=0.21803215146064758, c=0.61910504994279, ibs=0.208522350006869, ibnll=0.6153710448752622\n",
      "Validation loss decreased (0.218032 --> 0.213422).  Saving model ...                                                   \n",
      "it: 24, train loss=0.22800423204898834, validation loss=0.21342168748378754, c=0.6271453752667223, ibs=0.205778868137064, ibnll=0.6059209661376765\n",
      "Validation loss decreased (0.213422 --> 0.209145).  Saving model ...                                                   \n",
      "it: 25, train loss=0.22308197617530823, validation loss=0.20914499461650848, c=0.6294337755512261, ibs=0.20313271381982156, ibnll=0.597061793359759\n",
      "Validation loss decreased (0.209145 --> 0.205186).  Saving model ...                                                   \n",
      "it: 26, train loss=0.2185017168521881, validation loss=0.20518596470355988, c=0.6257228561709497, ibs=0.20057867024473486, ibnll=0.5887427485010447\n",
      "Validation loss decreased (0.205186 --> 0.201525).  Saving model ...                                                   \n",
      "it: 27, train loss=0.21424737572669983, validation loss=0.2015254944562912, c=0.6259393264681324, ibs=0.19812704039384152, ibnll=0.5809647077263894\n",
      "Validation loss decreased (0.201525 --> 0.198145).  Saving model ...                                                   \n",
      "it: 28, train loss=0.21030205488204956, validation loss=0.19814486801624298, c=0.6177134551751863, ibs=0.19578149416323276, ibnll=0.5737149231246493\n",
      "Validation loss decreased (0.198145 --> 0.195052).  Saving model ...                                                   \n",
      "it: 29, train loss=0.2066529393196106, validation loss=0.1950518637895584, c=0.6239910937934874, ibs=0.19356210287482978, ibnll=0.5670329481484147\n",
      "Validation loss decreased (0.195052 --> 0.192204).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.20329967141151428, validation loss=0.19220390915870667, c=0.6123016977456165, ibs=0.1914262074144969, ibnll=0.5607546935275544\n",
      "Validation loss decreased (0.192204 --> 0.189601).  Saving model ...                                                   \n",
      "it: 31, train loss=0.20020122826099396, validation loss=0.18960119783878326, c=0.639051241611776, ibs=0.18942258433987263, ibnll=0.5549970121895845\n",
      "Validation loss decreased (0.189601 --> 0.187235).  Saving model ...                                                   \n",
      "it: 32, train loss=0.19737215340137482, validation loss=0.18723466992378235, c=0.6355567925286824, ibs=0.18754172459750357, ibnll=0.5497184715951567\n",
      "Validation loss decreased (0.187235 --> 0.185082).  Saving model ...                                                   \n",
      "it: 33, train loss=0.1947798877954483, validation loss=0.18508240580558777, c=0.6476791291709187, ibs=0.1857371999535702, ibnll=0.5447735046393306\n",
      "Validation loss decreased (0.185082 --> 0.183193).  Saving model ...                                                   \n",
      "it: 34, train loss=0.1924131065607071, validation loss=0.18319322168827057, c=0.6434424962117699, ibs=0.18409735377472772, ibnll=0.5403744569724379\n",
      "Validation loss decreased (0.183193 --> 0.181353).  Saving model ...                                                   \n",
      "it: 35, train loss=0.19032306969165802, validation loss=0.18135255575180054, c=0.6494108915483812, ibs=0.18248907325173222, ibnll=0.536105650106565\n",
      "Validation loss decreased (0.181353 --> 0.179736).  Saving model ...                                                   \n",
      "it: 36, train loss=0.18829523026943207, validation loss=0.17973636090755463, c=0.6473080372328911, ibs=0.18114894267951995, ibnll=0.5326417878997689\n",
      "Validation loss decreased (0.179736 --> 0.178389).  Saving model ...                                                   \n",
      "it: 37, train loss=0.18655620515346527, validation loss=0.1783885657787323, c=0.6234035315582769, ibs=0.17966040445960235, ibnll=0.528940896267413\n",
      "Validation loss decreased (0.178389 --> 0.177024).  Saving model ...                                                   \n",
      "it: 38, train loss=0.18493619561195374, validation loss=0.1770235002040863, c=0.6317840244920679, ibs=0.17845181934177962, ibnll=0.525915453676705\n",
      "Validation loss decreased (0.177024 --> 0.175895).  Saving model ...                                                   \n",
      "it: 39, train loss=0.183467835187912, validation loss=0.17589478194713593, c=0.6249188236385564, ibs=0.17727989171864078, ibnll=0.5230500114432811\n",
      "Validation loss decreased (0.175895 --> 0.174860).  Saving model ...                                                   \n",
      "it: 40, train loss=0.18216830492019653, validation loss=0.17486022412776947, c=0.64944181587655, ibs=0.17626504134468238, ibnll=0.5206378992561074\n",
      "Validation loss decreased (0.174860 --> 0.174009).  Saving model ...                                                   \n",
      "it: 41, train loss=0.18102337419986725, validation loss=0.17400941252708435, c=0.6283823483934812, ibs=0.17515582116748174, ibnll=0.5180543229428418\n",
      "Validation loss decreased (0.174009 --> 0.173041).  Saving model ...                                                   \n",
      "it: 42, train loss=0.1799725741147995, validation loss=0.17304106056690216, c=0.6397625011596623, ibs=0.1745861701364594, ibnll=0.5164177963409229\n",
      "Validation loss decreased (0.173041 --> 0.172810).  Saving model ...                                                   \n",
      "it: 43, train loss=0.17909829318523407, validation loss=0.1728096604347229, c=0.6268361319850326, ibs=0.17359853877544967, ibnll=0.5145435308188228\n",
      "Validation loss decreased (0.172810 --> 0.172206).  Saving model ...                                                   \n",
      "it: 44, train loss=0.17841598391532898, validation loss=0.17220625281333923, c=0.6321551164300956, ibs=0.1728466579009702, ibnll=0.5128805781974044\n",
      "Validation loss decreased (0.172206 --> 0.171642).  Saving model ...                                                   \n",
      "it: 45, train loss=0.17770497500896454, validation loss=0.17164242267608643, c=0.6401335930976899, ibs=0.17225488089847346, ibnll=0.5114962407417156\n",
      "Validation loss decreased (0.171642 --> 0.171137).  Saving model ...                                                   \n",
      "it: 46, train loss=0.1771559715270996, validation loss=0.17113664746284485, c=0.6402572904103658, ibs=0.17190615200821402, ibnll=0.5105670135639522\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 47, train loss=0.17661361396312714, validation loss=0.17127424478530884, c=0.5370937316386801, ibs=0.17113601975582937, ibnll=0.5093550669764335\n",
      "Validation loss decreased (0.171137 --> 0.170954).  Saving model ...                                                   \n",
      "it: 48, train loss=0.17632263898849487, validation loss=0.17095351219177246, c=0.5218480378513777, ibs=0.17061353853322087, ibnll=0.5082568610260506\n",
      "Validation loss decreased (0.170954 --> 0.170527).  Saving model ...                                                   \n",
      "it: 49, train loss=0.17589479684829712, validation loss=0.17052650451660156, c=0.6162290874230757, ibs=0.170088515712684, ibnll=0.5071319681617054\n",
      "Validation loss decreased (0.170527 --> 0.170241).  Saving model ...                                                   \n",
      "it: 50, train loss=0.1753949075937271, validation loss=0.1702408790588379, c=0.5996845718526765, ibs=0.16960632378548487, ibnll=0.5061575620249753\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 51, train loss=0.1749819815158844, validation loss=0.1703421175479889, c=0.5526795930358412, ibs=0.1696444899749702, ibnll=0.5062438895861311\n",
      "Validation loss decreased (0.170241 --> 0.170088).  Saving model ...                                                   \n",
      "it: 52, train loss=0.17510664463043213, validation loss=0.17008750140666962, c=0.5153848532640628, ibs=0.16908652693171697, ibnll=0.5050937102293895\n",
      "Validation loss decreased (0.170088 --> 0.169812).  Saving model ...                                                   \n",
      "it: 53, train loss=0.17472083866596222, validation loss=0.16981181502342224, c=0.6127346383399821, ibs=0.16875324528758623, ibnll=0.5045126707271175\n",
      "Validation loss decreased (0.169812 --> 0.169711).  Saving model ...                                                   \n",
      "it: 54, train loss=0.17434407770633698, validation loss=0.16971059143543243, c=0.5125088907443486, ibs=0.1682708279111633, ibnll=0.5035694751882\n",
      "Validation loss decreased (0.169711 --> 0.169433).  Saving model ...                                                   \n",
      "it: 55, train loss=0.17412227392196655, validation loss=0.1694333702325821, c=0.6477719021554257, ibs=0.16803321013619107, ibnll=0.5030311572726393\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 56, train loss=0.1738588660955429, validation loss=0.1694646179676056, c=0.5779447691498902, ibs=0.16791397835425262, ibnll=0.502973998417086\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 57, train loss=0.17370204627513885, validation loss=0.16947974264621735, c=0.6050035562977394, ibs=0.16797137946447604, ibnll=0.503038856046913\n",
      "Validation loss decreased (0.169433 --> 0.169385).  Saving model ...                                                   \n",
      "it: 58, train loss=0.17374126613140106, validation loss=0.1693849414587021, c=0.6050654049540773, ibs=0.1677736348034655, ibnll=0.5027105235488006\n",
      "Validation loss decreased (0.169385 --> 0.169183).  Saving model ...                                                   \n",
      "it: 59, train loss=0.1735817939043045, validation loss=0.16918303072452545, c=0.6037665831709806, ibs=0.16723303089142538, ibnll=0.5016591047038315\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 60, train loss=0.17329783737659454, validation loss=0.16927418112754822, c=0.4963973157683149, ibs=0.1670755603286179, ibnll=0.5014893516073928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.169183 --> 0.169068).  Saving model ...                                                   \n",
      "it: 61, train loss=0.17330853641033173, validation loss=0.16906790435314178, c=0.6010761666202802, ibs=0.16684969866942137, ibnll=0.5009931758630704\n",
      "Validation loss decreased (0.169068 --> 0.168988).  Saving model ...                                                   \n",
      "it: 62, train loss=0.1731421798467636, validation loss=0.168988436460495, c=0.6216408448526456, ibs=0.1669092427973906, ibnll=0.5010262463342114\n",
      "Validation loss decreased (0.168988 --> 0.168777).  Saving model ...                                                   \n",
      "it: 63, train loss=0.173108771443367, validation loss=0.16877660155296326, c=0.6329591489624888, ibs=0.16676662814054735, ibnll=0.5007214088484543\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 64, train loss=0.17289744317531586, validation loss=0.168927863240242, c=0.5904072734019853, ibs=0.1666793819937016, ibnll=0.5006754771273851\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 65, train loss=0.1728866845369339, validation loss=0.16881749033927917, c=0.6331446949315026, ibs=0.16722283367002147, ibnll=0.5015512473679132\n",
      "Validation loss decreased (0.168777 --> 0.168720).  Saving model ...                                                   \n",
      "it: 66, train loss=0.1729925572872162, validation loss=0.16872049868106842, c=0.616971271299131, ibs=0.16654362986671026, ibnll=0.5001893488232302\n",
      "Validation loss decreased (0.168720 --> 0.168687).  Saving model ...                                                   \n",
      "it: 67, train loss=0.17275013029575348, validation loss=0.16868726909160614, c=0.6307635216624918, ibs=0.1666642053418962, ibnll=0.5002869048176469\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 68, train loss=0.1729975938796997, validation loss=0.16880734264850616, c=0.60859077836534, ibs=0.16712103599078262, ibnll=0.5011379043284386\n",
      "Validation loss decreased (0.168687 --> 0.168487).  Saving model ...                                                   \n",
      "it: 69, train loss=0.17326146364212036, validation loss=0.1684868186712265, c=0.6292482295822124, ibs=0.16669326006916577, ibnll=0.5003872883744769\n",
      "Validation loss decreased (0.168487 --> 0.168402).  Saving model ...                                                   \n",
      "it: 70, train loss=0.17266859114170074, validation loss=0.1684018224477768, c=0.6408757769737452, ibs=0.16643877091412235, ibnll=0.4999285439406386\n",
      "Validation loss decreased (0.168402 --> 0.168192).  Saving model ...                                                   \n",
      "it: 71, train loss=0.17267592251300812, validation loss=0.16819249093532562, c=0.6462566100751461, ibs=0.16739590140159852, ibnll=0.501268020583684\n",
      "Validation loss decreased (0.168192 --> 0.167895).  Saving model ...                                                   \n",
      "it: 72, train loss=0.17297658324241638, validation loss=0.16789524257183075, c=0.6343816680582615, ibs=0.1662635190217659, ibnll=0.49934911846154056\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 73, train loss=0.17238037288188934, validation loss=0.1684349775314331, c=0.5967776850047932, ibs=0.16720277521269689, ibnll=0.5013905260727485\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 74, train loss=0.17246384918689728, validation loss=0.1685010939836502, c=0.6336085598540372, ibs=0.1684743347965653, ibnll=0.5030321196172308\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 75, train loss=0.17381207644939423, validation loss=0.16833968460559845, c=0.6287225160033398, ibs=0.1673947083203517, ibnll=0.5011442279906098\n",
      "Validation loss decreased (0.167895 --> 0.167741).  Saving model ...                                                   \n",
      "it: 76, train loss=0.17324204742908478, validation loss=0.16774097084999084, c=0.6416488851779695, ibs=0.16589319351978352, ibnll=0.49832211813696203\n",
      "Validation loss decreased (0.167741 --> 0.167700).  Saving model ...                                                   \n",
      "it: 77, train loss=0.1723434329032898, validation loss=0.1676998883485794, c=0.6375977981878344, ibs=0.16540044137223883, ibnll=0.4975474435820409\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 78, train loss=0.17216014862060547, validation loss=0.16861343383789062, c=0.6358041871540341, ibs=0.1674179831520331, ibnll=0.5014054653949881\n",
      "Validation loss decreased (0.167700 --> 0.167446).  Saving model ...                                                   \n",
      "it: 79, train loss=0.1732655167579651, validation loss=0.16744601726531982, c=0.613816989825896, ibs=0.1673134489840564, ibnll=0.5003155372027578\n",
      "Validation loss decreased (0.167446 --> 0.166315).  Saving model ...                                                   \n",
      "it: 80, train loss=0.1728447675704956, validation loss=0.16631461679935455, c=0.6150230386244859, ibs=0.1657581827062997, ibnll=0.4971383424280825\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 81, train loss=0.1717229038476944, validation loss=0.16721130907535553, c=0.6235272288709528, ibs=0.16562543719239528, ibnll=0.4976181760036926\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 82, train loss=0.17210622131824493, validation loss=0.16685356199741364, c=0.6226922720103906, ibs=0.16580375088783128, ibnll=0.4975352175554073\n",
      "Validation loss decreased (0.166315 --> 0.166213).  Saving model ...                                                   \n",
      "it: 83, train loss=0.17211413383483887, validation loss=0.16621297597885132, c=0.6297120945047469, ibs=0.16462883569828185, ibnll=0.49550286832260004\n",
      "Validation loss decreased (0.166213 --> 0.166157).  Saving model ...                                                   \n",
      "it: 84, train loss=0.17110608518123627, validation loss=0.16615676879882812, c=0.6235581531991218, ibs=0.16461137297288989, ibnll=0.4952906147584758\n",
      "Validation loss decreased (0.166157 --> 0.164742).  Saving model ...                                                   \n",
      "it: 85, train loss=0.17119234800338745, validation loss=0.16474232077598572, c=0.6146519466864583, ibs=0.16439671262977038, ibnll=0.4941711244990824\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 86, train loss=0.17011724412441254, validation loss=0.16550548374652863, c=0.6163218604075826, ibs=0.16678508533567155, ibnll=0.4981688666921922\n",
      "Validation loss decreased (0.164742 --> 0.163937).  Saving model ...                                                   \n",
      "it: 87, train loss=0.17219454050064087, validation loss=0.16393738985061646, c=0.6340415004484028, ibs=0.1637424143520753, ibnll=0.4926354432328673\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 88, train loss=0.16961605846881866, validation loss=0.165736585855484, c=0.6278875591427776, ibs=0.16402020847360663, ibnll=0.4942075571908625\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 89, train loss=0.17078006267547607, validation loss=0.16477300226688385, c=0.6156106008596963, ibs=0.16511098950532466, ibnll=0.49514847305744664\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 90, train loss=0.1708286851644516, validation loss=0.1651281863451004, c=0.620094628444197, ibs=0.1634740405162537, ibnll=0.49284739305604597\n",
      "Validation loss decreased (0.163937 --> 0.163631).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.1703696846961975, validation loss=0.16363149881362915, c=0.6300213377864365, ibs=0.16226540502681946, ibnll=0.49045338150160683\n",
      "Validation loss decreased (0.163631 --> 0.162842).  Saving model ...                                                   \n",
      "it: 92, train loss=0.16884015500545502, validation loss=0.16284199059009552, c=0.6218573151498283, ibs=0.16441567911475416, ibnll=0.49270675806103365\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 93, train loss=0.1705198884010315, validation loss=0.16421718895435333, c=0.6251971425920771, ibs=0.16289496453639785, ibnll=0.4916255693652527\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 94, train loss=0.16941657662391663, validation loss=0.16516435146331787, c=0.6198163094906763, ibs=0.1636022980835212, ibnll=0.4930589880131397\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 95, train loss=0.170442134141922, validation loss=0.16348965466022491, c=0.6333302409005165, ibs=0.1628242463891224, ibnll=0.49113332105156987\n",
      "Validation loss decreased (0.162842 --> 0.162283).  Saving model ...                                                   \n",
      "it: 96, train loss=0.16909483075141907, validation loss=0.162283256649971, c=0.6302687324117884, ibs=0.16386466419858356, ibnll=0.49177259375172255\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 97, train loss=0.16919268667697906, validation loss=0.16546744108200073, c=0.625444537217429, ibs=0.16320956559645247, ibnll=0.49279778985428924\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 98, train loss=0.17037245631217957, validation loss=0.16423216462135315, c=0.6251662182639082, ibs=0.16310405356956725, ibnll=0.4918134294521245\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 99, train loss=0.1694522649049759, validation loss=0.1647874265909195, c=0.6233726072301079, ibs=0.16397169621815796, ibnll=0.4935252219749384\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 100, train loss=0.17008325457572937, validation loss=0.16460929811000824, c=0.6129511086371648, ibs=0.1667161648485014, ibnll=0.49735397563918987\n",
      "EarlyStopping counter: 5 out of 20                                                                                     \n",
      "it: 101, train loss=0.17163458466529846, validation loss=0.16593694686889648, c=0.6290008349568605, ibs=0.16328403805540126, ibnll=0.493211101909248\n",
      "EarlyStopping counter: 6 out of 20                                                                                     \n",
      "it: 102, train loss=0.17042917013168335, validation loss=0.16605278849601746, c=0.6339178031357269, ibs=0.1627195566325843, ibnll=0.49251232361788827\n",
      "EarlyStopping counter: 7 out of 20                                                                                     \n",
      "it: 103, train loss=0.17037135362625122, validation loss=0.16562418639659882, c=0.6314129325540403, ibs=0.16260460307712424, ibnll=0.4921337993831701\n",
      "EarlyStopping counter: 8 out of 20                                                                                     \n",
      "it: 104, train loss=0.1700081080198288, validation loss=0.1645013689994812, c=0.6295574728639021, ibs=0.1623401652085324, ibnll=0.49104539690888116\n",
      "EarlyStopping counter: 9 out of 20                                                                                     \n",
      "it: 105, train loss=0.1692180037498474, validation loss=0.1637195199728012, c=0.6293100782385502, ibs=0.16250699352690243, ibnll=0.49079029284250975\n",
      "EarlyStopping counter: 10 out of 20                                                                                    \n",
      "it: 106, train loss=0.16867020726203918, validation loss=0.16332362592220306, c=0.6313201595695334, ibs=0.16267366457493168, ibnll=0.490632041806183\n",
      "EarlyStopping counter: 11 out of 20                                                                                    \n",
      "it: 107, train loss=0.16855700314044952, validation loss=0.1629238873720169, c=0.631660327179392, ibs=0.16266970810258383, ibnll=0.49040474939690615\n",
      "EarlyStopping counter: 12 out of 20                                                                                    \n",
      "it: 108, train loss=0.1682601273059845, validation loss=0.16262340545654297, c=0.6312892352413644, ibs=0.162523901627663, ibnll=0.4901057087764499\n",
      "EarlyStopping counter: 13 out of 20                                                                                    \n",
      "it: 109, train loss=0.1678149551153183, validation loss=0.16257822513580322, c=0.6312583109131954, ibs=0.1625091239789983, ibnll=0.4900716854011877\n",
      "EarlyStopping counter: 14 out of 20                                                                                    \n",
      "it: 110, train loss=0.1677556037902832, validation loss=0.16250872611999512, c=0.6311036892723506, ibs=0.1624949458589035, ibnll=0.4900322646532654\n",
      "EarlyStopping counter: 15 out of 20                                                                                    \n",
      "it: 111, train loss=0.16767153143882751, validation loss=0.16241700947284698, c=0.6312892352413644, ibs=0.16248052085038986, ibnll=0.4899879889575674\n",
      "EarlyStopping counter: 16 out of 20                                                                                    \n",
      "it: 112, train loss=0.1675696074962616, validation loss=0.16230829060077667, c=0.6312892352413644, ibs=0.16246457754886104, ibnll=0.4899388782805467\n",
      "Validation loss decreased (0.162283 --> 0.162189).  Saving model ...                                                   \n",
      "it: 113, train loss=0.16745831072330475, validation loss=0.16218943893909454, c=0.631660327179392, ibs=0.1624463691383052, ibnll=0.4898845643560356\n",
      "Validation loss decreased (0.162189 --> 0.162065).  Saving model ...                                                   \n",
      "it: 114, train loss=0.16734573245048523, validation loss=0.1620650291442871, c=0.6314438568822093, ibs=0.16242565112881005, ibnll=0.4898223079888422\n",
      "Validation loss decreased (0.162065 --> 0.161934).  Saving model ...                                                   \n",
      "it: 115, train loss=0.16723603010177612, validation loss=0.1619344800710678, c=0.6318767974765748, ibs=0.1624019369577967, ibnll=0.48974530464617544\n",
      "Validation loss decreased (0.161934 --> 0.161793).  Saving model ...                                                   \n",
      "it: 116, train loss=0.16712826490402222, validation loss=0.16179285943508148, c=0.6318767974765748, ibs=0.16237506178888259, ibnll=0.48964601583046796\n",
      "Validation loss decreased (0.161793 --> 0.161637).  Saving model ...                                                   \n",
      "it: 117, train loss=0.16701847314834595, validation loss=0.16163694858551025, c=0.6321241921019266, ibs=0.1623476963112845, ibnll=0.48952486739688233\n",
      "Validation loss decreased (0.161637 --> 0.161472).  Saving model ...                                                   \n",
      "it: 118, train loss=0.1669030338525772, validation loss=0.16147223114967346, c=0.6314438568822093, ibs=0.16232726893729152, ibnll=0.489398924001622\n",
      "Validation loss decreased (0.161472 --> 0.161314).  Saving model ...                                                   \n",
      "it: 119, train loss=0.1667834371328354, validation loss=0.1613135039806366, c=0.6311964622568574, ibs=0.16232262958340898, ibnll=0.48929818826130045\n",
      "Validation loss decreased (0.161314 --> 0.161176).  Saving model ...                                                   \n",
      "it: 120, train loss=0.16666799783706665, validation loss=0.1611756831407547, c=0.631753100163899, ibs=0.16233573305380286, ibnll=0.4892437497856511\n",
      "Validation loss decreased (0.161176 --> 0.161064).  Saving model ...                                                   \n",
      "it: 121, train loss=0.16656675934791565, validation loss=0.16106364130973816, c=0.6322169650864335, ibs=0.1623569744273555, ibnll=0.48922582596244557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.161064 --> 0.160971).  Saving model ...                                                   \n",
      "it: 122, train loss=0.16648393869400024, validation loss=0.16097141802310944, c=0.6328973003061509, ibs=0.16237180743711768, ibnll=0.4892133927875638\n",
      "Validation loss decreased (0.160971 --> 0.160892).  Saving model ...                                                   \n",
      "it: 123, train loss=0.1664150506258011, validation loss=0.1608920395374298, c=0.6339487274638959, ibs=0.1623716580448848, ibnll=0.48918452953354163\n",
      "Validation loss decreased (0.160892 --> 0.160823).  Saving model ...                                                   \n",
      "it: 124, train loss=0.1663525253534317, validation loss=0.1608230322599411, c=0.6344125923864304, ibs=0.1623560474931089, ibnll=0.4891355339101855\n",
      "Validation loss decreased (0.160823 --> 0.160764).  Saving model ...                                                   \n",
      "it: 125, train loss=0.16629159450531006, validation loss=0.16076435148715973, c=0.6349383059653029, ibs=0.16232897171293525, ibnll=0.4890728447330975\n",
      "Validation loss decreased (0.160764 --> 0.160715).  Saving model ...                                                   \n",
      "it: 126, train loss=0.16623149812221527, validation loss=0.1607152223587036, c=0.6350620032779788, ibs=0.1622949591872301, ibnll=0.48900281471856116\n",
      "Validation loss decreased (0.160715 --> 0.160672).  Saving model ...                                                   \n",
      "it: 127, train loss=0.16617374122142792, validation loss=0.16067203879356384, c=0.6353403222314995, ibs=0.16225755994870197, ibnll=0.4889285887781035\n",
      "Validation loss decreased (0.160672 --> 0.160630).  Saving model ...                                                   \n",
      "it: 128, train loss=0.1661197394132614, validation loss=0.1606297492980957, c=0.6349073816371339, ibs=0.16221963913787366, ibnll=0.48885132813811655\n",
      "Validation loss decreased (0.160630 --> 0.160584).  Saving model ...                                                   \n",
      "it: 129, train loss=0.1660691499710083, validation loss=0.1605837047100067, c=0.6349692302934719, ibs=0.16218402721025185, ibnll=0.48877295687437\n",
      "Validation loss decreased (0.160584 --> 0.160531).  Saving model ...                                                   \n",
      "it: 130, train loss=0.16601979732513428, validation loss=0.16053113341331482, c=0.6356495655131892, ibs=0.16215363996096238, ibnll=0.4886971495231139\n",
      "Validation loss decreased (0.160531 --> 0.160472).  Saving model ...                                                   \n",
      "it: 131, train loss=0.16596896946430206, validation loss=0.16047169268131256, c=0.635896960138541, ibs=0.16213107240788818, ibnll=0.4886288118662143\n",
      "Validation loss decreased (0.160472 --> 0.160407).  Saving model ...                                                   \n",
      "it: 132, train loss=0.16591474413871765, validation loss=0.160406693816185, c=0.6357423384976961, ibs=0.16211773704355983, ibnll=0.4885718072901212\n",
      "Validation loss decreased (0.160407 --> 0.160338).  Saving model ...                                                   \n",
      "it: 133, train loss=0.16585701704025269, validation loss=0.16033823788166046, c=0.6357423384976961, ibs=0.16211336174251434, ibnll=0.4885276191140234\n",
      "Validation loss decreased (0.160338 --> 0.160269).  Saving model ...                                                   \n",
      "it: 134, train loss=0.16579721868038177, validation loss=0.1602691262960434, c=0.635958808794879, ibs=0.16211602519705606, ibnll=0.48849510219653675\n",
      "Validation loss decreased (0.160269 --> 0.160202).  Saving model ...                                                   \n",
      "it: 135, train loss=0.16573762893676758, validation loss=0.16020193696022034, c=0.6357732628258651, ibs=0.162122393240616, ibnll=0.48847047571857466\n",
      "Validation loss decreased (0.160202 --> 0.160139).  Saving model ...                                                   \n",
      "it: 136, train loss=0.16568034887313843, validation loss=0.16013868153095245, c=0.635958808794879, ibs=0.1621283134614922, ibnll=0.488448264359403\n",
      "Validation loss decreased (0.160139 --> 0.160080).  Saving model ...                                                   \n",
      "it: 137, train loss=0.16562630236148834, validation loss=0.16008004546165466, c=0.6360825061075548, ibs=0.16212952070389844, ibnll=0.4884220881706602\n",
      "Validation loss decreased (0.160080 --> 0.160026).  Saving model ...                                                   \n",
      "it: 138, train loss=0.1655750423669815, validation loss=0.1600257158279419, c=0.6360515817793858, ibs=0.16212278716671705, ibnll=0.4883865391465348\n",
      "Validation loss decreased (0.160026 --> 0.159975).  Saving model ...                                                   \n",
      "it: 139, train loss=0.16552545130252838, validation loss=0.15997536480426788, c=0.6362989764047376, ibs=0.16210724064715568, ibnll=0.4883401903401873\n",
      "Validation loss decreased (0.159975 --> 0.159928).  Saving model ...                                                   \n",
      "it: 140, train loss=0.16547688841819763, validation loss=0.1599283665418625, c=0.6363608250610755, ibs=0.16208399350763011, ibnll=0.48828458814292186\n",
      "Validation loss decreased (0.159928 --> 0.159884).  Saving model ...                                                   \n",
      "it: 141, train loss=0.16542910039424896, validation loss=0.15988412499427795, c=0.6364226737174135, ibs=0.16205559231155706, ibnll=0.4882233324920003\n",
      "Validation loss decreased (0.159884 --> 0.159842).  Saving model ...                                                   \n",
      "it: 142, train loss=0.1653817743062973, validation loss=0.1598423421382904, c=0.6363917493892445, ibs=0.16202604263738946, ibnll=0.4881623444630078\n",
      "Validation loss decreased (0.159842 --> 0.159801).  Saving model ...                                                   \n",
      "it: 143, train loss=0.16533489525318146, validation loss=0.15980131924152374, c=0.6361443547638927, ibs=0.16199765415542508, ibnll=0.4881013717985523\n",
      "Validation loss decreased (0.159801 --> 0.159759).  Saving model ...                                                   \n",
      "it: 144, train loss=0.16528822481632233, validation loss=0.15975920855998993, c=0.6361443547638927, ibs=0.16197385978107978, ibnll=0.4880468321921869\n",
      "Validation loss decreased (0.159759 --> 0.159714).  Saving model ...                                                   \n",
      "it: 145, train loss=0.16524139046669006, validation loss=0.15971407294273376, c=0.6362062034202307, ibs=0.161956048246813, ibnll=0.48799965704965304\n",
      "Validation loss decreased (0.159714 --> 0.159665).  Saving model ...                                                   \n",
      "it: 146, train loss=0.16519412398338318, validation loss=0.1596650779247284, c=0.6363608250610755, ibs=0.16194434913604228, ibnll=0.487959084423685\n",
      "Validation loss decreased (0.159665 --> 0.159613).  Saving model ...                                                   \n",
      "it: 147, train loss=0.16514581441879272, validation loss=0.1596127599477768, c=0.6362680520765687, ibs=0.16193858683291246, ibnll=0.4879252621281267\n",
      "Validation loss decreased (0.159613 --> 0.159558).  Saving model ...                                                   \n",
      "it: 148, train loss=0.1650969684123993, validation loss=0.1595580130815506, c=0.6365463710300894, ibs=0.16193775491437676, ibnll=0.48789751852236113\n",
      "Validation loss decreased (0.159558 --> 0.159502).  Saving model ...                                                   \n",
      "it: 149, train loss=0.1650485396385193, validation loss=0.1595020890235901, c=0.6366391440145963, ibs=0.16193981104894697, ibnll=0.48787371557438364\n",
      "Validation loss decreased (0.159502 --> 0.159446).  Saving model ...                                                   \n",
      "it: 150, train loss=0.16500119864940643, validation loss=0.1594455987215042, c=0.6366082196864273, ibs=0.1619421066747744, ibnll=0.487850311764613\n",
      "Validation loss decreased (0.159446 --> 0.159390).  Saving model ...                                                   \n",
      "it: 151, train loss=0.164955273270607, validation loss=0.15939030051231384, c=0.6368556143117791, ibs=0.16194258830186622, ibnll=0.4878252328130957\n",
      "Validation loss decreased (0.159390 --> 0.159336).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 152, train loss=0.1649111807346344, validation loss=0.15933604538440704, c=0.636886538639948, ibs=0.16193868796522515, ibnll=0.48779446749244315\n",
      "Validation loss decreased (0.159336 --> 0.159283).  Saving model ...                                                   \n",
      "it: 153, train loss=0.16486848890781403, validation loss=0.15928299725055695, c=0.637010235952624, ibs=0.16192880151713723, ibnll=0.48775532221377566\n",
      "Validation loss decreased (0.159283 --> 0.159231).  Saving model ...                                                   \n",
      "it: 154, train loss=0.16482660174369812, validation loss=0.15923123061656952, c=0.636948387296286, ibs=0.16191234358351123, ibnll=0.48770674152134746\n",
      "Validation loss decreased (0.159231 --> 0.159181).  Saving model ...                                                   \n",
      "it: 155, train loss=0.1647852659225464, validation loss=0.15918098390102386, c=0.6371339332652998, ibs=0.16189035211827868, ibnll=0.48765041164710043\n",
      "Validation loss decreased (0.159181 --> 0.159132).  Saving model ...                                                   \n",
      "it: 156, train loss=0.16474410891532898, validation loss=0.15913203358650208, c=0.6372885549061447, ibs=0.1618643915332325, ibnll=0.48758858668885874\n",
      "Validation loss decreased (0.159132 --> 0.159084).  Saving model ...                                                   \n",
      "it: 157, train loss=0.16470295190811157, validation loss=0.15908405184745789, c=0.6372885549061447, ibs=0.16183671437748975, ibnll=0.4875245529807112\n",
      "Validation loss decreased (0.159084 --> 0.159037).  Saving model ...                                                   \n",
      "it: 158, train loss=0.1646617203950882, validation loss=0.15903665125370026, c=0.6371648575934687, ibs=0.16180976259945576, ibnll=0.48746201892895297\n",
      "Validation loss decreased (0.159037 --> 0.158989).  Saving model ...                                                   \n",
      "it: 159, train loss=0.1646202802658081, validation loss=0.15898938477039337, c=0.6372576305779757, ibs=0.16178563297546975, ibnll=0.48740415931312014\n",
      "Validation loss decreased (0.158989 --> 0.158942).  Saving model ...                                                   \n",
      "it: 160, train loss=0.16457852721214294, validation loss=0.158941850066185, c=0.6371957819216377, ibs=0.16176576752096702, ibnll=0.4873531545621503\n",
      "Validation loss decreased (0.158942 --> 0.158894).  Saving model ...                                                   \n",
      "it: 161, train loss=0.164536252617836, validation loss=0.15889368951320648, c=0.637010235952624, ibs=0.16175062482350186, ibnll=0.48730948448216677\n",
      "Validation loss decreased (0.158894 --> 0.158845).  Saving model ...                                                   \n",
      "it: 162, train loss=0.16449354588985443, validation loss=0.1588449627161026, c=0.636917462968117, ibs=0.1617399096810635, ibnll=0.4872727452566778\n",
      "Validation loss decreased (0.158845 --> 0.158796).  Saving model ...                                                   \n",
      "it: 163, train loss=0.16445063054561615, validation loss=0.15879589319229126, c=0.6368556143117791, ibs=0.16173258552079472, ibnll=0.48724123900301886\n",
      "Validation loss decreased (0.158796 --> 0.158747).  Saving model ...                                                   \n",
      "it: 164, train loss=0.1644079089164734, validation loss=0.1587466150522232, c=0.636886538639948, ibs=0.16172701179024163, ibnll=0.48721226264402423\n",
      "Validation loss decreased (0.158747 --> 0.158697).  Saving model ...                                                   \n",
      "it: 165, train loss=0.164365753531456, validation loss=0.1586974412202835, c=0.637010235952624, ibs=0.16172134347826486, ibnll=0.4871827827665128\n",
      "Validation loss decreased (0.158697 --> 0.158649).  Saving model ...                                                   \n",
      "it: 166, train loss=0.16432465612888336, validation loss=0.15864868462085724, c=0.6370720846089619, ibs=0.1617138991515922, ibnll=0.4871500085411191\n",
      "Validation loss decreased (0.158649 --> 0.158601).  Saving model ...                                                   \n",
      "it: 167, train loss=0.16428495943546295, validation loss=0.15860062837600708, c=0.6371339332652998, ibs=0.16170352470673763, ibnll=0.48711198612078505\n",
      "Validation loss decreased (0.158601 --> 0.158554).  Saving model ...                                                   \n",
      "it: 168, train loss=0.16424688696861267, validation loss=0.15855365991592407, c=0.6373194792343136, ibs=0.16168985637231081, ibnll=0.4870683312482575\n",
      "Validation loss decreased (0.158554 --> 0.158508).  Saving model ...                                                   \n",
      "it: 169, train loss=0.16421036422252655, validation loss=0.15850788354873657, c=0.6373813278906516, ibs=0.16167301396413364, ibnll=0.48701928477156836\n",
      "Validation loss decreased (0.158508 --> 0.158463).  Saving model ...                                                   \n",
      "it: 170, train loss=0.16417516767978668, validation loss=0.1584632396697998, c=0.6376596468441723, ibs=0.16165372647485254, ibnll=0.48696632518939653\n",
      "Validation loss decreased (0.158463 --> 0.158420).  Saving model ...                                                   \n",
      "it: 171, train loss=0.164140984416008, validation loss=0.15841951966285706, c=0.6376596468441723, ibs=0.16163308953181804, ibnll=0.48691134130309666\n",
      "Validation loss decreased (0.158420 --> 0.158376).  Saving model ...                                                   \n",
      "it: 172, train loss=0.16410715878009796, validation loss=0.15837620198726654, c=0.637907041469524, ibs=0.161612363588686, ibnll=0.48685650264030006\n",
      "Validation loss decreased (0.158376 --> 0.158333).  Saving model ...                                                   \n",
      "it: 173, train loss=0.16407325863838196, validation loss=0.1583329439163208, c=0.6380307387822, ibs=0.16159281721545343, ibnll=0.4868040082318562\n",
      "Validation loss decreased (0.158333 --> 0.158290).  Saving model ...                                                   \n",
      "it: 174, train loss=0.16403897106647491, validation loss=0.15828953683376312, c=0.6380307387822, ibs=0.16157526748593465, ibnll=0.48675518996301004\n",
      "Validation loss decreased (0.158290 --> 0.158246).  Saving model ...                                                   \n",
      "it: 175, train loss=0.16400426626205444, validation loss=0.15824593603610992, c=0.6380925874385379, ibs=0.16156006334311357, ibnll=0.48671064754573545\n",
      "Validation loss decreased (0.158246 --> 0.158202).  Saving model ...                                                   \n",
      "it: 176, train loss=0.1639692783355713, validation loss=0.15820220112800598, c=0.6384636793765656, ibs=0.16154706034594343, ibnll=0.48667016350980197\n",
      "Validation loss decreased (0.158202 --> 0.158158).  Saving model ...                                                   \n",
      "it: 177, train loss=0.16393420100212097, validation loss=0.15815843641757965, c=0.6387419983300863, ibs=0.1615355017736456, ibnll=0.48663249663246627\n",
      "Validation loss decreased (0.158158 --> 0.158114).  Saving model ...                                                   \n",
      "it: 178, train loss=0.16389918327331543, validation loss=0.158114492893219, c=0.6385255280329035, ibs=0.16152425503776133, ibnll=0.4865955315445807\n",
      "Validation loss decreased (0.158114 --> 0.158071).  Saving model ...                                                   \n",
      "it: 179, train loss=0.1638643890619278, validation loss=0.15807071328163147, c=0.6386492253455793, ibs=0.16151244837440426, ibnll=0.48655781892512556\n",
      "Validation loss decreased (0.158071 --> 0.158027).  Saving model ...                                                   \n",
      "it: 180, train loss=0.16382989287376404, validation loss=0.15802700817584991, c=0.6388966199709312, ibs=0.16149929207619423, ibnll=0.4865179615247267\n",
      "Validation loss decreased (0.158027 --> 0.157983).  Saving model ...                                                   \n",
      "it: 181, train loss=0.1637955605983734, validation loss=0.15798316895961761, c=0.6388347713145932, ibs=0.16148425083783857, ibnll=0.48647480168264395\n",
      "Validation loss decreased (0.157983 --> 0.157940).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 182, train loss=0.1637614369392395, validation loss=0.15793956816196442, c=0.6388966199709312, ibs=0.16146772782630622, ibnll=0.4864291243767927\n",
      "Validation loss decreased (0.157940 --> 0.157896).  Saving model ...                                                   \n",
      "it: 183, train loss=0.16372747719287872, validation loss=0.15789617598056793, c=0.6388038469864242, ibs=0.16145028630011718, ibnll=0.4863818807122172\n",
      "Validation loss decreased (0.157896 --> 0.157853).  Saving model ...                                                   \n",
      "it: 184, train loss=0.1636936068534851, validation loss=0.15785303711891174, c=0.6390821659399449, ibs=0.1614330679817255, ibnll=0.48633571559978117\n",
      "Validation loss decreased (0.157853 --> 0.157810).  Saving model ...                                                   \n",
      "it: 185, train loss=0.16365978121757507, validation loss=0.15781007707118988, c=0.639051241611776, ibs=0.1614168353589616, ibnll=0.4862917522625187\n",
      "Validation loss decreased (0.157810 --> 0.157767).  Saving model ...                                                   \n",
      "it: 186, train loss=0.16362591087818146, validation loss=0.1577671617269516, c=0.6392677119089588, ibs=0.16140209385964982, ibnll=0.4862502282950664\n",
      "Validation loss decreased (0.157767 --> 0.157724).  Saving model ...                                                   \n",
      "it: 187, train loss=0.16359207034111023, validation loss=0.15772424638271332, c=0.6392058632526209, ibs=0.16138916445050558, ibnll=0.48621161721335526\n",
      "Validation loss decreased (0.157724 --> 0.157682).  Saving model ...                                                   \n",
      "it: 188, train loss=0.16355828940868378, validation loss=0.15768161416053772, c=0.6393604848934656, ibs=0.16137816391121854, ibnll=0.4861762205900056\n",
      "Validation loss decreased (0.157682 --> 0.157639).  Saving model ...                                                   \n",
      "it: 189, train loss=0.16352471709251404, validation loss=0.15763942897319794, c=0.6397315768314933, ibs=0.16136874208499388, ibnll=0.48614344209247523\n",
      "Validation loss decreased (0.157639 --> 0.157598).  Saving model ...                                                   \n",
      "it: 190, train loss=0.1634914129972458, validation loss=0.1575978398323059, c=0.6399480471286761, ibs=0.1613601485674772, ibnll=0.4861119506488035\n",
      "Validation loss decreased (0.157598 --> 0.157557).  Saving model ...                                                   \n",
      "it: 191, train loss=0.1634586900472641, validation loss=0.15755705535411835, c=0.6402263660821969, ibs=0.16135158895297882, ibnll=0.48608040204483677\n",
      "Validation loss decreased (0.157557 --> 0.157517).  Saving model ...                                                   \n",
      "it: 192, train loss=0.1634265035390854, validation loss=0.15751709043979645, c=0.6404737607075486, ibs=0.16134214014285886, ibnll=0.48604716711190626\n",
      "Validation loss decreased (0.157517 --> 0.157478).  Saving model ...                                                   \n",
      "it: 193, train loss=0.1633949875831604, validation loss=0.1574781835079193, c=0.6405665336920555, ibs=0.1613311934338869, ibnll=0.48601125360575675\n",
      "Validation loss decreased (0.157478 --> 0.157440).  Saving model ...                                                   \n",
      "it: 194, train loss=0.16336406767368317, validation loss=0.15744048357009888, c=0.6407520796610694, ibs=0.161318573120313, ibnll=0.4859725533867494\n",
      "Validation loss decreased (0.157440 --> 0.157404).  Saving model ...                                                   \n",
      "it: 195, train loss=0.1633337140083313, validation loss=0.1574038863182068, c=0.6406593066765625, ibs=0.16130446480614022, ibnll=0.4859314189132626\n",
      "Validation loss decreased (0.157404 --> 0.157369).  Saving model ...                                                   \n",
      "it: 196, train loss=0.16330376267433167, validation loss=0.15736857056617737, c=0.6402263660821969, ibs=0.16128957148459655, ibnll=0.48588927755593153\n",
      "Validation loss decreased (0.157369 --> 0.157334).  Saving model ...                                                   \n",
      "it: 197, train loss=0.16327418386936188, validation loss=0.15733441710472107, c=0.6401026687695209, ibs=0.16127464014137347, ibnll=0.4858475653860247\n",
      "Validation loss decreased (0.157334 --> 0.157301).  Saving model ...                                                   \n",
      "it: 198, train loss=0.16324494779109955, validation loss=0.15730123221874237, c=0.6400408201131831, ibs=0.16126034672531062, ibnll=0.48580747334352237\n",
      "Validation loss decreased (0.157301 --> 0.157269).  Saving model ...                                                   \n",
      "it: 199, train loss=0.16321603953838348, validation loss=0.1572689414024353, c=0.6399171228005072, ibs=0.1612472322330825, ibnll=0.4857699624368945\n",
      "                                                                                                                       \n",
      "Results: 0.5976715906145964, 0.17936274143064143, 0.5285761223584085\n",
      "{'batch_size': 128, 'encoder_dropout': 0.0, 'encoder_neurons': 275, 'lr': 0.0005, 'mu': 0.0001, 'multiplier': 1.0, 'num_encoder_layers': 4, 'num_latent': 155, 'num_odefunc_layers1': 4, 'odefunc_neurons1': 790, 'patience': 20, 'scheduler_epoch': 5, 'scheduler_gamma': 0.1, 'softplus_beta': 0.1, 'weight_decay': 0.0001}\n",
      "  8%|                                   | 8/100 [12:21:11<149:55:01, 5866.32s/trial, best loss: 0.13281087577342987]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cba609bdb584daf8ecd9c8055a3e59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.396619).  Saving model ...                                                        \n",
      "it: 0, train loss=0.43976372480392456, validation loss=0.396618515253067, c=0.5666913443148868, ibs=0.27651250300533387, ibnll=0.9736606082173378\n",
      "Validation loss decreased (0.396619 --> 0.387305).  Saving model ...                                                   \n",
      "it: 1, train loss=0.42959922552108765, validation loss=0.3873048722743988, c=0.5675859090628663, ibs=0.27430296005034693, ibnll=0.9571125897169593\n",
      "Validation loss decreased (0.387305 --> 0.374410).  Saving model ...                                                   \n",
      "it: 2, train loss=0.41960060596466064, validation loss=0.37441006302833557, c=0.6021963106915911, ibs=0.27105456340179407, ibnll=0.9338510640541148\n",
      "Validation loss decreased (0.374410 --> 0.361035).  Saving model ...                                                   \n",
      "it: 3, train loss=0.40586796402931213, validation loss=0.36103519797325134, c=0.5136344006416188, ibs=0.26757797693379254, ibnll=0.9096670772235398\n",
      "Validation loss decreased (0.361035 --> 0.348373).  Saving model ...                                                   \n",
      "it: 4, train loss=0.3913815915584564, validation loss=0.348373144865036, c=0.5918933925596891, ibs=0.26412288706267484, ibnll=0.8863761080965652\n",
      "Validation loss decreased (0.348373 --> 0.335508).  Saving model ...                                                   \n",
      "it: 5, train loss=0.37761586904525757, validation loss=0.33550792932510376, c=0.5894256277376766, ibs=0.2604211488125838, ibnll=0.8624725168834906\n",
      "Validation loss decreased (0.335508 --> 0.323470).  Saving model ...                                                   \n",
      "it: 6, train loss=0.3636314868927002, validation loss=0.32347017526626587, c=0.5078968474304398, ibs=0.2567482580967262, ibnll=0.8398424672499111\n",
      "Validation loss decreased (0.323470 --> 0.312190).  Saving model ...                                                   \n",
      "it: 7, train loss=0.35045701265335083, validation loss=0.3121902644634247, c=0.5595039792707754, ibs=0.2531347090342425, ibnll=0.8185488546389745\n",
      "Validation loss decreased (0.312190 --> 0.301415).  Saving model ...                                                   \n",
      "it: 8, train loss=0.33814069628715515, validation loss=0.30141451954841614, c=0.5686347091122216, ibs=0.24951457046917752, ibnll=0.7981437351192936\n",
      "Validation loss decreased (0.301415 --> 0.291091).  Saving model ...                                                   \n",
      "it: 9, train loss=0.3264150023460388, validation loss=0.29109126329421997, c=0.5843358627922759, ibs=0.24589119200562057, ibnll=0.7785693005433878\n",
      "Validation loss decreased (0.291091 --> 0.281232).  Saving model ...                                                   \n",
      "it: 10, train loss=0.3152589797973633, validation loss=0.2812317907810211, c=0.582361650934666, ibs=0.2422777412304869, ibnll=0.75981945837504\n",
      "Validation loss decreased (0.281232 --> 0.271821).  Saving model ...                                                   \n",
      "it: 11, train loss=0.3046517074108124, validation loss=0.2718208134174347, c=0.5911222160528101, ibs=0.23867359441734812, ibnll=0.7418416701123243\n",
      "Validation loss decreased (0.271821 --> 0.262856).  Saving model ...                                                   \n",
      "it: 12, train loss=0.29452046751976013, validation loss=0.2628558576107025, c=0.6083965698068974, ibs=0.23508352866432036, ibnll=0.7246215786141463\n",
      "Validation loss decreased (0.262856 --> 0.254358).  Saving model ...                                                   \n",
      "it: 13, train loss=0.2848319411277771, validation loss=0.2543577253818512, c=0.6212906410019126, ibs=0.23152018337825717, ibnll=0.7081767728892288\n",
      "Validation loss decreased (0.254358 --> 0.246411).  Saving model ...                                                   \n",
      "it: 14, train loss=0.2756122350692749, validation loss=0.2464105188846588, c=0.6249922882349312, ibs=0.22799138741412273, ibnll=0.6925020739716383\n",
      "Validation loss decreased (0.246411 --> 0.239074).  Saving model ...                                                   \n",
      "it: 15, train loss=0.26695576310157776, validation loss=0.2390744835138321, c=0.6144734406811031, ibs=0.22451011622277126, ibnll=0.6776084854914246\n",
      "Validation loss decreased (0.239074 --> 0.232181).  Saving model ...                                                   \n",
      "it: 16, train loss=0.2588582932949066, validation loss=0.2321814000606537, c=0.6149052995249553, ibs=0.2210874953178577, ibnll=0.6634953847332485\n",
      "Validation loss decreased (0.232181 --> 0.225709).  Saving model ...                                                   \n",
      "it: 17, train loss=0.25122836232185364, validation loss=0.2257092148065567, c=0.621969276327966, ibs=0.217734995938953, ibnll=0.6501623504813373\n",
      "Validation loss decreased (0.225709 --> 0.219664).  Saving model ...                                                   \n",
      "it: 18, train loss=0.24404965341091156, validation loss=0.21966437995433807, c=0.6071626873958912, ibs=0.21446432935478213, ibnll=0.6376069231208981\n",
      "Validation loss decreased (0.219664 --> 0.214049).  Saving model ...                                                   \n",
      "it: 19, train loss=0.23732520639896393, validation loss=0.2140493392944336, c=0.5864334628909865, ibs=0.21128525598688874, ibnll=0.6258180694662757\n",
      "Validation loss decreased (0.214049 --> 0.208859).  Saving model ...                                                   \n",
      "it: 20, train loss=0.23105573654174805, validation loss=0.20885895192623138, c=0.5824233450552162, ibs=0.20820638781205336, ibnll=0.6147801468614985\n",
      "Validation loss decreased (0.208859 --> 0.204099).  Saving model ...                                                   \n",
      "it: 21, train loss=0.22523537278175354, validation loss=0.204098641872406, c=0.5864334628909865, ibs=0.2052357743435849, ibnll=0.60447546859045\n",
      "Validation loss decreased (0.204099 --> 0.199759).  Saving model ...                                                   \n",
      "it: 22, train loss=0.219869464635849, validation loss=0.19975894689559937, c=0.598432969338022, ibs=0.2023809113987985, ibnll=0.5948850456286884\n",
      "Validation loss decreased (0.199759 --> 0.195785).  Saving model ...                                                   \n",
      "it: 23, train loss=0.21494802832603455, validation loss=0.19578534364700317, c=0.5989573693626997, ibs=0.1996480926766474, ibnll=0.5859869092411285\n",
      "Validation loss decreased (0.195785 --> 0.192148).  Saving model ...                                                   \n",
      "it: 24, train loss=0.2104165405035019, validation loss=0.19214805960655212, c=0.6038928990067246, ibs=0.19704246996904004, ibnll=0.5777567980080442\n",
      "Validation loss decreased (0.192148 --> 0.188839).  Saving model ...                                                   \n",
      "it: 25, train loss=0.20624464750289917, validation loss=0.18883857131004333, c=0.6135171818125733, ibs=0.19456823432849507, ibnll=0.5701691697831328\n",
      "Validation loss decreased (0.188839 --> 0.185853).  Saving model ...                                                   \n",
      "it: 26, train loss=0.20242388546466827, validation loss=0.18585343658924103, c=0.6139798877167006, ibs=0.19222834601727085, ibnll=0.5631966044962793\n",
      "Validation loss decreased (0.185853 --> 0.183187).  Saving model ...                                                   \n",
      "it: 27, train loss=0.19895043969154358, validation loss=0.18318650126457214, c=0.6160157936948609, ibs=0.1900243157863572, ibnll=0.5568096125761494\n",
      "Validation loss decreased (0.183187 --> 0.180809).  Saving model ...                                                   \n",
      "it: 28, train loss=0.19581784307956696, validation loss=0.18080881237983704, c=0.6259177000431859, ibs=0.18795675136520912, ibnll=0.5509784333401259\n",
      "Validation loss decreased (0.180809 --> 0.178687).  Saving model ...                                                   \n",
      "it: 29, train loss=0.19299666583538055, validation loss=0.1786871701478958, c=0.6288173237090505, ibs=0.1860251380237959, ibnll=0.5456727004251605\n",
      "Validation loss decreased (0.178687 --> 0.176805).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 30, train loss=0.19045336544513702, validation loss=0.17680463194847107, c=0.6308532296872108, ibs=0.18422778455007277, ibnll=0.5408612971417154\n",
      "Validation loss decreased (0.176805 --> 0.175138).  Saving model ...                                                   \n",
      "it: 31, train loss=0.18817032873630524, validation loss=0.17513830959796906, c=0.6303288296625331, ibs=0.18256203359222423, ibnll=0.5365129869037455\n",
      "Validation loss decreased (0.175138 --> 0.173683).  Saving model ...                                                   \n",
      "it: 32, train loss=0.18612416088581085, validation loss=0.1736830621957779, c=0.6298044296378555, ibs=0.18102446320845242, ibnll=0.5325968842076642\n",
      "Validation loss decreased (0.173683 --> 0.172434).  Saving model ...                                                   \n",
      "it: 33, train loss=0.18430913984775543, validation loss=0.17243418097496033, c=0.6240668764266766, ibs=0.17961078299848363, ibnll=0.5290823258516967\n",
      "Validation loss decreased (0.172434 --> 0.171352).  Saving model ...                                                   \n",
      "it: 34, train loss=0.18271996080875397, validation loss=0.17135201394557953, c=0.6228329940156703, ibs=0.17831606118187704, ibnll=0.5259392515986202\n",
      "Validation loss decreased (0.171352 --> 0.170417).  Saving model ...                                                   \n",
      "it: 35, train loss=0.1813165694475174, validation loss=0.1704174131155014, c=0.6228021469553952, ibs=0.1771348923293681, ibnll=0.5231384205477893\n",
      "Validation loss decreased (0.170417 --> 0.169623).  Saving model ...                                                   \n",
      "it: 36, train loss=0.18007908761501312, validation loss=0.16962333023548126, c=0.6188845703004504, ibs=0.1760618358870563, ibnll=0.5206520239016531\n",
      "Validation loss decreased (0.169623 --> 0.168956).  Saving model ...                                                   \n",
      "it: 37, train loss=0.17900052666664124, validation loss=0.1689557582139969, c=0.6177123820099945, ibs=0.17509035843459891, ibnll=0.518452608902467\n",
      "Validation loss decreased (0.168956 --> 0.168413).  Saving model ...                                                   \n",
      "it: 38, train loss=0.17806559801101685, validation loss=0.1684134155511856, c=0.6189771114812759, ibs=0.1742138744268775, ibnll=0.516514150881332\n",
      "Validation loss decreased (0.168413 --> 0.167962).  Saving model ...                                                   \n",
      "it: 39, train loss=0.1772720366716385, validation loss=0.16796153783798218, c=0.6194398173854032, ibs=0.17342585148588105, ibnll=0.5148119994103058\n",
      "Validation loss decreased (0.167962 --> 0.167588).  Saving model ...                                                   \n",
      "it: 40, train loss=0.1765846610069275, validation loss=0.16758787631988525, c=0.6170337466839411, ibs=0.17272032052288774, ibnll=0.5133212565636505\n",
      "Validation loss decreased (0.167588 --> 0.167288).  Saving model ...                                                   \n",
      "it: 41, train loss=0.17599233984947205, validation loss=0.16728800535202026, c=0.6116355111357887, ibs=0.17209332330872557, ibnll=0.5120238385236524\n",
      "Validation loss decreased (0.167288 --> 0.167059).  Saving model ...                                                   \n",
      "it: 42, train loss=0.1754905879497528, validation loss=0.16705870628356934, c=0.6099080757603801, ibs=0.17153261628507743, ibnll=0.5108896500405286\n",
      "Validation loss decreased (0.167059 --> 0.166877).  Saving model ...                                                   \n",
      "it: 43, train loss=0.17507323622703552, validation loss=0.16687673330307007, c=0.6096612992781788, ibs=0.17103122620457537, ibnll=0.509899195475611\n",
      "Validation loss decreased (0.166877 --> 0.166703).  Saving model ...                                                   \n",
      "it: 44, train loss=0.17471662163734436, validation loss=0.1667025238275528, c=0.6079955580233204, ibs=0.17059505655387952, ibnll=0.5090291981157289\n",
      "Validation loss decreased (0.166703 --> 0.166554).  Saving model ...                                                   \n",
      "it: 45, train loss=0.1743958294391632, validation loss=0.166553795337677, c=0.6058671108643346, ibs=0.17017775118341433, ibnll=0.5082220282081753\n",
      "Validation loss decreased (0.166554 --> 0.166342).  Saving model ...                                                   \n",
      "it: 46, train loss=0.1741037368774414, validation loss=0.16634199023246765, c=0.5994200752668271, ibs=0.16998689739240883, ibnll=0.5077072818447667\n",
      "Validation loss decreased (0.166342 --> 0.166065).  Saving model ...                                                   \n",
      "it: 47, train loss=0.17387035489082336, validation loss=0.16606451570987701, c=0.6032759578012216, ibs=0.16944667879650538, ibnll=0.506659406267351\n",
      "Validation loss decreased (0.166065 --> 0.165604).  Saving model ...                                                   \n",
      "it: 48, train loss=0.17345866560935974, validation loss=0.16560423374176025, c=0.5987105928804984, ibs=0.1695825010371244, ibnll=0.5064989319504629\n",
      "Validation loss decreased (0.165604 --> 0.165342).  Saving model ...                                                   \n",
      "it: 49, train loss=0.17318503558635712, validation loss=0.16534173488616943, c=0.6006848047381085, ibs=0.16940344746702224, ibnll=0.506031098803714\n",
      "Validation loss decreased (0.165342 --> 0.163860).  Saving model ...                                                   \n",
      "it: 50, train loss=0.17295867204666138, validation loss=0.1638600081205368, c=0.6007464988586587, ibs=0.16942342138314512, ibnll=0.5048334814761161\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 51, train loss=0.17210227251052856, validation loss=0.16662700474262238, c=0.616231723116787, ibs=0.16905031208288357, ibnll=0.5063368125335724\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 52, train loss=0.1736326813697815, validation loss=0.16677254438400269, c=0.6106484052069838, ibs=0.16883473132467194, ibnll=0.5060541068009444\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 53, train loss=0.1736443191766739, validation loss=0.16662293672561646, c=0.6161083348756864, ibs=0.16859989636148773, ibnll=0.5055707443559366\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 54, train loss=0.1734602451324463, validation loss=0.16578809916973114, c=0.6094145227959775, ibs=0.16849546901265874, ibnll=0.5048389081797999\n",
      "EarlyStopping counter: 5 out of 20                                                                                     \n",
      "it: 55, train loss=0.1728302240371704, validation loss=0.16482721269130707, c=0.6104633228453329, ibs=0.1687674571237864, ibnll=0.5046106124488839\n",
      "EarlyStopping counter: 6 out of 20                                                                                     \n",
      "it: 56, train loss=0.17229357361793518, validation loss=0.16496555507183075, c=0.6167561231414647, ibs=0.16812367902761974, ibnll=0.5037753982594374\n",
      "EarlyStopping counter: 7 out of 20                                                                                     \n",
      "it: 57, train loss=0.17220330238342285, validation loss=0.16452164947986603, c=0.6166944290209143, ibs=0.16793997993189666, ibnll=0.5031740918055505\n",
      "Validation loss decreased (0.163860 --> 0.163656).  Saving model ...                                                   \n",
      "it: 58, train loss=0.1718318909406662, validation loss=0.16365566849708557, c=0.6165401937195385, ibs=0.16765905947476376, ibnll=0.5020924057794303\n",
      "Validation loss decreased (0.163656 --> 0.163104).  Saving model ...                                                   \n",
      "it: 59, train loss=0.17119275033473969, validation loss=0.16310440003871918, c=0.6130236288481707, ibs=0.1677170138546367, ibnll=0.5017044887596539\n",
      "Validation loss decreased (0.163104 --> 0.162746).  Saving model ...                                                   \n",
      "it: 60, train loss=0.17078417539596558, validation loss=0.16274599730968475, c=0.6116355111357887, ibs=0.16759734360370743, ibnll=0.501291949544873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.162746 --> 0.162468).  Saving model ...                                                   \n",
      "it: 61, train loss=0.170539990067482, validation loss=0.16246813535690308, c=0.6110802640508359, ibs=0.16728499509017702, ibnll=0.50067520782583\n",
      "Validation loss decreased (0.162468 --> 0.162128).  Saving model ...                                                   \n",
      "it: 62, train loss=0.17023339867591858, validation loss=0.16212797164916992, c=0.6135788759331235, ibs=0.16699392399023527, ibnll=0.5000208752420051\n",
      "Validation loss decreased (0.162128 --> 0.161508).  Saving model ...                                                   \n",
      "it: 63, train loss=0.16988718509674072, validation loss=0.16150790452957153, c=0.6126226170645938, ibs=0.16671177812903776, ibnll=0.49912727231688964\n",
      "Validation loss decreased (0.161508 --> 0.160763).  Saving model ...                                                   \n",
      "it: 64, train loss=0.169437438249588, validation loss=0.16076251864433289, c=0.6131778641495466, ibs=0.16652002569112642, ibnll=0.4981893477585417\n",
      "Validation loss decreased (0.160763 --> 0.160253).  Saving model ...                                                   \n",
      "it: 65, train loss=0.16898848116397858, validation loss=0.16025295853614807, c=0.6155530877907336, ibs=0.16632927062919767, ibnll=0.49742859619380264\n",
      "Validation loss decreased (0.160253 --> 0.159905).  Saving model ...                                                   \n",
      "it: 66, train loss=0.1686411201953888, validation loss=0.15990473330020905, c=0.6183601702757727, ibs=0.1658777497079331, ibnll=0.4965390989066985\n",
      "Validation loss decreased (0.159905 --> 0.159623).  Saving model ...                                                   \n",
      "it: 67, train loss=0.1682710200548172, validation loss=0.15962320566177368, c=0.6191621938429268, ibs=0.16556022523766545, ibnll=0.4958841661754029\n",
      "Validation loss decreased (0.159623 --> 0.159054).  Saving model ...                                                   \n",
      "it: 68, train loss=0.1679736226797104, validation loss=0.1590539813041687, c=0.6177432290702696, ibs=0.16543938737647576, ibnll=0.49516659035175997\n",
      "Validation loss decreased (0.159054 --> 0.158557).  Saving model ...                                                   \n",
      "it: 69, train loss=0.16759967803955078, validation loss=0.15855705738067627, c=0.6169412055031156, ibs=0.16532182398736667, ibnll=0.49451391967087754\n",
      "Validation loss decreased (0.158557 --> 0.158292).  Saving model ...                                                   \n",
      "it: 70, train loss=0.16725732386112213, validation loss=0.15829189121723175, c=0.6182676290949473, ibs=0.16497551789226053, ibnll=0.4938120360375618\n",
      "Validation loss decreased (0.158292 --> 0.158025).  Saving model ...                                                   \n",
      "it: 71, train loss=0.16689816117286682, validation loss=0.158025324344635, c=0.6183601702757727, ibs=0.16471790480170237, ibnll=0.4931549240290247\n",
      "Validation loss decreased (0.158025 --> 0.157467).  Saving model ...                                                   \n",
      "it: 72, train loss=0.16658611595630646, validation loss=0.1574673354625702, c=0.6188537232401752, ibs=0.1645369969481132, ibnll=0.49227835044729534\n",
      "Validation loss decreased (0.157467 --> 0.157041).  Saving model ...                                                   \n",
      "it: 73, train loss=0.16623488068580627, validation loss=0.1570410281419754, c=0.6195323585662287, ibs=0.16424739827961105, ibnll=0.4914639291940285\n",
      "Validation loss decreased (0.157041 --> 0.156787).  Saving model ...                                                   \n",
      "it: 74, train loss=0.1659068614244461, validation loss=0.15678702294826508, c=0.6202418409525572, ibs=0.16395691001756452, ibnll=0.49083499834858363\n",
      "Validation loss decreased (0.156787 --> 0.156374).  Saving model ...                                                   \n",
      "it: 75, train loss=0.16560043394565582, validation loss=0.1563739776611328, c=0.6207662409772349, ibs=0.16381071835525113, ibnll=0.4901672373956528\n",
      "Validation loss decreased (0.156374 --> 0.156057).  Saving model ...                                                   \n",
      "it: 76, train loss=0.16526085138320923, validation loss=0.15605689585208893, c=0.6213523351224628, ibs=0.1636654800119523, ibnll=0.48960090794962813\n",
      "Validation loss decreased (0.156057 --> 0.155937).  Saving model ...                                                   \n",
      "it: 77, train loss=0.1649152785539627, validation loss=0.15593722462654114, c=0.6208279350977852, ibs=0.16347646832941035, ibnll=0.48917930123839043\n",
      "Validation loss decreased (0.155937 --> 0.155575).  Saving model ...                                                   \n",
      "it: 78, train loss=0.16456887125968933, validation loss=0.15557534992694855, c=0.6200567585909063, ibs=0.16334809389621832, ibnll=0.4885625121248713\n",
      "Validation loss decreased (0.155575 --> 0.155265).  Saving model ...                                                   \n",
      "it: 79, train loss=0.1642305701971054, validation loss=0.15526504814624786, c=0.6197174409278796, ibs=0.16320456266011002, ibnll=0.4879619124301011\n",
      "Validation loss decreased (0.155265 --> 0.155170).  Saving model ...                                                   \n",
      "it: 80, train loss=0.16389942169189453, validation loss=0.1551704853773117, c=0.6210130174594362, ibs=0.16299395740654796, ibnll=0.4874832789992082\n",
      "Validation loss decreased (0.155170 --> 0.154687).  Saving model ...                                                   \n",
      "it: 81, train loss=0.16355104744434357, validation loss=0.15468734502792358, c=0.6217224998457647, ibs=0.16284165361203107, ibnll=0.4867026033734231\n",
      "Validation loss decreased (0.154687 --> 0.154620).  Saving model ...                                                   \n",
      "it: 82, train loss=0.16316723823547363, validation loss=0.15461960434913635, c=0.6230180763773212, ibs=0.16267089913310315, ibnll=0.48628150668005155\n",
      "Validation loss decreased (0.154620 --> 0.154364).  Saving model ...                                                   \n",
      "it: 83, train loss=0.16277168691158295, validation loss=0.154364213347435, c=0.6247763588130051, ibs=0.16258996936497846, ibnll=0.4857919058499925\n",
      "Validation loss decreased (0.154364 --> 0.154065).  Saving model ...                                                   \n",
      "it: 84, train loss=0.16240257024765015, validation loss=0.15406544506549835, c=0.6254241470787834, ibs=0.16247223702327773, ibnll=0.4852019419004318\n",
      "Validation loss decreased (0.154065 --> 0.153989).  Saving model ...                                                   \n",
      "it: 85, train loss=0.16203179955482483, validation loss=0.15398892760276794, c=0.626133629465112, ibs=0.16226174284458716, ibnll=0.48465040287583133\n",
      "Validation loss decreased (0.153989 --> 0.153232).  Saving model ...                                                   \n",
      "it: 86, train loss=0.16163653135299683, validation loss=0.1532316952943802, c=0.6247763588130051, ibs=0.16220444234505774, ibnll=0.48371838668575323\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 87, train loss=0.16134513914585114, validation loss=0.15564745664596558, c=0.6260410882842865, ibs=0.16231245958605067, ibnll=0.48567788832168723\n",
      "Validation loss decreased (0.153232 --> 0.153093).  Saving model ...                                                   \n",
      "it: 88, train loss=0.16177505254745483, validation loss=0.1530926376581192, c=0.6224628292923684, ibs=0.16334904816847368, ibnll=0.4847228338168409\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 89, train loss=0.1626199185848236, validation loss=0.15412868559360504, c=0.6253624529582331, ibs=0.1618592253984786, ibnll=0.4834028416360727\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 90, train loss=0.16051633656024933, validation loss=0.1554807871580124, c=0.6279536060213462, ibs=0.16203528888976765, ibnll=0.484603639290245\n",
      "Validation loss decreased (0.153093 --> 0.152584).  Saving model ...                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 91, train loss=0.16110706329345703, validation loss=0.1525839865207672, c=0.6254858411993337, ibs=0.1618250214679401, ibnll=0.48165837992958016\n",
      "Validation loss decreased (0.152584 --> 0.152295).  Saving model ...                                                   \n",
      "it: 92, train loss=0.16002731025218964, validation loss=0.15229454636573792, c=0.62474551175273, ibs=0.16196401004573946, ibnll=0.4814552568918095\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 93, train loss=0.16016116738319397, validation loss=0.15381596982479095, c=0.6282929236843728, ibs=0.1614447836039798, ibnll=0.4819950715039992\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 94, train loss=0.15968750417232513, validation loss=0.1536223292350769, c=0.6280153001418964, ibs=0.16132549463798768, ibnll=0.4815453871816267\n",
      "Validation loss decreased (0.152295 --> 0.151935).  Saving model ...                                                   \n",
      "it: 95, train loss=0.15943947434425354, validation loss=0.15193504095077515, c=0.6265037941884138, ibs=0.16150421881081492, ibnll=0.4801068873154977\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 96, train loss=0.1592288762331009, validation loss=0.15195223689079285, c=0.6272749706952927, ibs=0.16124203124228292, ibnll=0.4796241929192122\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 97, train loss=0.15876200795173645, validation loss=0.1533484011888504, c=0.6283854648651983, ibs=0.16105540569628118, ibnll=0.48057968408954105\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 98, train loss=0.15883557498455048, validation loss=0.15249763429164886, c=0.6272132765747425, ibs=0.16091289503266035, ibnll=0.4795319814139903\n",
      "Validation loss decreased (0.151935 --> 0.151425).  Saving model ...                                                   \n",
      "it: 99, train loss=0.158249631524086, validation loss=0.15142536163330078, c=0.6272749706952927, ibs=0.16124537182368878, ibnll=0.4789005957991117\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 100, train loss=0.1584414839744568, validation loss=0.15167701244354248, c=0.6272441236350176, ibs=0.1608367913094145, ibnll=0.47857198559083086\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 101, train loss=0.15774236619472504, validation loss=0.15261995792388916, c=0.6278302177802455, ibs=0.16076146694669183, ibnll=0.4793031091263557\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 102, train loss=0.15793365240097046, validation loss=0.1514311134815216, c=0.6277993707199704, ibs=0.16062616438274158, ibnll=0.4779623185273268\n",
      "Validation loss decreased (0.151425 --> 0.150817).  Saving model ...                                                   \n",
      "it: 103, train loss=0.15732231736183167, validation loss=0.1508171409368515, c=0.6272749706952927, ibs=0.16079675883910458, ibnll=0.47749789441114976\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 104, train loss=0.15749967098236084, validation loss=0.15138348937034607, c=0.6291874884323524, ibs=0.1603837627767754, ibnll=0.4774906856564641\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 105, train loss=0.15696963667869568, validation loss=0.151668980717659, c=0.6291874884323524, ibs=0.16031297308978776, ibnll=0.47760840949798516\n",
      "Validation loss decreased (0.150817 --> 0.150446).  Saving model ...                                                   \n",
      "it: 106, train loss=0.1569548100233078, validation loss=0.15044601261615753, c=0.6283854648651983, ibs=0.1603398289269407, ibnll=0.47643453458338\n",
      "Validation loss decreased (0.150446 --> 0.150337).  Saving model ...                                                   \n",
      "it: 107, train loss=0.15664806962013245, validation loss=0.15033674240112305, c=0.6282312295638226, ibs=0.16025350011311576, ibnll=0.47619795963414807\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 108, train loss=0.15643273293972015, validation loss=0.15122190117835999, c=0.6295576531556543, ibs=0.16011889235432789, ibnll=0.47681967297011874\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 109, train loss=0.1563686579465866, validation loss=0.15053123235702515, c=0.6295268060953791, ibs=0.16005935619381137, ibnll=0.47608572561205276\n",
      "Validation loss decreased (0.150337 --> 0.149854).  Saving model ...                                                   \n",
      "it: 110, train loss=0.15599988400936127, validation loss=0.14985395967960358, c=0.6292800296131779, ibs=0.16019288770638956, ibnll=0.47559537950141695\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 111, train loss=0.15604576468467712, validation loss=0.15049000084400177, c=0.6287556295885002, ibs=0.15996491062106932, ibnll=0.4758835385103479\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 112, train loss=0.15568970143795013, validation loss=0.15056054294109344, c=0.6291874884323524, ibs=0.15993169301584628, ibnll=0.4758764388442191\n",
      "Validation loss decreased (0.149854 --> 0.149610).  Saving model ...                                                   \n",
      "it: 113, train loss=0.1555749475955963, validation loss=0.14960983395576477, c=0.6299278178789561, ibs=0.1600228216243877, ibnll=0.4750928221919955\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 114, train loss=0.15544484555721283, validation loss=0.1498330533504486, c=0.6300203590597816, ibs=0.15987045023483032, ibnll=0.47506650089732927\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 115, train loss=0.15515127778053284, validation loss=0.15031906962394714, c=0.6287247825282251, ibs=0.15978521881619392, ibnll=0.4753332129102101\n",
      "Validation loss decreased (0.149610 --> 0.149473).  Saving model ...                                                   \n",
      "it: 116, train loss=0.15510427951812744, validation loss=0.1494728922843933, c=0.6295268060953791, ibs=0.15977267359165978, ibnll=0.47449340710534016\n",
      "Validation loss decreased (0.149473 --> 0.149392).  Saving model ...                                                   \n",
      "it: 117, train loss=0.15485596656799316, validation loss=0.14939245581626892, c=0.629310876673453, ibs=0.15970633634308606, ibnll=0.4742725782694391\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 118, train loss=0.1546880006790161, validation loss=0.15002186596393585, c=0.6294651119748288, ibs=0.15959916211299235, ibnll=0.4746437727573712\n",
      "Validation loss decreased (0.149392 --> 0.149360).  Saving model ...                                                   \n",
      "it: 119, train loss=0.1545768827199936, validation loss=0.14935986697673798, c=0.6292800296131779, ibs=0.15959555306548265, ibnll=0.47402499262868997\n",
      "Validation loss decreased (0.149360 --> 0.149099).  Saving model ...                                                   \n",
      "it: 120, train loss=0.15430690348148346, validation loss=0.14909903705120087, c=0.6292491825529027, ibs=0.159618433387152, ibnll=0.47382056337779166\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 121, train loss=0.15417367219924927, validation loss=0.14972524344921112, c=0.6277068295391449, ibs=0.1595242786618832, ibnll=0.4742511024992986\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 122, train loss=0.1540268510580063, validation loss=0.14912784099578857, c=0.6282003825035474, ibs=0.15953965774124437, ibnll=0.47374092574184623\n",
      "Validation loss decreased (0.149099 --> 0.148885).  Saving model ...                                                   \n",
      "it: 123, train loss=0.15377511084079742, validation loss=0.14888527989387512, c=0.6282312295638226, ibs=0.1595425126165235, ibnll=0.4735164909614056\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 124, train loss=0.15363933145999908, validation loss=0.1494952142238617, c=0.6282929236843728, ibs=0.1594289276465864, ibnll=0.4738722307427349\n",
      "Validation loss decreased (0.148885 --> 0.148820).  Saving model ...                                                   \n",
      "it: 125, train loss=0.1535041779279709, validation loss=0.1488202065229416, c=0.6283854648651983, ibs=0.15945045888828482, ibnll=0.47328848549767466\n",
      "Validation loss decreased (0.148820 --> 0.148790).  Saving model ...                                                   \n",
      "it: 126, train loss=0.15325745940208435, validation loss=0.14878956973552704, c=0.6287864766487754, ibs=0.15943414275479564, ibnll=0.4732293298183007\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 127, train loss=0.15308348834514618, validation loss=0.14923781156539917, c=0.6283854648651983, ibs=0.15938619426257164, ibnll=0.47355947597860554\n",
      "Validation loss decreased (0.148790 --> 0.148496).  Saving model ...                                                   \n",
      "it: 128, train loss=0.15298564732074738, validation loss=0.14849631488323212, c=0.6278919119007959, ibs=0.15945218762282895, ibnll=0.473002887846228\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 129, train loss=0.15282019972801208, validation loss=0.14886868000030518, c=0.6263187118267629, ibs=0.15932164193211426, ibnll=0.4731382548942587\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 130, train loss=0.1525973379611969, validation loss=0.14867258071899414, c=0.6265037941884138, ibs=0.15927596718555925, ibnll=0.4728925201706393\n",
      "Validation loss decreased (0.148496 --> 0.148286).  Saving model ...                                                   \n",
      "it: 131, train loss=0.1524147093296051, validation loss=0.14828604459762573, c=0.6274600530569436, ibs=0.15928012675343986, ibnll=0.47255007813671646\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 132, train loss=0.15228231251239777, validation loss=0.1488325446844101, c=0.6267197236103399, ibs=0.15916306679632305, ibnll=0.47282559604195\n",
      "Validation loss decreased (0.148286 --> 0.148109).  Saving model ...                                                   \n",
      "it: 133, train loss=0.15213926136493683, validation loss=0.14810898900032043, c=0.6272132765747425, ibs=0.1592351573469256, ibnll=0.47230569209901546\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 134, train loss=0.1519569754600525, validation loss=0.14854483306407928, c=0.6268122647911654, ibs=0.15914387242543077, ibnll=0.47253385812150334\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 135, train loss=0.15175840258598328, validation loss=0.14818288385868073, c=0.6267505706706151, ibs=0.1591782199293359, ibnll=0.4722887929183784\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 136, train loss=0.15157489478588104, validation loss=0.14813151955604553, c=0.6270590412733666, ibs=0.15915191830395845, ibnll=0.47221515219790094\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 137, train loss=0.1514102667570114, validation loss=0.1482633501291275, c=0.6269356530322661, ibs=0.15907363512477943, ibnll=0.4722075020215395\n",
      "Validation loss decreased (0.148109 --> 0.147742).  Saving model ...                                                   \n",
      "it: 138, train loss=0.15125998854637146, validation loss=0.14774174988269806, c=0.6268431118514406, ibs=0.15909402076253784, ibnll=0.47180736122792927\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 139, train loss=0.15113024413585663, validation loss=0.14842350780963898, c=0.625547535319884, ibs=0.15894233586010414, ibnll=0.4721355006747783\n",
      "Validation loss decreased (0.147742 --> 0.147344).  Saving model ...                                                   \n",
      "it: 140, train loss=0.15103089809417725, validation loss=0.14734366536140442, c=0.6269048059719908, ibs=0.15911700984186145, ibnll=0.4715242320009262\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 141, train loss=0.15098975598812103, validation loss=0.14900554716587067, c=0.6240668764266766, ibs=0.1589276485284578, ibnll=0.4725922118095076\n",
      "Validation loss decreased (0.147344 --> 0.147096).  Saving model ...                                                   \n",
      "it: 142, train loss=0.15107126533985138, validation loss=0.1470956951379776, c=0.6267197236103399, ibs=0.1592556570717522, ibnll=0.4715489811067978\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 143, train loss=0.1509956270456314, validation loss=0.14899273216724396, c=0.6232340057992474, ibs=0.15890465813336563, ibnll=0.47254008808144915\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 144, train loss=0.1508149951696396, validation loss=0.14724870026111603, c=0.6266888765500648, ibs=0.15899942003703812, ibnll=0.47126656085608637\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 145, train loss=0.15025034546852112, validation loss=0.14743497967720032, c=0.6257634647418101, ibs=0.15889141030217913, ibnll=0.4712566821087913\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 146, train loss=0.1499774008989334, validation loss=0.1482892781496048, c=0.623573323462274, ibs=0.15880277071826662, ibnll=0.4718274965973856\n",
      "Validation loss decreased (0.147096 --> 0.146882).  Saving model ...                                                   \n",
      "it: 147, train loss=0.15004253387451172, validation loss=0.14688195288181305, c=0.6261644765253871, ibs=0.15911362415638453, ibnll=0.47120892080144955\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 148, train loss=0.15006762742996216, validation loss=0.14842906594276428, c=0.6223085939909927, ibs=0.15873780271789723, ibnll=0.47183841216092387\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 149, train loss=0.1498902589082718, validation loss=0.14695613086223602, c=0.6246529705719045, ibs=0.15883871864177285, ibnll=0.47082949310006095\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 150, train loss=0.14942847192287445, validation loss=0.14708386361598969, c=0.6237275587636498, ibs=0.15872873954739486, ibnll=0.47074443663365045\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 151, train loss=0.14918789267539978, validation loss=0.14782275259494781, c=0.6225553704731939, ibs=0.1586143842031684, ibnll=0.4711341765510447\n",
      "Validation loss decreased (0.146882 --> 0.146602).  Saving model ...                                                   \n",
      "it: 152, train loss=0.1492001712322235, validation loss=0.14660190045833588, c=0.6246838176321796, ibs=0.15894757534690626, ibnll=0.47071134268931225\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 153, train loss=0.1491970270872116, validation loss=0.14798963069915771, c=0.6220001233882411, ibs=0.15857005908651442, ibnll=0.47114315329778467\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 154, train loss=0.14903947710990906, validation loss=0.14667101204395294, c=0.6240051823061262, ibs=0.15875647833098777, ibnll=0.47039928278947296\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 155, train loss=0.14865081012248993, validation loss=0.14692629873752594, c=0.6225245234129188, ibs=0.15859852620159864, ibnll=0.47033241332631803\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 156, train loss=0.14838071167469025, validation loss=0.14727085828781128, c=0.6222468998704423, ibs=0.15848214499972182, ibnll=0.4704097516306054\n",
      "Validation loss decreased (0.146602 --> 0.146334).  Saving model ...                                                   \n",
      "it: 157, train loss=0.1483028680086136, validation loss=0.1463339775800705, c=0.6229872293170461, ibs=0.15873257615557612, ibnll=0.47008449703169064\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 158, train loss=0.14830057322978973, validation loss=0.14766791462898254, c=0.6206736997964094, ibs=0.15831960756694655, ibnll=0.4704006790087815\n",
      "Validation loss decreased (0.146334 --> 0.146195).  Saving model ...                                                   \n",
      "it: 159, train loss=0.1482621282339096, validation loss=0.14619502425193787, c=0.6227404528348448, ibs=0.1586330254281461, ibnll=0.469789181439418\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 160, train loss=0.14798195660114288, validation loss=0.1470140665769577, c=0.6211055586402616, ibs=0.15829497061354933, ibnll=0.4698613430212475\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 161, train loss=0.1476718932390213, validation loss=0.1464412659406662, c=0.621784193966315, ibs=0.15840403994680843, ibnll=0.46961703579417835\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 162, train loss=0.14741894602775574, validation loss=0.1462733894586563, c=0.6225245234129188, ibs=0.1583710248253277, ibnll=0.46940527841284085\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 163, train loss=0.14727714657783508, validation loss=0.1468956023454666, c=0.6219384292676908, ibs=0.1581026435178635, ibnll=0.4693800458115006\n",
      "Validation loss decreased (0.146195 --> 0.145887).  Saving model ...                                                   \n",
      "it: 164, train loss=0.1472310870885849, validation loss=0.145887091755867, c=0.6238509470047504, ibs=0.15849262415406196, ibnll=0.469274477754119\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 165, train loss=0.14723780751228333, validation loss=0.1475236862897873, c=0.6209821703991609, ibs=0.15805025153558772, ibnll=0.4697357818397468\n",
      "Validation loss decreased (0.145887 --> 0.145811).  Saving model ...                                                   \n",
      "it: 166, train loss=0.14731809496879578, validation loss=0.14581070840358734, c=0.6232340057992474, ibs=0.15863283923382415, ibnll=0.46944099304700515\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 167, train loss=0.14712779223918915, validation loss=0.14728952944278717, c=0.6205503115553088, ibs=0.15801025609192276, ibnll=0.4694599033795552\n",
      "EarlyStopping counter: 2 out of 20                                                                                     \n",
      "it: 168, train loss=0.14689670503139496, validation loss=0.14588560163974762, c=0.6227404528348448, ibs=0.15830740973312996, ibnll=0.46891337612883244\n",
      "EarlyStopping counter: 3 out of 20                                                                                     \n",
      "it: 169, train loss=0.14645665884017944, validation loss=0.1461879462003708, c=0.6219384292676908, ibs=0.15808527217038135, ibnll=0.4687828379965164\n",
      "EarlyStopping counter: 4 out of 20                                                                                     \n",
      "it: 170, train loss=0.14617648720741272, validation loss=0.14646869897842407, c=0.6205503115553088, ibs=0.1579642067141482, ibnll=0.468793712703352\n",
      "Validation loss decreased (0.145811 --> 0.145650).  Saving model ...                                                   \n",
      "it: 171, train loss=0.14609664678573608, validation loss=0.14564989507198334, c=0.62277129989512, ibs=0.15829456623647808, ibnll=0.46873591866900055\n",
      "EarlyStopping counter: 1 out of 20                                                                                     \n",
      "it: 172, train loss=0.14613866806030273, validation loss=0.14723502099514008, c=0.6214140292430131, ibs=0.15778416132464076, ibnll=0.4689764995771348\n",
      "Validation loss decreased (0.145650 --> 0.145597).  Saving model ...                                                   \n",
      "it: 173, train loss=0.1462712287902832, validation loss=0.14559657871723175, c=0.6232956999197976, ibs=0.15850623899669972, ibnll=0.46907286672903237\n",
      "  8%|                                   | 8/100 [16:34:58<149:55:01, 5866.32s/trial, best loss: 0.13281087577342987]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 8.00 GiB total capacity; 5.17 GiB already allocated; 342.27 MiB free; 6.11 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|                                   | 8/100 [16:35:00<190:42:37, 7462.58s/trial, best loss: 0.13281087577342987]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 8.00 GiB total capacity; 5.17 GiB already allocated; 342.27 MiB free; 6.11 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0abb205f1923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# # minimize the objective over the space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             )\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2b39d536d40c>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mibs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mibnll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0modesurv_manual_benchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"metabrick_test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nResults: {}, {}, {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mibs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mibnll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hp_sep_log.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ecd52ccf80de>\u001b[0m in \u001b[0;36modesurv_manual_benchmark\u001b[1;34m(df_train, df_test, config, name)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mmyloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modesurv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mu\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mmyloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sebastian\\research\\survnode\\sandbox\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 8.00 GiB total capacity; 5.17 GiB already allocated; 342.27 MiB free; 6.11 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# # minimize the objective over the space\n",
    "from hyperopt import fmin, tpe, space_eval\n",
    "best = fmin(objective, args, algo=tpe.suggest, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)\n",
    "print(space_eval(space, best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
